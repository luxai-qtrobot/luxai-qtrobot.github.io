{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"QTrobot Documentation This is the documentation page for all QTrobot developers out there. This page was made for helping our QTrobot community to have easier access to user manual, examples, demos, API reference and FAQ. Quick Start Notice To properly develop and use QTrobot for Research, knowledge of ROS is needed. Can't find what you're looking for? ASK YOUR QUESTION HERE","title":"Home"},{"location":"#qtrobot-documentation","text":"This is the documentation page for all QTrobot developers out there. This page was made for helping our QTrobot community to have easier access to user manual, examples, demos, API reference and FAQ. Quick Start Notice To properly develop and use QTrobot for Research, knowledge of ROS is needed. Can't find what you're looking for? ASK YOUR QUESTION HERE","title":"QTrobot Documentation"},{"location":"api/","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE QTrobot Interface The QTrobot interface aims to facilitate accessing basic robot functionalities leveraging a set of user-friendly ROS interfaces. Each robot\u2019s functionality can be accessed in blocking and non-blocking mode using ROS publish/subscribe and Service/Client interfaces. Following are the main interfaces of QTrobot: QTrobot Emotion : implements robot facial emotion QTrobot Speech : implements robot text to speech functionality QTrobot Audio : implement a simple player to play standard audio files QTrobot Gesture : implements robot gesture control QTrobot Behavior : implements more complex behaviors by combining the robot basic functionality QTrobot Motor : implements robot motor controls using standard ros_control QTrobot 3D Camera : implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK QTrobot Settings : implements some basic setting of robot such as speaker volume control and some extra interfaces which are available for some models of QTrobot: For each of the main interfaces, there are two different way of accessing them: Using ROS Publisher/Subscribers to allow non-blocking call to the interfaces Using ROS Services for blocking and accessing more sophisticated interfaces 1. Naming convention All the QTRobot ROS interfaces have /qt_robot/... prefix in front of their names. The word follows the prefix is the name of the main service (e.g. /qt_robot/speech/.. ). Each interface may have more sub-services (methods) which come after the main service name. For example: /qt_robot/speech/say : implements say() method of QTrobot TTS /qt_robot/speech/config : implements configure() method of QTrobot TTS For user\u2019s convenience we have given the same name to the service and subscriber for each QTrobot ROS interface. That means one can access, for example, the speech functionality using ROS service call or publish/subscribe via the same interface name (e.g. qt_robot/speech/say ). Please notice that some of the complex services (e.g. speech configuration for language, pitch,\u2026) are accessible only via ROS service call. 2. Accessing QTrobot interface from bash You can access each Robot functionality via its publish/subscribe or Service/Client interfaces. For example to use robot 'Speech' functionality you can try the following: Using ROS Publisher $ rostopic pub /qt_robot/speech/say std_msgs/String \"data: 'I am QT'\" Using ROS Service $ rosservice call /qt_robot/speech/say \"message: 'I am QT.'\" 3. Accessing QTrobot interface from a python script [Non-blocking mode] The following example shows how to access QTrobot Speech functionality using ROS publish/subscribe method from python: import rospy from std_msgs.msg import String # create a publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) ... # publish a text message to TTS (non-blocking) speechSay_pub . publish ( \"Hello! I am QT!\" ) [Blocking mode] And the following example, re-implements the previous one using ROS Service/Client method from python: import rospy from std_msgs.msg import String from qt_robot_interface.srv import * # create a service clients speechSay = rospy . ServiceProxy ( '/qt_robot/speech/say' , speech_say ) ... # call the service (blocking) resp = speechSay ( \"Hello! I am QT.\" ) Notice All QTrobot service interfaces returns the status of the call upun success or failure. For example to check whether a call to a service was successful in python, you can check the return value resp.status. 4. List of available interfaces Currently the following interfaces have been implemented: INTERFACES Functionality Interface prefix Description Speech /qt_robot/speech/... robot text to speech functionality Audio /qt_robot/audio/... simple standard audio file player Emotion /qt_robot/emotion/... robot facial emotion Gesture /qt_robot/gesture/... robot gesture control Behavior /qt_robot/behavior/... more complex behaviors by combining the robot basic functionality Motor /qt_robot/motor/... robot motor controls using standard ros_control Setting /qt_robot/setting/... basic setting of robot such as speaker volume control Human 3D Skeleton /qt_nuitrack_app/skeletons Human full body skeleton information Human 3D Hand Pos /qt_nuitrack_app/hands Human 3D hand pos and state Human Hand Gestures /qt_nuitrack_app/gestures Some basic human gestures recognition Human facial /qt_nuitrack_app/faces human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition 4.1 Speech Interface This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot. following are some standard supported languages: en-US (american english) fr-FR (French) de-DE (German) SUBSCRIBERS Interface Name Data Type Description /qt_robot/speech/say 'std_msgs/String' (text) Read a text using built-in TTS SERVICES Interface Name Service Name Parameters Description /qt_robot/speech/say 'speech_say' message Read a text using built-in TTS /qt_robot/speech/config 'speech_config' 'language', 'pitch', 'speed' Configure TTS /qt_robot/speech/stop 'speech_stop' Stops current speech activity. Info Default language is set in /opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml . the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them. 4.2 Audio Interface Play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy . In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wavand so on. SUBSCRIBERS Interface Name Data Type Description /qt_robot/audio/play 'std_msgs/String' (audio file name) Play an sudio file SERVICES Interface Name Service Name Parameters Description /qt_robot/audio/play 'audio_play' 'filename', 'filepath' Play an audio file given by its filename and filepath /qt_robot/audio/stop 'audio_stop' Stops current audio activity. Info The default path for the audio files is ~/robot/data/audios/ . To play the audio file from the default path, pass an empty string to filepath parameter. 4.3 Emotion Interface Change the robot facial emotions such as 'QT/happy', 'QT/sad', etc. SUBSCRIBERS Interface Name Data Type Description /qt_robot/emotion/show 'std_msgs/String' (emotion name) Show a facial emotion given by its name SERVICES Interface Name Service Name Parameters Description /qt_robot/emotion/show 'emotion_show' 'name' Show a facial emotion given by its name /qt_robot/emotion/stop 'emotion_stop' Stops current emotion activity. Info The complete list of emotion files can be found in '~/robot/data/emotions/'. 4.4 Gesture Interface Plays recored robot gesture (arms, head) such as 'happy', 'discust', etc. The complete list of gesture files can be found in ~/robot/data/gestures/ . SUBSCRIBERS Interface Name Data Type Description /qt_robot/gesture/play 'std_msgs/String' (gesture name) Play a robot gesture given by its name SERVICES Interface Name Service Name Parameters Description /qt_robot/gesture/play 'gesture_play' 'name', 'speed' Play a robot gesture given by its name and speed (default 1.0) /qt_robot/gesture/record 'gesture_record' 'parts', 'idleParts' Start recording a new gesture. 'parts' is a string array of parts name ('head', 'left_arm','right_arm') which specifies which robot part will be used for recording the gesture. 'idleParts' must be set to 'true' to release the motor PWM and put them in idle mode. If not you must put them in idle mode using '/qt_robot/motors/setControlMode' interface /qt_robot/gesture/save 'gesture_save' 'name', 'path' stops the current recording process and save the recorded gesture given by its 'name'. 'path' specifies where to save the gesture instead of the default path /qt_robot/gesture/list 'gesture_list' return a list of a the available gestures within the default gesture path /qt_robot/gesture/stop 'gesture_stop' Stops current gesture activity. Info The default value for speed is 1.0 and it is the default speed with which the gesture got recorded. Notice If the speed param value is 0 the default speed will be used to play the gestures. Default path to record/play the gesture is ~/robot/data/gestures/ . 4.5 Behavior Interface This interface implements higher-level and more complex behaviors by combing robot basic functionality. SUBSCRIBERS Interface Name Data Type Description /qt_robot/behavior/talkText 'std_msgs/String' (message) Read a text using TTS and show talking emotion /qt_robot/behavior/talkAudio 'std_msgs/String' (audio filename) Play an audio file and show talking emotion SERVICES Interface Name Service Name Parameters Description /qt_robot/behavior/talkText 'behavior_talk_text' 'message' Read a text using TTS and show talking emotion /qt_robot/behavior/talkAudio 'behavior_talk_audio' 'filename', 'filepath' Play an audio file and show talking emotion Info To play the audio file from the default path, pass an empty string to filepath param. Notice The talkAudio and talkText services are mutually exclusive and cannot be used with Emotion Interface at the same time. 4.6 Motor Interface Motor interface provide access to the robot actuators using standard ros_control system. Currently the interface implements ROS 'JointStateController', 'JointGroupPositionController' and a custom 'QTMotorsController' controllers. Notice Before using the Motor interface, ensure that you fully understood ros_control system and have a clear understanding of what you do at the motor joint level. 4.6.1 QTrobot parts The robot joints are structured as different parts as shown bellow: head HeadYaw HeadPitch right_arm RightShoulderPitch RightShoulderRoll RightElbowRoll left_arm LeftShoulderPitch LeftShoulderRoll LeftElbowRoll SUBSCRIBERS Interface Name Data Type Description /qt_robot/joints/state sensor_msgs/JointState publishes joint states (currently only positions) /qt_robot/head_position/command std_msgs/Float64MultiArray move the robot head to desired position given by (HeadYaw, HeadPitch). /qt_robot/right_arm_position/command std_msgs/Float64MultiArray move the right_arm to desired position given by (RightShoulderPitch, RightShoulderRoll, RightElbowRoll). /qt_robot/left_arm_position/command std_msgs/Float64MultiArray move the left_arm to desired position given by (LeftShoulderPitch, LeftShoulderRoll, LeftElbowRoll). SERVICES Interface Name Service Name Parameters Description /qt_robot/motors/home 'home' 'parts' moves the desired robot part to the home position. 'parts' is an array of robot parts and/or single joint name (e.g.['left_arm', 'right_arm', 'HeadPitch']) /qt_robot/motors/setControlMode 'set_control_mode' 'parts' set the control mode (M_ON=0,M_OFF=1, M_BRAKE=2) of desired robot part. 'parts' is an array of robot parts and/or single joint name (e.g. ['left_arm', 'right_arm', 'HeadPitch']). M_ON: motor is controlled. M_OFF: motor is idle and M_BRAKE: motor is in brake mode (not controlled) /qt_robot/motors/setVelocity 'set_velocity' 'parts', 'velocity' sets the moving velocity of the desired robot part. 'parts' is an array of robot parts and/or single joint name (e.g. ['left_arm', 'right_arm', 'HeadPitch']). 'velocity' is given as percentage. Notice For safety purpose, every joint has a maximum velocity limits. For example you cannot run 'HeadPitch' joint with more than 20% of the maximum velocity. 4.7 Setting Interface This interface provides some basic setting of robot such as speaker volume control. SERVICES Interface Name Service Name Parameters Description /qt_robot/setting/setVolume 'setting_setVolume' 'volume' set the robot speaker volume to the desired level (in percentage) 4.8 Human 3D Tracking Interface This interface implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK . PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image 5. Tutorials Please check our Demo and Examples sections. You can also check our repository for the full list of examples and interesting demos for QTrobot.","title":"API Reference"},{"location":"api/#qtrobot-interface","text":"The QTrobot interface aims to facilitate accessing basic robot functionalities leveraging a set of user-friendly ROS interfaces. Each robot\u2019s functionality can be accessed in blocking and non-blocking mode using ROS publish/subscribe and Service/Client interfaces. Following are the main interfaces of QTrobot: QTrobot Emotion : implements robot facial emotion QTrobot Speech : implements robot text to speech functionality QTrobot Audio : implement a simple player to play standard audio files QTrobot Gesture : implements robot gesture control QTrobot Behavior : implements more complex behaviors by combining the robot basic functionality QTrobot Motor : implements robot motor controls using standard ros_control QTrobot 3D Camera : implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK QTrobot Settings : implements some basic setting of robot such as speaker volume control and some extra interfaces which are available for some models of QTrobot: For each of the main interfaces, there are two different way of accessing them: Using ROS Publisher/Subscribers to allow non-blocking call to the interfaces Using ROS Services for blocking and accessing more sophisticated interfaces","title":"QTrobot Interface"},{"location":"api/#1-naming-convention","text":"All the QTRobot ROS interfaces have /qt_robot/... prefix in front of their names. The word follows the prefix is the name of the main service (e.g. /qt_robot/speech/.. ). Each interface may have more sub-services (methods) which come after the main service name. For example: /qt_robot/speech/say : implements say() method of QTrobot TTS /qt_robot/speech/config : implements configure() method of QTrobot TTS For user\u2019s convenience we have given the same name to the service and subscriber for each QTrobot ROS interface. That means one can access, for example, the speech functionality using ROS service call or publish/subscribe via the same interface name (e.g. qt_robot/speech/say ). Please notice that some of the complex services (e.g. speech configuration for language, pitch,\u2026) are accessible only via ROS service call.","title":"1. Naming convention"},{"location":"api/#2-accessing-qtrobot-interface-from-bash","text":"You can access each Robot functionality via its publish/subscribe or Service/Client interfaces. For example to use robot 'Speech' functionality you can try the following: Using ROS Publisher $ rostopic pub /qt_robot/speech/say std_msgs/String \"data: 'I am QT'\" Using ROS Service $ rosservice call /qt_robot/speech/say \"message: 'I am QT.'\"","title":"2. Accessing QTrobot interface from bash"},{"location":"api/#3-accessing-qtrobot-interface-from-a-python-script","text":"[Non-blocking mode] The following example shows how to access QTrobot Speech functionality using ROS publish/subscribe method from python: import rospy from std_msgs.msg import String # create a publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) ... # publish a text message to TTS (non-blocking) speechSay_pub . publish ( \"Hello! I am QT!\" ) [Blocking mode] And the following example, re-implements the previous one using ROS Service/Client method from python: import rospy from std_msgs.msg import String from qt_robot_interface.srv import * # create a service clients speechSay = rospy . ServiceProxy ( '/qt_robot/speech/say' , speech_say ) ... # call the service (blocking) resp = speechSay ( \"Hello! I am QT.\" ) Notice All QTrobot service interfaces returns the status of the call upun success or failure. For example to check whether a call to a service was successful in python, you can check the return value resp.status.","title":"3. Accessing QTrobot interface from a python script"},{"location":"api/#4-list-of-available-interfaces","text":"Currently the following interfaces have been implemented: INTERFACES Functionality Interface prefix Description Speech /qt_robot/speech/... robot text to speech functionality Audio /qt_robot/audio/... simple standard audio file player Emotion /qt_robot/emotion/... robot facial emotion Gesture /qt_robot/gesture/... robot gesture control Behavior /qt_robot/behavior/... more complex behaviors by combining the robot basic functionality Motor /qt_robot/motor/... robot motor controls using standard ros_control Setting /qt_robot/setting/... basic setting of robot such as speaker volume control Human 3D Skeleton /qt_nuitrack_app/skeletons Human full body skeleton information Human 3D Hand Pos /qt_nuitrack_app/hands Human 3D hand pos and state Human Hand Gestures /qt_nuitrack_app/gestures Some basic human gestures recognition Human facial /qt_nuitrack_app/faces human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition","title":"4. List of available interfaces"},{"location":"api/#5-tutorials","text":"Please check our Demo and Examples sections. You can also check our repository for the full list of examples and interesting demos for QTrobot.","title":"5. Tutorials"},{"location":"demos/","text":"Demos About Here is the list of all demos with QTrobot. All demos and code you can find in our github tutorial repository. Range of Motion & DOF contains motor movement demo source code! Emotion Game Image recognition emotion demo Download a PDF file for the demo source code! Memory Game Image recognition memory demo Download a PDF file for the demo source code! Gesture recognition Gesture recognition with 3D camera source code! Face, Age & Gender recognition Face, Age & Gender recognition with 3D camera source code! Voice interaction Voice interaction with Far-Field Microphone Array source code! Voice activity & direction detection Voice activity & direction detection with Far-Field Microphone Array source code! Skeleton Tracking Skeleton Tracking with 3D camera","title":"Demos"},{"location":"demos/#demos","text":"About Here is the list of all demos with QTrobot. All demos and code you can find in our github tutorial repository.","title":"Demos"},{"location":"examples/","text":"About Here is the list of all examples with QTrobot . All examples and code you can find in our github tutorial repository . Interfaces Audio interface : demonstrates how to make robot sing something Emotion interface : demonstrates how to make robot show some emotion Speech interface : demonstrates how to make robot say something Motor Motors command : demonstrates how to command robot motors positions Motors gesture : demonstrates how to run prerecorded gestures Motors state : demonstrates how to read robot joints state such as positions Motors ROS Controller : demonstrates how to implement ROS controller for QTrobot Motors MoveIt : demonstrates how to use ROS MoveIT to control QTrobot arms Voice Speech Recognition - Google SpeechToText : example of google speech-to-text ros service Voice activity : demonstrates how to show voice activity Voice direction : demonstrates how to read direction of voice Vision Gestures detection : demonstrates how to read from qtrobot nuitrack gesture topic Skeletons detection : demonstrates how to read from qtrobot nuitrack skeletons topic Hands detection : demonstrates how to read from qtrobot nuitrack hands topic Faces detection : demonstrates how to read from qtrobot nuitrack faces topic","title":"Examples"},{"location":"examples/#interfaces","text":"Audio interface : demonstrates how to make robot sing something Emotion interface : demonstrates how to make robot show some emotion Speech interface : demonstrates how to make robot say something","title":"Interfaces"},{"location":"examples/#motor","text":"Motors command : demonstrates how to command robot motors positions Motors gesture : demonstrates how to run prerecorded gestures Motors state : demonstrates how to read robot joints state such as positions Motors ROS Controller : demonstrates how to implement ROS controller for QTrobot Motors MoveIt : demonstrates how to use ROS MoveIT to control QTrobot arms","title":"Motor"},{"location":"examples/#voice","text":"Speech Recognition - Google SpeechToText : example of google speech-to-text ros service Voice activity : demonstrates how to show voice activity Voice direction : demonstrates how to read direction of voice","title":"Voice"},{"location":"examples/#vision","text":"Gestures detection : demonstrates how to read from qtrobot nuitrack gesture topic Skeletons detection : demonstrates how to read from qtrobot nuitrack skeletons topic Hands detection : demonstrates how to read from qtrobot nuitrack hands topic Faces detection : demonstrates how to read from qtrobot nuitrack faces topic","title":"Vision"},{"location":"user-manual/","text":"QTrobot is a high-end research platform with powerfull computers and state-of-the-art hardware such as 3D camera and far-field michrohone array. It comes with two computers: i) QTRP, a Raspberry Pi based computer to control the main hardware and ii) QTPC, an Intel\u00ae NUC i5/i7 PC to provide more computational power and to accelerate software development cycle. Both computers run on Ubuntu/Debian linux operating systems and leverage ROS to offer easy-to-use yet very flexible software atchitecture. QTrobot's hardware is easily extendable via USB-C and USB adaptors, for example, to connect external monitor, keyboard and mouse. QTrobot also provides a rich set of APIs for several programing languages such as C++, Python and JavaScript. 1. QTrobot Architecture As it is shown in the following diagram, QTrobot consists of two computers, one in the QTrobot head (QTRP) and the other in the body (QTPC), which are internally connected via ethernet cable. All of the QTrobot devices such as display, speakers, microphone and motors are connected to QTRP except the 3D Camera which is connected directly to QTPC. The Wi-Fi of the QTRP is used to create the QTrobot hotspot with the same SSID as the robot\u2019s serial number (e.g. QTRD000101). Both computers are configured to interact in one ROS environment. Upon starting the QTrobot, QTRP runs the roscore and initializes the ROS environment. It also turn on QTPC via wake-on-lan when the ROS environment is up. You will also find two USB port at the back of the QTrobot. One of the is attached to QTRP and the second one (USB-C) is connected to QTPC. This USB-C port can be used to connect keyboard,mouse and monitor via a USB-C extension hub to QTPC. Notice QTRP running roscore and QTPC are connected over LAN cable so you don't need any other configuration or connection to QTRP to access/work with QTrobot's ROS environment. QTPC is already prepared and configured to work with roscore which is running on QTRP and you can access all QTrobot ROS services ands topics. 2. How to work with QTrobot There are few ways of working with QTrobot. We recommend working on QTPC, which is the fastest and easiest way to start. [Step 1] Turn on QTrobot To power the QTrobot on, simply connect the QTrobot's power supply 1 .This triggers the power on process and your QTrobot becomes ready less than a minute. After powering on, you can see the QTrobot face is on and the motors are in their home position. If the QTrobot is already connected to the power supply simply push the power button to turn on the QTrobot. Working on QTPC [Step 2] Connect keyboard/mouse and monitor to QTPC Connect keyboard, mouse and HDMI Monitor to your USB-C extention then plug it in the USB-C port at the back of QTrobot. Example Notice If you see that your keyboard or mouse doesn't work. Disconnect the USB-C adapter from the QTrobot, reverse or turn around the connector and plug it back in. [Step 3] Wait for QTPC to boot After you turned on your QTrobot it will need some time until your QTPC turns on, because QTRP will turn on QTPC after ROS environment is up. You will see the Ubuntu desktop of QTPC and you can use in the same way you use a standard desktop for development. Working from your Laptop [Step 2] Setup your ROS environment Notice For this steps you wiil need ROS installed on your computer. If you don't have it please follow this instalation instructions Install QTrobot ROS headers files Clone QTrobot Software repository. cd ~/ git clone https://github.com/luxai-qtrobot/software.git Copy the headers folder or make a link in your Catkin source workspace: cd ~/catkin_ws/src ln -s ~/software/headers ./ Build the messages in your Catkin worksapce cd ~/catkin_ws catkin_make Configure ROS environment for QTrobot Connect to the QTrobot Wi-Fi hotspot. Then open a terminal on your PC and run the following command: ifconfig Write down or copy IP address that is written of wifi inet (e.g. inet 10.42.0.55 ) wifi0: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10 .42.0.55 netmask 255 .255.255.0 broadcast 10 .42.0.255 inet6 fe80::c5ab:2083:f0c2:daa3 prefixlen 64 scopeid 0xfd<compat,link,site,host> ether e4:b3:18:dd:87:31 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Edit ~/.bash_aliases on your laptop/PC and add following lines. Change ROS_IP to your IP from last step. source /opt/ros/noetic/setup.bash source ~/catkin_ws/devel/setup.bash ## QTrobot export ROS_IP = <your PC IP address> export ROS_MASTER_URI = http://192.168.100.1:11311 To apply this configuration run this command: source ~/.bash_aliases [Step 3] Test your setup Last step is to connect to the QTrobot Wi-Fi hotspot and test your ROS environment. One option is to list all ROS topics that are running on QTRobot: rostopic list If everything is working you should see output similar to this one. /rosout /qt_robot/audio/play /qt_robot/behavior/talkAudio /qt_robot/emotion/show /qt_robot/gesture/play /qt_robot/head_position/command /qt_robot/joints/state ... or call some of the QTrobot ROS interfaces: rosservice call /qt_robot/speech/say \"message: 'Hello.'\" You will hear QTrobot say Hello . Powering off QTrobot To power off the QTrobot simply press the power button on the back of the QTrobot. This will turn off both QTRP and QTPC. Wait enough for QTrobot to turn off before unpluging it from power supply: a blue screen shows up immedietly goes off. Continue reading You can follow any of next topics to learn more on how to setup other things on QTrobot, we recommend to continue with How to SSH to QTrobot . How to SSH to QTrobot How to connect internet to QTrobot How to use QTrobot ROS API 3. How to SSH to QTrobot SSH to QTRP To ssh to QTRP, first connect your laptop to the QTrobot Wi-Fi hotspot and run the following command 2 . You can alternatively run the following command from QTPC's termianl. For the password use qtrobot . ssh developer@192.168.100.1 Notice For some older version of QTrobots you need to ssh with qtrobot user instead of developer (i.e. ssh qtrobot@192.168.100.1 ). For the password use qtrobot . SSH to QTPC To ssh to QTPC, first connect your laptop to the QTrobot Wi-Fi hotspot and run the following command 2 . For the password use qtrobot . ssh qtrobot@192.168.100.2 Alternatively you can ssh to QTPC via QTRP with the same command. 4. Connecting to a Home Network and Internet The older version of QTrobots (which had been released before May 2021) have different operating system for QTRP. Those set of robots (with QTRP verision older than QTRDTP2105 ) require slightly different set of steps for connecting them to the Internet. Please carefully do the following steps to find the version of your QTRP. Finding the version of QTRP If you do not know how to access QTRP, please first read How to SSH to QTrobot . 1. ssh to QTRP and run the folowing command to check the codename of its operating system: lsb_release -c 2. if the codename is xenial then the version of your QTRP is older than QTRDTP2105. Recent QTrobots The following diagram demonstrates a common scenario where different devices (e.g. laptops) are connected to the QTrobot via it's Wi-Fi hotspot. The Wi-Fi of QTRP is setup to operate in AP/STA mode. That means the QTRP's Wi-Fi can be connected to a home router (as Wi-Fi client) and at the same time operates as hotspot (access point). To have access to the internet on all machines (as depecited in the diagram), the Wi-Fi of QTRP should be connected to the home router. The internet from the home router, therefore is shared to all other machines (including QTPC) via QTRP. The green arrows shows the direction and how internet shared between QTRP, QTPC and any other devices connected to QTrobot hotspot. Therefore, you need to connect the QTRP to your home router's Wi-Fi. To do that do the following steps: [Step 1] access QTRP via ssh Please see SSH to QTRP if you have not done this before. [Step 2] configure the wpa_suplicant for wlan0 Edit the /etc/wpa_supplicant/wpa_supplicant-wlan0.conf file to update ssid and psk accordignly. sudo nano /etc/wpa_supplicant/wpa_supplicant-wlan0.conf set the ssid (your router ssid) and psk (your router passphrase) of your home router setup: country=LU ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"<your router SSID>\" psk=\"<your router passphrase>\" key_mgmt=WPA-PSK # see ref (4) } [Step 3] reboot the QTrobot fron the ssh terminal of QTRP type: sudo reboot Alternativey, just turn off and then turn on your QTrobot. After the reboot, QTRP gets internet connection from your router and will automaticly share it with QTPC and any other devices connected to QTrobot Wi-FI hotspot. Warning Do not create or change any other files related to wpa_supplicant or systemd network! this may break your QTrobot network setup! QTrobots older than QTRDTP2105 Warning Do not use the QTRP Wi-Fi to connect to a network for QTrobot older than QTRDTP2105. Instead, use Wi-Fi from QTPC as explained bellow! The following diagram demosntrates a common scenario where different devices (e.g. laptops) are connected to the QTrobot via it's Wi-Fi hotspot. To have access to the internet on all these machines, the Wi-Fi of QTPC should be connected to the home router. The internet from the home router, therefore is shared to all other machines via QTPC. The green arrows shows the direction and how internet shared between QTPC, QTRP and any other devices connected to QTrobot hotspot. Therefore, all you need to do is to connect the QTPC to your home router's Wi-Fi. After your QTPC gets internet connection, it will automaticly share it with QTRP and any other devices connected to QTrobot Wi-FI hotspot. Connect to a wireless network from Ubuntu desktop Access the QTPC ubuntu desktop (see Working on QTPC ) Open the system menu from the right side of the top bar . Select Wi-Fi and click Select Network. Click the name of the network you want, then click Connect. ... If the network is protected by a password (encryption key), enter the password when prompted and click Connect. 5. QTRobot Autostart scripts Autostart scripts are simply some bash scripts which are executed at QTrobot startup time. There are different set of autostart scripts, one for QTRP and the other set for QTPC. Generally, these scripts prepare QTrobot network, setup ROS environment and launch specefic ros nodes on each machine. For example, autostart scripts of QTRP, run roscore and launch QTrobot motor and other important controllers. Moreover, as it can be infered from QTrobot Architecture , one of the autostart scripts of QTPC is responsible to launch the qt_nuitrack_app node which provide ROS interface for Nuitrack skeleton tracking using 3D camera. Therefore, it is very important that you completely understood the purpose of each script before disabling or modifying the QTrobot autostart behavior. How QTrobot autostart scripts work For both QTRP and QTPC, the autostart scripts are located in ~/robot/autostart folder. The scripts are run by linux Cron job scheduler. In fact, a specific cron job is configured to run the autostart_screens.sh script at boot time. The other scripts are launched by autostart_screens.sh and their output are redirected to the corresponding log files. __________ ______________________ ___________________ | Cron job | ---> | autostart_screens.sh | ----> | start_script_1.sh | \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af | \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af | ___________________ \u2514--> | start_script_2.sh | \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af ... How to check the log files of autostart scripts All programs/ROS nodes which are run by QTrobot autostart scripts redirect their standard output (info/war/error messages) to their corresponding log file. These log files can be found under ~/robot/autostart/logs folder. How to run my own script at startup For example, to run your custom script on QTPC at the startup, you need to first create a startup script file and call it from autostart_screens.sh . Do the following steps: [Step 1] Create custom startup script open a terminal on QTPC and create a bash script file. Let's name it start_my_script.sh . nano ~/robot/autostart/start_my_script.sh with the following example content: # !/bin/bash source /home/qtrobot/robot/autostart/qt_robot.inc SCRIPT_NAME = \"start_my_script\" LOG_FILE = $( prepare_logfile \" $SCRIPT_NAME \" ) { prepare_ros_environment wait_for_ros_node \"/rosout\" 60 # ... # add your main code here /usr/bin/echo \"This is my script!\" # ... } & >> ${ LOG_FILE } [Step 2] Add it to autostart_screens.sh Carefully edit the autostart_screens.sh : nano ~/robot/autostart/autostart_screens.sh To add your own script (e.g. start_my_script.sh ) you can simply add the following line to the buttom of autostart_screens.sh right before the } &>> ${LOG_FILE} line. The autostart_screens.sh should look like the following: { wait_for_network ... ... run_script \"start_my_script.sh\" } &>> ${LOG_FILE} [Step 3] Reboot and check Reboot the QTrobot. After reboot, your script should be executed on startup of QTPC. You can now check the log file of your script: cat ~/robot/austostart/logs/start_my_script.log Can't find what you're looking for? ASK YOUR QUESTION HERE Only use the provided power supply with your QTrobot. \u21a9 Windows users can use Putty ( www.putty.org ) \u21a9 \u21a9","title":"User Manual"},{"location":"user-manual/#1-qtrobot-architecture","text":"As it is shown in the following diagram, QTrobot consists of two computers, one in the QTrobot head (QTRP) and the other in the body (QTPC), which are internally connected via ethernet cable. All of the QTrobot devices such as display, speakers, microphone and motors are connected to QTRP except the 3D Camera which is connected directly to QTPC. The Wi-Fi of the QTRP is used to create the QTrobot hotspot with the same SSID as the robot\u2019s serial number (e.g. QTRD000101). Both computers are configured to interact in one ROS environment. Upon starting the QTrobot, QTRP runs the roscore and initializes the ROS environment. It also turn on QTPC via wake-on-lan when the ROS environment is up. You will also find two USB port at the back of the QTrobot. One of the is attached to QTRP and the second one (USB-C) is connected to QTPC. This USB-C port can be used to connect keyboard,mouse and monitor via a USB-C extension hub to QTPC. Notice QTRP running roscore and QTPC are connected over LAN cable so you don't need any other configuration or connection to QTRP to access/work with QTrobot's ROS environment. QTPC is already prepared and configured to work with roscore which is running on QTRP and you can access all QTrobot ROS services ands topics.","title":"1. QTrobot Architecture"},{"location":"user-manual/#2-how-to-work-with-qtrobot","text":"There are few ways of working with QTrobot. We recommend working on QTPC, which is the fastest and easiest way to start. [Step 1] Turn on QTrobot To power the QTrobot on, simply connect the QTrobot's power supply 1 .This triggers the power on process and your QTrobot becomes ready less than a minute. After powering on, you can see the QTrobot face is on and the motors are in their home position. If the QTrobot is already connected to the power supply simply push the power button to turn on the QTrobot. Working on QTPC [Step 2] Connect keyboard/mouse and monitor to QTPC Connect keyboard, mouse and HDMI Monitor to your USB-C extention then plug it in the USB-C port at the back of QTrobot. Example Notice If you see that your keyboard or mouse doesn't work. Disconnect the USB-C adapter from the QTrobot, reverse or turn around the connector and plug it back in. [Step 3] Wait for QTPC to boot After you turned on your QTrobot it will need some time until your QTPC turns on, because QTRP will turn on QTPC after ROS environment is up. You will see the Ubuntu desktop of QTPC and you can use in the same way you use a standard desktop for development. Working from your Laptop [Step 2] Setup your ROS environment Notice For this steps you wiil need ROS installed on your computer. If you don't have it please follow this instalation instructions Install QTrobot ROS headers files Clone QTrobot Software repository. cd ~/ git clone https://github.com/luxai-qtrobot/software.git Copy the headers folder or make a link in your Catkin source workspace: cd ~/catkin_ws/src ln -s ~/software/headers ./ Build the messages in your Catkin worksapce cd ~/catkin_ws catkin_make Configure ROS environment for QTrobot Connect to the QTrobot Wi-Fi hotspot. Then open a terminal on your PC and run the following command: ifconfig Write down or copy IP address that is written of wifi inet (e.g. inet 10.42.0.55 ) wifi0: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10 .42.0.55 netmask 255 .255.255.0 broadcast 10 .42.0.255 inet6 fe80::c5ab:2083:f0c2:daa3 prefixlen 64 scopeid 0xfd<compat,link,site,host> ether e4:b3:18:dd:87:31 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Edit ~/.bash_aliases on your laptop/PC and add following lines. Change ROS_IP to your IP from last step. source /opt/ros/noetic/setup.bash source ~/catkin_ws/devel/setup.bash ## QTrobot export ROS_IP = <your PC IP address> export ROS_MASTER_URI = http://192.168.100.1:11311 To apply this configuration run this command: source ~/.bash_aliases [Step 3] Test your setup Last step is to connect to the QTrobot Wi-Fi hotspot and test your ROS environment. One option is to list all ROS topics that are running on QTRobot: rostopic list If everything is working you should see output similar to this one. /rosout /qt_robot/audio/play /qt_robot/behavior/talkAudio /qt_robot/emotion/show /qt_robot/gesture/play /qt_robot/head_position/command /qt_robot/joints/state ... or call some of the QTrobot ROS interfaces: rosservice call /qt_robot/speech/say \"message: 'Hello.'\" You will hear QTrobot say Hello . Powering off QTrobot To power off the QTrobot simply press the power button on the back of the QTrobot. This will turn off both QTRP and QTPC. Wait enough for QTrobot to turn off before unpluging it from power supply: a blue screen shows up immedietly goes off. Continue reading You can follow any of next topics to learn more on how to setup other things on QTrobot, we recommend to continue with How to SSH to QTrobot . How to SSH to QTrobot How to connect internet to QTrobot How to use QTrobot ROS API","title":"2. How to work with QTrobot"},{"location":"user-manual/#3-how-to-ssh-to-qtrobot","text":"SSH to QTRP To ssh to QTRP, first connect your laptop to the QTrobot Wi-Fi hotspot and run the following command 2 . You can alternatively run the following command from QTPC's termianl. For the password use qtrobot . ssh developer@192.168.100.1 Notice For some older version of QTrobots you need to ssh with qtrobot user instead of developer (i.e. ssh qtrobot@192.168.100.1 ). For the password use qtrobot . SSH to QTPC To ssh to QTPC, first connect your laptop to the QTrobot Wi-Fi hotspot and run the following command 2 . For the password use qtrobot . ssh qtrobot@192.168.100.2 Alternatively you can ssh to QTPC via QTRP with the same command.","title":"3. How to SSH to QTrobot"},{"location":"user-manual/#4-connecting-to-a-home-network-and-internet","text":"The older version of QTrobots (which had been released before May 2021) have different operating system for QTRP. Those set of robots (with QTRP verision older than QTRDTP2105 ) require slightly different set of steps for connecting them to the Internet. Please carefully do the following steps to find the version of your QTRP. Finding the version of QTRP If you do not know how to access QTRP, please first read How to SSH to QTrobot . 1. ssh to QTRP and run the folowing command to check the codename of its operating system: lsb_release -c 2. if the codename is xenial then the version of your QTRP is older than QTRDTP2105. Recent QTrobots The following diagram demonstrates a common scenario where different devices (e.g. laptops) are connected to the QTrobot via it's Wi-Fi hotspot. The Wi-Fi of QTRP is setup to operate in AP/STA mode. That means the QTRP's Wi-Fi can be connected to a home router (as Wi-Fi client) and at the same time operates as hotspot (access point). To have access to the internet on all machines (as depecited in the diagram), the Wi-Fi of QTRP should be connected to the home router. The internet from the home router, therefore is shared to all other machines (including QTPC) via QTRP. The green arrows shows the direction and how internet shared between QTRP, QTPC and any other devices connected to QTrobot hotspot. Therefore, you need to connect the QTRP to your home router's Wi-Fi. To do that do the following steps: [Step 1] access QTRP via ssh Please see SSH to QTRP if you have not done this before. [Step 2] configure the wpa_suplicant for wlan0 Edit the /etc/wpa_supplicant/wpa_supplicant-wlan0.conf file to update ssid and psk accordignly. sudo nano /etc/wpa_supplicant/wpa_supplicant-wlan0.conf set the ssid (your router ssid) and psk (your router passphrase) of your home router setup: country=LU ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\"<your router SSID>\" psk=\"<your router passphrase>\" key_mgmt=WPA-PSK # see ref (4) } [Step 3] reboot the QTrobot fron the ssh terminal of QTRP type: sudo reboot Alternativey, just turn off and then turn on your QTrobot. After the reboot, QTRP gets internet connection from your router and will automaticly share it with QTPC and any other devices connected to QTrobot Wi-FI hotspot. Warning Do not create or change any other files related to wpa_supplicant or systemd network! this may break your QTrobot network setup! QTrobots older than QTRDTP2105 Warning Do not use the QTRP Wi-Fi to connect to a network for QTrobot older than QTRDTP2105. Instead, use Wi-Fi from QTPC as explained bellow! The following diagram demosntrates a common scenario where different devices (e.g. laptops) are connected to the QTrobot via it's Wi-Fi hotspot. To have access to the internet on all these machines, the Wi-Fi of QTPC should be connected to the home router. The internet from the home router, therefore is shared to all other machines via QTPC. The green arrows shows the direction and how internet shared between QTPC, QTRP and any other devices connected to QTrobot hotspot. Therefore, all you need to do is to connect the QTPC to your home router's Wi-Fi. After your QTPC gets internet connection, it will automaticly share it with QTRP and any other devices connected to QTrobot Wi-FI hotspot. Connect to a wireless network from Ubuntu desktop Access the QTPC ubuntu desktop (see Working on QTPC ) Open the system menu from the right side of the top bar . Select Wi-Fi and click Select Network. Click the name of the network you want, then click Connect. ... If the network is protected by a password (encryption key), enter the password when prompted and click Connect.","title":"4. Connecting to a Home Network and Internet"},{"location":"user-manual/#5-qtrobot-autostart-scripts","text":"Autostart scripts are simply some bash scripts which are executed at QTrobot startup time. There are different set of autostart scripts, one for QTRP and the other set for QTPC. Generally, these scripts prepare QTrobot network, setup ROS environment and launch specefic ros nodes on each machine. For example, autostart scripts of QTRP, run roscore and launch QTrobot motor and other important controllers. Moreover, as it can be infered from QTrobot Architecture , one of the autostart scripts of QTPC is responsible to launch the qt_nuitrack_app node which provide ROS interface for Nuitrack skeleton tracking using 3D camera. Therefore, it is very important that you completely understood the purpose of each script before disabling or modifying the QTrobot autostart behavior. How QTrobot autostart scripts work For both QTRP and QTPC, the autostart scripts are located in ~/robot/autostart folder. The scripts are run by linux Cron job scheduler. In fact, a specific cron job is configured to run the autostart_screens.sh script at boot time. The other scripts are launched by autostart_screens.sh and their output are redirected to the corresponding log files. __________ ______________________ ___________________ | Cron job | ---> | autostart_screens.sh | ----> | start_script_1.sh | \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af | \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af | ___________________ \u2514--> | start_script_2.sh | \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af ... How to check the log files of autostart scripts All programs/ROS nodes which are run by QTrobot autostart scripts redirect their standard output (info/war/error messages) to their corresponding log file. These log files can be found under ~/robot/autostart/logs folder. How to run my own script at startup For example, to run your custom script on QTPC at the startup, you need to first create a startup script file and call it from autostart_screens.sh . Do the following steps: [Step 1] Create custom startup script open a terminal on QTPC and create a bash script file. Let's name it start_my_script.sh . nano ~/robot/autostart/start_my_script.sh with the following example content: # !/bin/bash source /home/qtrobot/robot/autostart/qt_robot.inc SCRIPT_NAME = \"start_my_script\" LOG_FILE = $( prepare_logfile \" $SCRIPT_NAME \" ) { prepare_ros_environment wait_for_ros_node \"/rosout\" 60 # ... # add your main code here /usr/bin/echo \"This is my script!\" # ... } & >> ${ LOG_FILE } [Step 2] Add it to autostart_screens.sh Carefully edit the autostart_screens.sh : nano ~/robot/autostart/autostart_screens.sh To add your own script (e.g. start_my_script.sh ) you can simply add the following line to the buttom of autostart_screens.sh right before the } &>> ${LOG_FILE} line. The autostart_screens.sh should look like the following: { wait_for_network ... ... run_script \"start_my_script.sh\" } &>> ${LOG_FILE} [Step 3] Reboot and check Reboot the QTrobot. After reboot, your script should be executed on startup of QTPC. You can now check the log file of your script: cat ~/robot/austostart/logs/start_my_script.log Can't find what you're looking for? ASK YOUR QUESTION HERE Only use the provided power supply with your QTrobot. \u21a9 Windows users can use Putty ( www.putty.org ) \u21a9 \u21a9","title":"5. QTRobot Autostart scripts"},{"location":"FAQ/Audio/","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1.Can I stream microphone audio? When using audio_common you just need to configure capture.launch file to use proper device, so our mic is hw:1,0. Running audio_capture it will create \"/audio/audio\" topic and on that topic you can listen to the mic, read data etc. There is already tutorial on that page how to set it up. You can also try this respeaker_ros and use some of this topics : /sound_direction # Result of DoA /sound_localization # Result of DoA as Pose /is_speeching # Result of VAD /audio # Raw audio /speech_audio # Audio data while speeching","title":"Audio"},{"location":"FAQ/Audio/#1can-i-stream-microphone-audio","text":"When using audio_common you just need to configure capture.launch file to use proper device, so our mic is hw:1,0. Running audio_capture it will create \"/audio/audio\" topic and on that topic you can listen to the mic, read data etc. There is already tutorial on that page how to set it up. You can also try this respeaker_ros and use some of this topics : /sound_direction # Result of DoA /sound_localization # Result of DoA as Pose /is_speeching # Result of VAD /audio # Raw audio /speech_audio # Audio data while speeching","title":"1.Can I stream microphone audio?"},{"location":"FAQ/Miscellaneous/","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1. Can I use C# to program QTrobot? QTrobot APIs are based on the most popular software framework in robotic, ROS \u2013 a publish/subscribe middleware. ROS by default supports C++ and Python but other languages are covered via different open-source and well-maintained client libraries. Indeed you can use C# to program QTrobot. You can refer to ros-sharp open-source library which leverages websockets for underlying communication and cover all QTrobot APIs. It has good integration with Unity3D too. We have also developed different Android apps using websockets in Jscript and using native Java APIs for our QTrobot. 2. How to connect Bluetooth mouse/keyboard to QTrobot? This tutorial explain how to pair and connect a Bluetooth mouse/keyboard to QTrobot QTPC pc via terminal/ssh. The same procedure works also for QTRP. Notice : for QTRP you may need to run the bluetoothctl with sudo ! [Step 1] SSH to QTPC Connect to the QTrobot Wi-Fi and ssh into QTPC: ``` ssh qtrobot @192.168.100.2 ``` [Step 2] launch bluetoothctl qtrobot@QTPC:~$ bluetoothctl [NEW] Controller F8:63:3F:40:61:B2 QTPC [default] [Step 3] turn bluetooth power on and register the agent [bluetooth]# power on Changing power on succeeded [bluetooth]# agent on Agent registered [bluetooth]# default-agent Default agent request successful [Step 4] scan and pair the bluetooth device To scan the bluettoth devices: ``` [ bluetooth ] # scan on Discovery started [ CHG ] Controller F8 : 63 : 3 F : 40 : 61 : B2 Discovering : yes [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Name : Bluetooth 3.0 Keyboard ... ``` in my case Device 17:13:00:00:8A:04 Name: Bluetooth 3.0 Keyboard is what I am looking for. Now to pair the bluetooth device: ``` [ bluetooth ] # pair 17 : 13 : 00 : 00 : 8 A : 04 Attempting to pair with 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Connected : yes [ agent ] PIN code : xxxx ... [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Paired : yes Pairing successful ``` Some keyboards require to type a PIN code for pairing. Just type the code using your bluetooth keyboard . You do not see anything on the terminal. The code is sent directly by your keyboard while you are typing it. [Step 5] trust and connect to the device trust the device: [bluetooth]# trust 17:13:00:00:8A:04 [CHG] Device 17:13:00:00:8A:04 Trusted: yes and finally connect to the bluettoh/keyboard mouse: [bluetooth]# connect 17:13:00:00:8A:04 Attempting to connect to 17:13:00:00:8A:04 [CHG] Device 17:13:00:00:8A:04 Connected: yes Connection successful 3. Is QTrobot SDK open source? QTrobot SDK software is not licensed as open source. For the time being, we do not have any plan to make them open source. But these are just few software components which implement the required basic ROS interfaces such as motor control, gesture, speech, emotion. The main components (implemented as ROS node) are: qt_motor including motor_controller and gesture_controller plugins implement QTrobot motor joints control and QTrobot gesture record/play functionalities. qt_robot_interface implements speech, audio, basic behavior and setting functionalities. You can always open a new issue/feature request for those interfaces and we do our best to fix/implement them. The rest of the codes including all demo application are open source and you are more than welcome to contribute to those. 4. How to reboot QTrobot If you reboot/poweroff the QTRP, it also reboots/poweroff the QTPC but not the vice versa! This is actually useful for developers to update/restart the QTPC without needing to reboot the whole QTrobot.","title":"Miscellaneous"},{"location":"FAQ/Miscellaneous/#1-can-i-use-c-to-program-qtrobot","text":"QTrobot APIs are based on the most popular software framework in robotic, ROS \u2013 a publish/subscribe middleware. ROS by default supports C++ and Python but other languages are covered via different open-source and well-maintained client libraries. Indeed you can use C# to program QTrobot. You can refer to ros-sharp open-source library which leverages websockets for underlying communication and cover all QTrobot APIs. It has good integration with Unity3D too. We have also developed different Android apps using websockets in Jscript and using native Java APIs for our QTrobot.","title":"1. Can I use C# to program QTrobot?"},{"location":"FAQ/Miscellaneous/#2-how-to-connect-bluetooth-mousekeyboard-to-qtrobot","text":"This tutorial explain how to pair and connect a Bluetooth mouse/keyboard to QTrobot QTPC pc via terminal/ssh. The same procedure works also for QTRP. Notice : for QTRP you may need to run the bluetoothctl with sudo ! [Step 1] SSH to QTPC Connect to the QTrobot Wi-Fi and ssh into QTPC: ``` ssh qtrobot @192.168.100.2 ``` [Step 2] launch bluetoothctl qtrobot@QTPC:~$ bluetoothctl [NEW] Controller F8:63:3F:40:61:B2 QTPC [default] [Step 3] turn bluetooth power on and register the agent [bluetooth]# power on Changing power on succeeded [bluetooth]# agent on Agent registered [bluetooth]# default-agent Default agent request successful [Step 4] scan and pair the bluetooth device To scan the bluettoth devices: ``` [ bluetooth ] # scan on Discovery started [ CHG ] Controller F8 : 63 : 3 F : 40 : 61 : B2 Discovering : yes [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Name : Bluetooth 3.0 Keyboard ... ``` in my case Device 17:13:00:00:8A:04 Name: Bluetooth 3.0 Keyboard is what I am looking for. Now to pair the bluetooth device: ``` [ bluetooth ] # pair 17 : 13 : 00 : 00 : 8 A : 04 Attempting to pair with 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Connected : yes [ agent ] PIN code : xxxx ... [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Paired : yes Pairing successful ``` Some keyboards require to type a PIN code for pairing. Just type the code using your bluetooth keyboard . You do not see anything on the terminal. The code is sent directly by your keyboard while you are typing it. [Step 5] trust and connect to the device trust the device: [bluetooth]# trust 17:13:00:00:8A:04 [CHG] Device 17:13:00:00:8A:04 Trusted: yes and finally connect to the bluettoh/keyboard mouse: [bluetooth]# connect 17:13:00:00:8A:04 Attempting to connect to 17:13:00:00:8A:04 [CHG] Device 17:13:00:00:8A:04 Connected: yes Connection successful","title":"2. How to connect Bluetooth mouse/keyboard to QTrobot?"},{"location":"FAQ/Miscellaneous/#3-is-qtrobot-sdk-open-source","text":"QTrobot SDK software is not licensed as open source. For the time being, we do not have any plan to make them open source. But these are just few software components which implement the required basic ROS interfaces such as motor control, gesture, speech, emotion. The main components (implemented as ROS node) are: qt_motor including motor_controller and gesture_controller plugins implement QTrobot motor joints control and QTrobot gesture record/play functionalities. qt_robot_interface implements speech, audio, basic behavior and setting functionalities. You can always open a new issue/feature request for those interfaces and we do our best to fix/implement them. The rest of the codes including all demo application are open source and you are more than welcome to contribute to those.","title":"3. Is QTrobot SDK open source?"},{"location":"FAQ/Miscellaneous/#4-how-to-reboot-qtrobot","text":"If you reboot/poweroff the QTRP, it also reboots/poweroff the QTPC but not the vice versa! This is actually useful for developers to update/restart the QTPC without needing to reboot the whole QTrobot.","title":"4. How to reboot QTrobot"},{"location":"FAQ/Motors/","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1. Does the QT Motor interface implement joint limits? The short answer is Yes . QTrobot motor interface sets all joint, torque limits of each motor independently at the startup. Each motor separately does the joint limit check on each position command that it receives. NOTICE : This does not mean that the QTrobot has self collision awareness when moving its joints! 2. Gesture kinematic diagram/QTrobot URDF Gesture files the gesture are simply xml files containing robots joint waypoints and their timestamps. They are located under ~/robot/data/gestures/ folder in QTRP (head) board. here is an example: <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\" ?> <gesture> <name> QT/bye </name> <parts> <part> right_arm </part> <part> left_arm </part> </parts> <duration> 5.54 </duration> <waypoints count= \"97\" > <point time= \"1558971402671926152\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -62.90 </LeftShoulderRoll> <RightElbowRoll> -31.90 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -59.30 </RightShoulderRoll> </point> <point time= \"1558971402704626777\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -64.80 </LeftShoulderRoll> <RightElbowRoll> -30.60 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -60.60 </RightShoulderRoll> </point> ... QTrobot Joints configuration in QTRP, under /opt/ros/kinetic/share/qt_motor/ there are motor controller config files: qtrobot-hardware.yaml : contains each motor configuration such as joints limit, PID, calibration offsets, etc. qtrobot-controller.yaml : contains QTrobot controller interface configuration such as joint_state , gesture , etc. These interface are implemented using ROS control system. QTRobot URDF You can find the QTrobot URDF fils in the following links: - qtrobot.pdf - qtrobot_urdf.zip You can also load them in Gazebo to visualize joint configurations and test them: $ roslaunch urdf_tutorial display.launch model:=qtrobot.urdf you need to copy the qtrobot.urdf file to the proper folder in your PC (where you have installed Gazebo). Check the display.launch to find the path.","title":"Motors"},{"location":"FAQ/Motors/#1-does-the-qt-motor-interface-implement-joint-limits","text":"The short answer is Yes . QTrobot motor interface sets all joint, torque limits of each motor independently at the startup. Each motor separately does the joint limit check on each position command that it receives. NOTICE : This does not mean that the QTrobot has self collision awareness when moving its joints!","title":"1. Does the QT Motor interface implement joint limits?"},{"location":"FAQ/Motors/#2-gesture-kinematic-diagramqtrobot-urdf","text":"","title":"2. Gesture kinematic diagram/QTrobot URDF"},{"location":"FAQ/Motors/#gesture-files","text":"the gesture are simply xml files containing robots joint waypoints and their timestamps. They are located under ~/robot/data/gestures/ folder in QTRP (head) board. here is an example: <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\" ?> <gesture> <name> QT/bye </name> <parts> <part> right_arm </part> <part> left_arm </part> </parts> <duration> 5.54 </duration> <waypoints count= \"97\" > <point time= \"1558971402671926152\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -62.90 </LeftShoulderRoll> <RightElbowRoll> -31.90 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -59.30 </RightShoulderRoll> </point> <point time= \"1558971402704626777\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -64.80 </LeftShoulderRoll> <RightElbowRoll> -30.60 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -60.60 </RightShoulderRoll> </point> ...","title":"Gesture files"},{"location":"FAQ/Motors/#qtrobot-joints-configuration","text":"in QTRP, under /opt/ros/kinetic/share/qt_motor/ there are motor controller config files: qtrobot-hardware.yaml : contains each motor configuration such as joints limit, PID, calibration offsets, etc. qtrobot-controller.yaml : contains QTrobot controller interface configuration such as joint_state , gesture , etc. These interface are implemented using ROS control system.","title":"QTrobot Joints configuration"},{"location":"FAQ/Motors/#qtrobot-urdf","text":"You can find the QTrobot URDF fils in the following links: - qtrobot.pdf - qtrobot_urdf.zip You can also load them in Gazebo to visualize joint configurations and test them: $ roslaunch urdf_tutorial display.launch model:=qtrobot.urdf you need to copy the qtrobot.urdf file to the proper folder in your PC (where you have installed Gazebo). Check the display.launch to find the path.","title":"QTRobot URDF"},{"location":"demos/qt_emotion_game/","text":"Emotion Game Demo About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_emotion_app/qt_emotion_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_emotion_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_emotion_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_emotion_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { inState = true ; ROS_INFO ( \"Play.entry()\" ); ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language to English std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } ROS_INFO_STREAM ( \"emotion: \" << shown_lables ); if ( shown_lables == \"happy\" ) { talk ( \"emotion_recognition_001\" ); serviceHelper . showEmotionPlayGesture ( \"QT/happy\" , \"QT/happy\" , 0.5 , true ); } else if ( shown_lables == \"sad\" ) { talk ( \"emotion_recognition_002\" ); serviceHelper . showEmotionPlayGesture ( \"QT/sad\" , \"QT/sad\" , 1.0 , true ); talk ( \"emotion_recognition_003\" ); } else if ( shown_lables == \"angry\" ) { talk ( \"emotion_recognition_004\" ); serviceHelper . showEmotionPlayGesture ( \"QT/angry\" , \"QT/angry\" , 0.5 , true ); talk ( \"emotion_recognition_005\" ); serviceHelper . showEmotionPlayGesture ( \"QT/breathing_exercise\" , \"QT/breathing_exercise\" , 0.5 , true ); } else if ( shown_lables == \"disgusted\" ) { talk ( \"emotion_recognition_006\" ); serviceHelper . showEmotion ( \"QT/disgusted\" ); talk ( \"emotion_recognition_007\" ); } else if ( shown_lables == \"surprised\" ) { talk ( \"emotion_recognition_008\" ); serviceHelper . showEmotionPlayGesture ( \"QT/surprise\" , \"QT/surprise\" , 2.0 , true ); talk ( \"emotion_recognition_009\" ); } } virtual void exit () { ROS_INFO ( \"Play.exit()\" ); serviceHelper . homeAll (); ros :: Duration ( 1.0 ). sleep (); inState = false ; if ( ! with_audio_files ) { // set speech language to English if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set back speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); if ( ! inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool inState ; bool interrupted ; std :: string shown_lables ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTEmotionApp :: QTEmotionApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_emotion_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_emotion_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_emotion_app/suspend\", &QTEmotionApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTEmotionApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTEmotionApp :: timerCallback , this ); } QTEmotionApp ::~ QTEmotionApp () { timer . stop (); } void QTEmotionApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTEmotionApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTEmotionApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 3 ) lables = \"surprised\" ; else if ( objects -> data [ i ] == 4 ) lables = \"disgusted\" ; else if ( objects -> data [ i ] == 5 ) lables = \"angry\" ; else if ( objects -> data [ i ] == 6 ) lables = \"sad\" ; else if ( objects -> data [ i ] == 7 ) lables = \"happy\" ; } // call related callbacks if ( lables . size ()) { dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"**Emotion Game Demo**"},{"location":"demos/qt_emotion_game/#emotion-game-demo","text":"About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Emotion Game Demo"},{"location":"demos/qt_emotion_game/#video","text":"","title":"Video"},{"location":"demos/qt_emotion_game/#code","text":"Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_emotion_app/qt_emotion_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_emotion_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_emotion_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_emotion_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { inState = true ; ROS_INFO ( \"Play.entry()\" ); ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language to English std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } ROS_INFO_STREAM ( \"emotion: \" << shown_lables ); if ( shown_lables == \"happy\" ) { talk ( \"emotion_recognition_001\" ); serviceHelper . showEmotionPlayGesture ( \"QT/happy\" , \"QT/happy\" , 0.5 , true ); } else if ( shown_lables == \"sad\" ) { talk ( \"emotion_recognition_002\" ); serviceHelper . showEmotionPlayGesture ( \"QT/sad\" , \"QT/sad\" , 1.0 , true ); talk ( \"emotion_recognition_003\" ); } else if ( shown_lables == \"angry\" ) { talk ( \"emotion_recognition_004\" ); serviceHelper . showEmotionPlayGesture ( \"QT/angry\" , \"QT/angry\" , 0.5 , true ); talk ( \"emotion_recognition_005\" ); serviceHelper . showEmotionPlayGesture ( \"QT/breathing_exercise\" , \"QT/breathing_exercise\" , 0.5 , true ); } else if ( shown_lables == \"disgusted\" ) { talk ( \"emotion_recognition_006\" ); serviceHelper . showEmotion ( \"QT/disgusted\" ); talk ( \"emotion_recognition_007\" ); } else if ( shown_lables == \"surprised\" ) { talk ( \"emotion_recognition_008\" ); serviceHelper . showEmotionPlayGesture ( \"QT/surprise\" , \"QT/surprise\" , 2.0 , true ); talk ( \"emotion_recognition_009\" ); } } virtual void exit () { ROS_INFO ( \"Play.exit()\" ); serviceHelper . homeAll (); ros :: Duration ( 1.0 ). sleep (); inState = false ; if ( ! with_audio_files ) { // set speech language to English if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set back speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); if ( ! inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool inState ; bool interrupted ; std :: string shown_lables ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTEmotionApp :: QTEmotionApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_emotion_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_emotion_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_emotion_app/suspend\", &QTEmotionApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTEmotionApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTEmotionApp :: timerCallback , this ); } QTEmotionApp ::~ QTEmotionApp () { timer . stop (); } void QTEmotionApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTEmotionApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTEmotionApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 3 ) lables = \"surprised\" ; else if ( objects -> data [ i ] == 4 ) lables = \"disgusted\" ; else if ( objects -> data [ i ] == 5 ) lables = \"angry\" ; else if ( objects -> data [ i ] == 6 ) lables = \"sad\" ; else if ( objects -> data [ i ] == 7 ) lables = \"happy\" ; } // call related callbacks if ( lables . size ()) { dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"Code"},{"location":"demos/qt_face_recognition/","text":"Face recognition, Age & Gender About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #!/usr/bin/env python from __future__ import print_function # import sys import rospy import cv2 import threading from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError from qt_nuitrack_app.msg import Faces , FaceInfo class image_converter : faces = None faces_time = None def __init__ ( self ): self . lock = threading . Lock () self . bridge = CvBridge () self . image_pub = rospy . Publisher ( \"/face_recognition/out\" , Image , queue_size = 1 ) self . image_sub = rospy . Subscriber ( \"/camera/color/image_raw\" , Image , self . image_callback ) self . face_sub = rospy . Subscriber ( \"/qt_nuitrack_app/faces\" , Faces , self . face_callback ) def face_callback ( self , data ): # print(\"face_callback\") self . lock . acquire () self . faces = data . faces self . faces_time = rospy . Time . now () self . lock . release () def image_callback ( self , data ): try : cv_image = self . bridge . imgmsg_to_cv2 ( data , \"bgr8\" ) except CvBridgeError as e : print ( e ) ( rows , cols , channels ) = cv_image . shape self . lock . acquire () new_faces = self . faces new_faces_time = self . faces_time self . lock . release () if new_faces and ( rospy . Time . now () - new_faces_time ) < rospy . Duration ( 5.0 ): for face in new_faces : rect = face . rectangle cv2 . rectangle ( cv_image , ( int ( rect [ 0 ] * cols ), int ( rect [ 1 ] * rows )), ( int ( rect [ 0 ] * cols + rect [ 2 ] * cols ), int ( rect [ 1 ] * rows + rect [ 3 ] * rows )), ( 0 , 255 , 0 ), 2 ) x = int ( rect [ 0 ] * cols ) y = int ( rect [ 1 ] * rows ) w = int ( rect [ 2 ] * cols ) h = int ( rect [ 3 ] * rows ) #cv2.putText(cv_image, \"Gender:\", (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), lineType=cv2.LINE_AA) cv2 . putText ( cv_image , \"Gender: %s \" % face . gender , ( x , y + h + 20 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . putText ( cv_image , \"Age: %d \" % face . age_years , ( x , y + h + 40 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) # Neutral cv2 . putText ( cv_image , \"Neutral:\" , ( x , y + h + 60 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + int ( face . emotion_neutral * 100 ), y + h + 10 + 50 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + 100 , y + h + 10 + 50 ), ( 255 , 255 , 255 ), 1 ) # Angry cv2 . putText ( cv_image , \"Angry:\" , ( x , y + h + 80 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + int ( face . emotion_angry * 100 ), y + h + 10 + 70 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + 100 , y + h + 10 + 70 ), ( 255 , 255 , 255 ), 1 ) # Happy cv2 . putText ( cv_image , \"Happy:\" , ( x , y + h + 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + int ( face . emotion_happy * 100 ), y + h + 10 + 90 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + 100 , y + h + 10 + 90 ), ( 255 , 255 , 255 ), 1 ) cv2 . putText ( cv_image , \"Surprise:\" , ( x , y + h + 120 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + int ( face . emotion_surprise * 100 ), y + h + 10 + 110 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + 100 , y + h + 10 + 110 ), ( 255 , 255 , 255 ), 1 ) try : self . image_pub . publish ( self . bridge . cv2_to_imgmsg ( cv_image , \"bgr8\" )) except CvBridgeError as e : print ( e ) if __name__ == '__main__' : rospy . init_node ( 'qt_face_recognition' , anonymous = True ) ic = image_converter () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" )","title":"**Face recognition, Age & Gender**"},{"location":"demos/qt_face_recognition/#face-recognition-age-gender","text":"About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Face recognition, Age &amp; Gender"},{"location":"demos/qt_face_recognition/#video","text":"","title":"Video"},{"location":"demos/qt_face_recognition/#code","text":"Check code here #!/usr/bin/env python from __future__ import print_function # import sys import rospy import cv2 import threading from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError from qt_nuitrack_app.msg import Faces , FaceInfo class image_converter : faces = None faces_time = None def __init__ ( self ): self . lock = threading . Lock () self . bridge = CvBridge () self . image_pub = rospy . Publisher ( \"/face_recognition/out\" , Image , queue_size = 1 ) self . image_sub = rospy . Subscriber ( \"/camera/color/image_raw\" , Image , self . image_callback ) self . face_sub = rospy . Subscriber ( \"/qt_nuitrack_app/faces\" , Faces , self . face_callback ) def face_callback ( self , data ): # print(\"face_callback\") self . lock . acquire () self . faces = data . faces self . faces_time = rospy . Time . now () self . lock . release () def image_callback ( self , data ): try : cv_image = self . bridge . imgmsg_to_cv2 ( data , \"bgr8\" ) except CvBridgeError as e : print ( e ) ( rows , cols , channels ) = cv_image . shape self . lock . acquire () new_faces = self . faces new_faces_time = self . faces_time self . lock . release () if new_faces and ( rospy . Time . now () - new_faces_time ) < rospy . Duration ( 5.0 ): for face in new_faces : rect = face . rectangle cv2 . rectangle ( cv_image , ( int ( rect [ 0 ] * cols ), int ( rect [ 1 ] * rows )), ( int ( rect [ 0 ] * cols + rect [ 2 ] * cols ), int ( rect [ 1 ] * rows + rect [ 3 ] * rows )), ( 0 , 255 , 0 ), 2 ) x = int ( rect [ 0 ] * cols ) y = int ( rect [ 1 ] * rows ) w = int ( rect [ 2 ] * cols ) h = int ( rect [ 3 ] * rows ) #cv2.putText(cv_image, \"Gender:\", (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), lineType=cv2.LINE_AA) cv2 . putText ( cv_image , \"Gender: %s \" % face . gender , ( x , y + h + 20 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . putText ( cv_image , \"Age: %d \" % face . age_years , ( x , y + h + 40 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) # Neutral cv2 . putText ( cv_image , \"Neutral:\" , ( x , y + h + 60 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + int ( face . emotion_neutral * 100 ), y + h + 10 + 50 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + 100 , y + h + 10 + 50 ), ( 255 , 255 , 255 ), 1 ) # Angry cv2 . putText ( cv_image , \"Angry:\" , ( x , y + h + 80 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + int ( face . emotion_angry * 100 ), y + h + 10 + 70 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + 100 , y + h + 10 + 70 ), ( 255 , 255 , 255 ), 1 ) # Happy cv2 . putText ( cv_image , \"Happy:\" , ( x , y + h + 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + int ( face . emotion_happy * 100 ), y + h + 10 + 90 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + 100 , y + h + 10 + 90 ), ( 255 , 255 , 255 ), 1 ) cv2 . putText ( cv_image , \"Surprise:\" , ( x , y + h + 120 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + int ( face . emotion_surprise * 100 ), y + h + 10 + 110 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + 100 , y + h + 10 + 110 ), ( 255 , 255 , 255 ), 1 ) try : self . image_pub . publish ( self . bridge . cv2_to_imgmsg ( cv_image , \"bgr8\" )) except CvBridgeError as e : print ( e ) if __name__ == '__main__' : rospy . init_node ( 'qt_face_recognition' , anonymous = True ) ic = image_converter () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" )","title":"Code"},{"location":"demos/qt_gesture_recognition/","text":"Gesture Game Demo About This is gesture game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #include \"qt_gesturegame_app/qt_gesturegame_app.h\" #include \"qt_idle_app/suspend.h\" #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'U' , 'R' , 'L' }; static int current_user_id = -1 ; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_gesturegame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_gesturegame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = -1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 3 )]; } ROS_INFO_STREAM ( \"Show \" << robotMemory ); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'U' ) serviceHelper . playGesture ( \"QT/up_right\" , 1.2 ); else if ( robotMemory [ i ] == 'R' ) serviceHelper . playGesture ( \"QT/swipe_right\" , 1.2 ); else if ( robotMemory [ i ] == 'L' ) serviceHelper . playGesture ( \"QT/swipe_left\" , 1.2 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } //ros::Duration(2.0).sleep(); // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_gesture . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_gesture . size ()) { ROS_INFO_STREAM ( \"shown \" << shown_gesture ); read = true ; if ( shown_gesture == \"SWIPE UP\" ) { playerMemory += \"U\" ; talk ( \"memory_game_015\" ); } else if ( shown_gesture == \"SWIPE RIGHT\" ) { playerMemory += \"R\" ; talk ( \"memory_game_007\" ); } else if ( shown_gesture == \"SWIPE LEFT\" ) { playerMemory += \"L\" ; talk ( \"memory_game_008\" ); } else { read = false ; } } mutexLables . unlock (); } //ros::Duration(0.5).sleep(); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != -1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onGestureDetected ( const std :: string & gesture , int id ) { if (( gesture == \"SWIPE DOWN\" ) || ( gesture == \"WAVING\" )) return ; if ( id != current_user_id ) return ; mutexLables . lock (); shown_gesture = gesture ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_gesture ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onGestureDetected ( const std :: string & gesture , int id ) { // TODO check if both Q & T are shown if ( inState && gesture == \"SWIPE UP\" ) { /* static int gesture_count = 0; static ros::Time prev_gesture_time = ros::Time::now(); if((ros::Time::now()-prev_gesture_time).toSec() > 3.0) { gesture_count = 0; prev_gesture_time = ros::Time::now(); return; } if(gesture_count >= 1) { gesture_count = 0; current_user_id = id; rfsm.sendEvent(\"e_start_game\"); return; } if(gesture_count == 0) { gesture_count++; prev_gesture_time = ros::Time::now(); return; } gesture_count++; */ current_user_id = id ; rfsm . sendEvent ( \"e_start_game\" ); } } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper & serviceHelper ; }; QTGestureGameApp :: QTGestureGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_gesturegame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_gesturegame_app/suspend\", &QTGestureGameApp::suspendCB, this); subGestures = nh . subscribe ( \"/qt_nuitrack_app/gestures\" , 10 , & QTGestureGameApp :: gestureSubCB , this ); // set speech language to English if ( ! serviceHelper . speechConfig ( \"en-US\" )) { ROS_ERROR_STREAM ( \"Cannot set speech language to 'en-US'\" ); ros :: shutdown (); } // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } fsmTimer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTGestureGameApp :: fsmTimerCallback , this ); } QTGestureGameApp ::~ QTGestureGameApp () { fsmTimer . stop (); } void QTGestureGameApp :: fsmTimerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } /* void QTGestureGameApp::onNewGestures(const tdv::nuitrack::GestureData::Ptr gesture_data) { //ROS_INFO_STREAM(\"onNewGestures...\"); const std::vector<tdv::nuitrack::Gesture> gestures = gesture_data->getGestures(); for( const tdv::nuitrack::Gesture& gesture : gestures ){ ROS_INFO_STREAM(gesture.userId << \": \" << type2string(gesture.type)); if(gesture.type == GestureType::GESTURE_SWIPE_UP || gesture.type == GestureType::GESTURE_SWIPE_RIGHT || gesture.type == GestureType::GESTURE_SWIPE_LEFT) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onGestureDetected(type2string(gesture.type)); dynamic_cast<PlayStateCB*>(playStateCB)->onGestureDetected(type2string(gesture.type)); return; } } } */ bool QTGestureGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTGestureGameApp :: gestureSubCB ( const qt_nuitrack_app :: Gestures :: ConstPtr & gestures ) { for ( int i = 0 ; i < gestures -> gestures . size (); i ++ ) { ROS_INFO_STREAM ( \"gesture: \" << gestures -> gestures [ i ]. name << \", id: \" << gestures -> gestures [ i ]. id ); dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); } } /* void QTGestureGameApp::objectsSubCB(const std_msgs::Float32MultiArray::ConstPtr& objects) { if(objects->data.size() <= 0 ) return; std::string lables; for(size_t i=0; i<objects->data.size(); i+=12) { if(objects->data[i] == 1) lables.push_back('Q'); else if(objects->data[i] == 2) lables.push_back('T'); } // call related callbacks if(lables.size()) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onObjectsDetected(lables); dynamic_cast<PlayStateCB*>(playStateCB)->onObjectsDetected(lables); } } */","title":"**Gesture Game Demo**"},{"location":"demos/qt_gesture_recognition/#gesture-game-demo","text":"About This is gesture game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Gesture Game Demo"},{"location":"demos/qt_gesture_recognition/#video","text":"","title":"Video"},{"location":"demos/qt_gesture_recognition/#code","text":"Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #include \"qt_gesturegame_app/qt_gesturegame_app.h\" #include \"qt_idle_app/suspend.h\" #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'U' , 'R' , 'L' }; static int current_user_id = -1 ; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_gesturegame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_gesturegame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = -1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 3 )]; } ROS_INFO_STREAM ( \"Show \" << robotMemory ); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'U' ) serviceHelper . playGesture ( \"QT/up_right\" , 1.2 ); else if ( robotMemory [ i ] == 'R' ) serviceHelper . playGesture ( \"QT/swipe_right\" , 1.2 ); else if ( robotMemory [ i ] == 'L' ) serviceHelper . playGesture ( \"QT/swipe_left\" , 1.2 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } //ros::Duration(2.0).sleep(); // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_gesture . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_gesture . size ()) { ROS_INFO_STREAM ( \"shown \" << shown_gesture ); read = true ; if ( shown_gesture == \"SWIPE UP\" ) { playerMemory += \"U\" ; talk ( \"memory_game_015\" ); } else if ( shown_gesture == \"SWIPE RIGHT\" ) { playerMemory += \"R\" ; talk ( \"memory_game_007\" ); } else if ( shown_gesture == \"SWIPE LEFT\" ) { playerMemory += \"L\" ; talk ( \"memory_game_008\" ); } else { read = false ; } } mutexLables . unlock (); } //ros::Duration(0.5).sleep(); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != -1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onGestureDetected ( const std :: string & gesture , int id ) { if (( gesture == \"SWIPE DOWN\" ) || ( gesture == \"WAVING\" )) return ; if ( id != current_user_id ) return ; mutexLables . lock (); shown_gesture = gesture ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_gesture ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onGestureDetected ( const std :: string & gesture , int id ) { // TODO check if both Q & T are shown if ( inState && gesture == \"SWIPE UP\" ) { /* static int gesture_count = 0; static ros::Time prev_gesture_time = ros::Time::now(); if((ros::Time::now()-prev_gesture_time).toSec() > 3.0) { gesture_count = 0; prev_gesture_time = ros::Time::now(); return; } if(gesture_count >= 1) { gesture_count = 0; current_user_id = id; rfsm.sendEvent(\"e_start_game\"); return; } if(gesture_count == 0) { gesture_count++; prev_gesture_time = ros::Time::now(); return; } gesture_count++; */ current_user_id = id ; rfsm . sendEvent ( \"e_start_game\" ); } } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper & serviceHelper ; }; QTGestureGameApp :: QTGestureGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_gesturegame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_gesturegame_app/suspend\", &QTGestureGameApp::suspendCB, this); subGestures = nh . subscribe ( \"/qt_nuitrack_app/gestures\" , 10 , & QTGestureGameApp :: gestureSubCB , this ); // set speech language to English if ( ! serviceHelper . speechConfig ( \"en-US\" )) { ROS_ERROR_STREAM ( \"Cannot set speech language to 'en-US'\" ); ros :: shutdown (); } // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } fsmTimer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTGestureGameApp :: fsmTimerCallback , this ); } QTGestureGameApp ::~ QTGestureGameApp () { fsmTimer . stop (); } void QTGestureGameApp :: fsmTimerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } /* void QTGestureGameApp::onNewGestures(const tdv::nuitrack::GestureData::Ptr gesture_data) { //ROS_INFO_STREAM(\"onNewGestures...\"); const std::vector<tdv::nuitrack::Gesture> gestures = gesture_data->getGestures(); for( const tdv::nuitrack::Gesture& gesture : gestures ){ ROS_INFO_STREAM(gesture.userId << \": \" << type2string(gesture.type)); if(gesture.type == GestureType::GESTURE_SWIPE_UP || gesture.type == GestureType::GESTURE_SWIPE_RIGHT || gesture.type == GestureType::GESTURE_SWIPE_LEFT) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onGestureDetected(type2string(gesture.type)); dynamic_cast<PlayStateCB*>(playStateCB)->onGestureDetected(type2string(gesture.type)); return; } } } */ bool QTGestureGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTGestureGameApp :: gestureSubCB ( const qt_nuitrack_app :: Gestures :: ConstPtr & gestures ) { for ( int i = 0 ; i < gestures -> gestures . size (); i ++ ) { ROS_INFO_STREAM ( \"gesture: \" << gestures -> gestures [ i ]. name << \", id: \" << gestures -> gestures [ i ]. id ); dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); } } /* void QTGestureGameApp::objectsSubCB(const std_msgs::Float32MultiArray::ConstPtr& objects) { if(objects->data.size() <= 0 ) return; std::string lables; for(size_t i=0; i<objects->data.size(); i+=12) { if(objects->data[i] == 1) lables.push_back('Q'); else if(objects->data[i] == 2) lables.push_back('T'); } // call related callbacks if(lables.size()) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onObjectsDetected(lables); dynamic_cast<PlayStateCB*>(playStateCB)->onObjectsDetected(lables); } } */","title":"Code"},{"location":"demos/qt_memory_game/","text":"Memory Game Demo About This is memory game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_memgame_app/qt_memgame_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'Q' , 'T' }; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_memgame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_memgame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_memgame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = -1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 2 )]; } //ROS_INFO_STREAM(\"Show \"<<robotMemory); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'Q' ) serviceHelper . playGesture ( \"QT/show_right\" , 1.0 ); else if ( robotMemory [ i ] == 'T' ) serviceHelper . playGesture ( \"QT/show_left\" , 1.0 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_lables . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_lables . size ()) { //ROS_INFO_STREAM(\"shown \"<< shown_lables[0]); talk (( shown_lables [ 0 ] == 'Q' ) ? \"memory_game_007\" : \"memory_game_008\" ); playerMemory += shown_lables [ 0 ]; read = true ; } mutexLables . unlock (); } ros :: Duration ( 0.5 ). sleep (); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != -1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_lables ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onObjectsDetected ( const std :: string & lables ) { // TODO check if both Q & T are shown if ( inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTMemGameApp :: QTMemGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_memgame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_memgame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_memgame_app/suspend\", &QTMemGameApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTMemGameApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTMemGameApp :: timerCallback , this ); } QTMemGameApp ::~ QTMemGameApp () { timer . stop (); } void QTMemGameApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTMemGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTMemGameApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 1 ) lables . push_back ( 'Q' ); else if ( objects -> data [ i ] == 2 ) lables . push_back ( 'T' ); } // call related callbacks if ( lables . size ()) { dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onObjectsDetected ( lables ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"**Memory Game Demo**"},{"location":"demos/qt_memory_game/#memory-game-demo","text":"About This is memory game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Memory Game Demo"},{"location":"demos/qt_memory_game/#video","text":"","title":"Video"},{"location":"demos/qt_memory_game/#code","text":"Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_memgame_app/qt_memgame_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'Q' , 'T' }; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_memgame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_memgame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_memgame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = -1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 2 )]; } //ROS_INFO_STREAM(\"Show \"<<robotMemory); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'Q' ) serviceHelper . playGesture ( \"QT/show_right\" , 1.0 ); else if ( robotMemory [ i ] == 'T' ) serviceHelper . playGesture ( \"QT/show_left\" , 1.0 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_lables . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_lables . size ()) { //ROS_INFO_STREAM(\"shown \"<< shown_lables[0]); talk (( shown_lables [ 0 ] == 'Q' ) ? \"memory_game_007\" : \"memory_game_008\" ); playerMemory += shown_lables [ 0 ]; read = true ; } mutexLables . unlock (); } ros :: Duration ( 0.5 ). sleep (); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != -1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_lables ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onObjectsDetected ( const std :: string & lables ) { // TODO check if both Q & T are shown if ( inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTMemGameApp :: QTMemGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_memgame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_memgame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_memgame_app/suspend\", &QTMemGameApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTMemGameApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTMemGameApp :: timerCallback , this ); } QTMemGameApp ::~ QTMemGameApp () { timer . stop (); } void QTMemGameApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTMemGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTMemGameApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 1 ) lables . push_back ( 'Q' ); else if ( objects -> data [ i ] == 2 ) lables . push_back ( 'T' ); } // call related callbacks if ( lables . size ()) { dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onObjectsDetected ( lables ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"Code"},{"location":"demos/qt_microphone_detection/","text":"Microphone Voice Activity & Direction Detection About This is microphone voice activity and direction detection demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Setup NOTICE You must run this demo on QTRP (head) How to set it up? This demo needs some prerequisites: sudo apt-get update sudo pip install pyusb click OR sudo apt-get update sudo `which pip` install pyusb click PyUSB needs root privileges, if you run the script without root you will see something like this: usb.core.USBError: [Errno 13] Access denied (insufficient permissions) To fix this error we need to set up a udev rule file for the microphone to be able to access it with normal user. * Create a udev rules file: ACTION==\"add\", SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2886\", ATTRS{idProduct}==\"0018\", MODE=\"660\", GROUP=\"plugdev\" Create this file in folder /etc/udev/rules.d/ . For example usual structure of the file name can be Number-Name.rules Add the user to the plugdev group: adduser username plugdev Reload udev system to see your changes: sudo udevadm control --reload sudo udevadm trigger Reboot QTrobot: sudo reboot To run the demo go to qt_microphone_detection folder and run voice_direction python script: cd qt_microphone_detection/ python voice_direction.py Now you can speak or sing around the QTrobot and he will follow your voice. Code Check code here from tuning import Tuning import usb.core import usb.util import time import sys import rospy from std_msgs.msg import Float64MultiArray # microphone orientation MICROPHONE_ANGLE_OFFSET = 90 if __name__ == '__main__' : rospy . init_node ( 'voice_direction' , anonymous = True ) dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if not dev : rospy . logerr ( \"Cannot establish connection!\" ) sys . exit () head_pub = rospy . Publisher ( '/qt_robot/head_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher connections wtime_begin = rospy . get_time () while ( head_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) # centering the head href = Float64MultiArray ( data = [ 0 , 0 ]) head_pub . publish ( href ) rospy . sleep ( 1 ) microphone = Tuning ( dev ) while not rospy . is_shutdown (): if not microphone . is_voice (): continue mic = abs ( microphone . direction - 180 ) angle = mic - MICROPHONE_ANGLE_OFFSET rospy . loginfo ( \"mic: %d , head: %d \" % ( mic , angle )) href . data = [ angle , 0 ] head_pub . publish ( href ) rospy . loginfo ( \"shutdowned!\" )","title":"**Microphone Voice Activity & Direction Detection**"},{"location":"demos/qt_microphone_detection/#microphone-voice-activity-direction-detection","text":"About This is microphone voice activity and direction detection demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Microphone Voice Activity &amp; Direction Detection"},{"location":"demos/qt_microphone_detection/#video","text":"","title":"Video"},{"location":"demos/qt_microphone_detection/#setup","text":"NOTICE You must run this demo on QTRP (head) How to set it up? This demo needs some prerequisites: sudo apt-get update sudo pip install pyusb click OR sudo apt-get update sudo `which pip` install pyusb click PyUSB needs root privileges, if you run the script without root you will see something like this: usb.core.USBError: [Errno 13] Access denied (insufficient permissions) To fix this error we need to set up a udev rule file for the microphone to be able to access it with normal user. * Create a udev rules file: ACTION==\"add\", SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2886\", ATTRS{idProduct}==\"0018\", MODE=\"660\", GROUP=\"plugdev\" Create this file in folder /etc/udev/rules.d/ . For example usual structure of the file name can be Number-Name.rules Add the user to the plugdev group: adduser username plugdev Reload udev system to see your changes: sudo udevadm control --reload sudo udevadm trigger Reboot QTrobot: sudo reboot To run the demo go to qt_microphone_detection folder and run voice_direction python script: cd qt_microphone_detection/ python voice_direction.py Now you can speak or sing around the QTrobot and he will follow your voice.","title":"Setup"},{"location":"demos/qt_microphone_detection/#code","text":"Check code here from tuning import Tuning import usb.core import usb.util import time import sys import rospy from std_msgs.msg import Float64MultiArray # microphone orientation MICROPHONE_ANGLE_OFFSET = 90 if __name__ == '__main__' : rospy . init_node ( 'voice_direction' , anonymous = True ) dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if not dev : rospy . logerr ( \"Cannot establish connection!\" ) sys . exit () head_pub = rospy . Publisher ( '/qt_robot/head_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher connections wtime_begin = rospy . get_time () while ( head_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) # centering the head href = Float64MultiArray ( data = [ 0 , 0 ]) head_pub . publish ( href ) rospy . sleep ( 1 ) microphone = Tuning ( dev ) while not rospy . is_shutdown (): if not microphone . is_voice (): continue mic = abs ( microphone . direction - 180 ) angle = mic - MICROPHONE_ANGLE_OFFSET rospy . loginfo ( \"mic: %d , head: %d \" % ( mic , angle )) href . data = [ angle , 0 ] head_pub . publish ( href ) rospy . loginfo ( \"shutdowned!\" )","title":"Code"},{"location":"demos/qt_microphone_interaction/","text":"Microphone Voice Interaction About This is microphone voice interaction demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Setup NOTICE You must read and fully understand Snips voice system before starting this Please read the documentation of snips to understand it https://console.snips.ai/login How to set it up? This demo is already installed on your QTrobot. you can find it under ~/robot/code/qt_app Snips already installed on QTRP (head) how to run the snip servers: sudo systemctl start snips-asr.service snips-audio-server.service snips-dialogue.service snips-hotword.service snips-nlu.service how to run qt_voice_app demo rosrun qt_voice_app qt_voice_app.py Voice commands Hey QT, show me your happy emotion Hey QT, play happy gesture Snips configuration file /etc/snips.toml Code Check code here #!/usr/bin/env python2 # -*- coding: utf-8 -*- import time import threading from hermes_python.hermes import Hermes import rospy from std_msgs.msg import String from qt_gesture_controller.srv import * from qt_motors_controller.srv import * from qt_robot_interface.srv import * talkText = rospy . ServiceProxy ( '/qt_robot/behavior/talkText' , behavior_talk_text ) gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) gestureList = rospy . ServiceProxy ( '/qt_robot/gesture/list' , gesture_list ) gestureSave = rospy . ServiceProxy ( '/qt_robot/gesture/save' , gesture_save ) setControlMode = rospy . ServiceProxy ( '/qt_robot/motors/setControlMode' , set_control_mode ) gestureRecord = rospy . ServiceProxy ( '/qt_robot/gesture/record' , gesture_record ) #gesturePlay_pub = rospy.Publisher('/qt_robot/gesture/play', String, queue_size=10) emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) action_thread = None def emotion_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in emotion_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"ShowEmotion\" : emotion_name = slot [ 0 ] . slot_value . value . value try : print ( \"Showing emotion %s \" % \"QT/ %s \" % emotion_name ) talkText ( \"This is my %s face.\" % emotion_name . replace ( \"_\" , \" \" )) emotionShow_pub . publish ( \"QT/ %s \" % emotion_name ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for showing emotion %s \" % emotion_name ) else : talkText ( \"I did not understand which emotion.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def play_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) # action code goes here... print ( \"in play_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"GastureName\" : gesture_name = slot [ 0 ] . slot_value . value . value try : res = gestureList () if gesture_name not in res . gestures : talkText ( \"I do not know this gesture. But you can always record your gesture. Just ask me, hey Q.T. . record, new gesture!\" ) else : if gesture_name != \"my\" : gesture_name = \"QT/\" + gesture_name print ( \"playing gesture ' %s '.\" % gesture_name ) res = gesturePlay ( gesture_name , 1.0 ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for playing gesture %s \" % gesture_name ) else : talkText ( \"I did not understand which gesture.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def record_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in record_intent_callback\" ) talkText ( \"You can start recording your gesture.\" ) res = gestureRecord ([ \"right_arm\" , \"left_arm\" ], True , 0 , 0 ) print ( \"done!\" ) def stop_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in stop_intent_callback\" ) res = gestureSave ( \"my\" , \"\" ) if not res . status : talkText ( \"I have a problem with recording the gesture\" ) setControlMode ([ \"right_arm\" , \"left_arm\" ], 1 ) talkText ( \"Gesture recorded!\" ) print ( \"done!\" ) def unknown_intent_callback ( hermes , intent_message ): talkText ( \"I do not know this. But you can ask me to play gesture or show emotions.\" ) print ( \"done\" ) def intent_received ( hermes , intent_message ): hermes . publish_end_session ( intent_message . session_id , None ) coming_intent = intent_message . intent . intent_name print ( intent_message . intent . confidence_score ) if intent_message . intent . confidence_score < 0.5 : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () return print if coming_intent == 'apaikan:Play' : action_thread = threading . Thread ( target = play_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Record' : action_thread = threading . Thread ( target = record_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Emotion' : action_thread = threading . Thread ( target = emotion_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Stop' : action_thread = threading . Thread ( target = stop_intent_callback , args = ( hermes , intent_message )) action_thread . start () else : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () def intent_not_recognized ( hermes , intent_nr_message ): print ( \"not recogniozed\" ) action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_nr_message )) action_thread . start () if __name__ == \"__main__\" : # call the relevant service rospy . init_node ( 'qt_voice_interface' , disable_signals = True ) with Hermes ( 'localhost:1883' ) as h : # h.subscribe_intent(\"apaikan:Play\", play_intent_callback) # h.subscribe_intent(\"apaikan:Record\", record_intent_callback) # h.subscribe_intent(\"apaikan:Stop\", stop_intent_callback) # h.subscribe_intent(\"apaikan:Emotion\", emotion_intent_callback) h . subscribe_intents ( intent_received ) h . subscribe_intent_not_recognized ( intent_not_recognized ) h . loop_forever () # async mode using #h.loop_start() rospy.spin() h.loop_stop()","title":"**Microphone Voice Interaction**"},{"location":"demos/qt_microphone_interaction/#microphone-voice-interaction","text":"About This is microphone voice interaction demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Microphone Voice Interaction"},{"location":"demos/qt_microphone_interaction/#video","text":"","title":"Video"},{"location":"demos/qt_microphone_interaction/#setup","text":"NOTICE You must read and fully understand Snips voice system before starting this Please read the documentation of snips to understand it https://console.snips.ai/login How to set it up? This demo is already installed on your QTrobot. you can find it under ~/robot/code/qt_app Snips already installed on QTRP (head) how to run the snip servers: sudo systemctl start snips-asr.service snips-audio-server.service snips-dialogue.service snips-hotword.service snips-nlu.service how to run qt_voice_app demo rosrun qt_voice_app qt_voice_app.py Voice commands Hey QT, show me your happy emotion Hey QT, play happy gesture Snips configuration file /etc/snips.toml","title":"Setup"},{"location":"demos/qt_microphone_interaction/#code","text":"Check code here #!/usr/bin/env python2 # -*- coding: utf-8 -*- import time import threading from hermes_python.hermes import Hermes import rospy from std_msgs.msg import String from qt_gesture_controller.srv import * from qt_motors_controller.srv import * from qt_robot_interface.srv import * talkText = rospy . ServiceProxy ( '/qt_robot/behavior/talkText' , behavior_talk_text ) gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) gestureList = rospy . ServiceProxy ( '/qt_robot/gesture/list' , gesture_list ) gestureSave = rospy . ServiceProxy ( '/qt_robot/gesture/save' , gesture_save ) setControlMode = rospy . ServiceProxy ( '/qt_robot/motors/setControlMode' , set_control_mode ) gestureRecord = rospy . ServiceProxy ( '/qt_robot/gesture/record' , gesture_record ) #gesturePlay_pub = rospy.Publisher('/qt_robot/gesture/play', String, queue_size=10) emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) action_thread = None def emotion_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in emotion_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"ShowEmotion\" : emotion_name = slot [ 0 ] . slot_value . value . value try : print ( \"Showing emotion %s \" % \"QT/ %s \" % emotion_name ) talkText ( \"This is my %s face.\" % emotion_name . replace ( \"_\" , \" \" )) emotionShow_pub . publish ( \"QT/ %s \" % emotion_name ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for showing emotion %s \" % emotion_name ) else : talkText ( \"I did not understand which emotion.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def play_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) # action code goes here... print ( \"in play_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"GastureName\" : gesture_name = slot [ 0 ] . slot_value . value . value try : res = gestureList () if gesture_name not in res . gestures : talkText ( \"I do not know this gesture. But you can always record your gesture. Just ask me, hey Q.T. . record, new gesture!\" ) else : if gesture_name != \"my\" : gesture_name = \"QT/\" + gesture_name print ( \"playing gesture ' %s '.\" % gesture_name ) res = gesturePlay ( gesture_name , 1.0 ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for playing gesture %s \" % gesture_name ) else : talkText ( \"I did not understand which gesture.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def record_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in record_intent_callback\" ) talkText ( \"You can start recording your gesture.\" ) res = gestureRecord ([ \"right_arm\" , \"left_arm\" ], True , 0 , 0 ) print ( \"done!\" ) def stop_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in stop_intent_callback\" ) res = gestureSave ( \"my\" , \"\" ) if not res . status : talkText ( \"I have a problem with recording the gesture\" ) setControlMode ([ \"right_arm\" , \"left_arm\" ], 1 ) talkText ( \"Gesture recorded!\" ) print ( \"done!\" ) def unknown_intent_callback ( hermes , intent_message ): talkText ( \"I do not know this. But you can ask me to play gesture or show emotions.\" ) print ( \"done\" ) def intent_received ( hermes , intent_message ): hermes . publish_end_session ( intent_message . session_id , None ) coming_intent = intent_message . intent . intent_name print ( intent_message . intent . confidence_score ) if intent_message . intent . confidence_score < 0.5 : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () return print if coming_intent == 'apaikan:Play' : action_thread = threading . Thread ( target = play_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Record' : action_thread = threading . Thread ( target = record_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Emotion' : action_thread = threading . Thread ( target = emotion_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Stop' : action_thread = threading . Thread ( target = stop_intent_callback , args = ( hermes , intent_message )) action_thread . start () else : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () def intent_not_recognized ( hermes , intent_nr_message ): print ( \"not recogniozed\" ) action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_nr_message )) action_thread . start () if __name__ == \"__main__\" : # call the relevant service rospy . init_node ( 'qt_voice_interface' , disable_signals = True ) with Hermes ( 'localhost:1883' ) as h : # h.subscribe_intent(\"apaikan:Play\", play_intent_callback) # h.subscribe_intent(\"apaikan:Record\", record_intent_callback) # h.subscribe_intent(\"apaikan:Stop\", stop_intent_callback) # h.subscribe_intent(\"apaikan:Emotion\", emotion_intent_callback) h . subscribe_intents ( intent_received ) h . subscribe_intent_not_recognized ( intent_not_recognized ) h . loop_forever () # async mode using #h.loop_start() rospy.spin() h.loop_stop()","title":"Code"},{"location":"demos/qt_range_of_motion/","text":"Range of Motion and DOF About This is range of motion and DOF demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"**Range of Motion and DOF**"},{"location":"demos/qt_range_of_motion/#range-of-motion-and-dof","text":"About This is range of motion and DOF demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Range of Motion and DOF"},{"location":"demos/qt_range_of_motion/#video","text":"","title":"Video"},{"location":"demos/qt_range_of_motion/#code","text":"Check code here #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"Code"},{"location":"examples/qt_audio_interface/","text":"Accessing QTrobot audio interface with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot audio interface with rospy module. This interface allows you to play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy. In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wav and so on. NOTICE The default path for the audio files is '~/robot/data/audios/' Audio interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.2 Audio Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot audio interface is \"/qt_robot/audio/play\" and data type is text \"String\" audio file name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a audio ros publisher audioPlay_pub = rospy . Publisher ( '/qt_robot/audio/play' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( audioPlay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To play audio file with the QTrobot audio interface we need to call publish function of the ROS publisher(audioPlay_pub) that we created with the audio file name. Step 3 # publish audio file audioPlay_pub . publish ( \"QT/5LittleBunnies\" ) NOTICE To play the audio file from the default path, pass an empty string to filepath parameter. Get the full code in our github tutorial repository.","title":"Accessing QTrobot audio interface with Python"},{"location":"examples/qt_audio_interface/#accessing-qtrobot-audio-interface-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot audio interface with rospy module. This interface allows you to play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy. In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wav and so on. NOTICE The default path for the audio files is '~/robot/data/audios/' Audio interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.2 Audio Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot audio interface is \"/qt_robot/audio/play\" and data type is text \"String\" audio file name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a audio ros publisher audioPlay_pub = rospy . Publisher ( '/qt_robot/audio/play' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( audioPlay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To play audio file with the QTrobot audio interface we need to call publish function of the ROS publisher(audioPlay_pub) that we created with the audio file name. Step 3 # publish audio file audioPlay_pub . publish ( \"QT/5LittleBunnies\" ) NOTICE To play the audio file from the default path, pass an empty string to filepath parameter. Get the full code in our github tutorial repository.","title":"Accessing QTrobot audio interface with Python"},{"location":"examples/qt_emotion_interface/","text":"Accessing QTrobot Emotion interface with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot emotion interface with rospy module. This interface allows you to change the QTrobot facial emotions such as 'QT/happy', 'QT/sad', etc. NOTICE The complete list of emotion files can be found in '~/robot/data/emotions/'. Emotion interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.3 Emotion Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot emotion interface is \"/qt_robot/emotion/show\" and data type is text \"String\" emotion name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a emotion ros publisher emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( emotionShow_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To show emotion with the QTrobot emotion interface we need to call publish function of the ROS publisher(emotionShow_pub) that we created with the emotion file name. Step 3 # publish emotion to QTrobot emotionShow_pub . publish ( \"QT/happy\" ) Get the full code in our github tutorial repository.","title":"Accessing QTrobot Emotion interface with Python"},{"location":"examples/qt_emotion_interface/#accessing-qtrobot-emotion-interface-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot emotion interface with rospy module. This interface allows you to change the QTrobot facial emotions such as 'QT/happy', 'QT/sad', etc. NOTICE The complete list of emotion files can be found in '~/robot/data/emotions/'. Emotion interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.3 Emotion Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot emotion interface is \"/qt_robot/emotion/show\" and data type is text \"String\" emotion name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a emotion ros publisher emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( emotionShow_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To show emotion with the QTrobot emotion interface we need to call publish function of the ROS publisher(emotionShow_pub) that we created with the emotion file name. Step 3 # publish emotion to QTrobot emotionShow_pub . publish ( \"QT/happy\" ) Get the full code in our github tutorial repository.","title":"Accessing QTrobot Emotion interface with Python"},{"location":"examples/qt_face_example/","text":"Reading QTrobot nuitrack topics with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack faces topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Faces topic publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition ROS Faces message structure qt_nuitrack_app/FaceInfo [] faces int32 id string gender int32 age_years string age_type float64 emotion_neutral float64 emotion_angry float64 emotion_happy float64 emotion_surprise float64 [] rectangle float64 [] left_eye float64 [] right_eye float64 [] angles You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/faces' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Faces data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Faces Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def face_callback ( msg ): rospy . loginfo ( msg ) def listener (): face_sub = rospy . Subscriber ( '/qt_nuitrack_app/faces' , Faces , face_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot. When QTrobot detects you it will print something like this in terminal: [ INFO ] [ 1620743499 .741132 ] : faces: - id: 2 gender: \"male\" age_years: 16 age_type: \"kid\" emotion_neutral: 0 .398449302 emotion_angry: 0 .59506011 emotion_happy: 0 .00152670825 emotion_surprise: 0 .00496386969 rectangle: [ -0.0218749996, 0 .00208333344, 0 .1609375, 0 .21666666666666667 ] left_eye: [ 0 .0281414744, 0 .0821291357 ] right_eye: [ 0 .0766262114, 0 .0779390857 ] angles: [ -20.7057095, -0.0146303261, -4.30262899 ] Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_face_example/#reading-qtrobot-nuitrack-topics-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack faces topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Faces topic publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition ROS Faces message structure qt_nuitrack_app/FaceInfo [] faces int32 id string gender int32 age_years string age_type float64 emotion_neutral float64 emotion_angry float64 emotion_happy float64 emotion_surprise float64 [] rectangle float64 [] left_eye float64 [] right_eye float64 [] angles You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/faces' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Faces data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Faces Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def face_callback ( msg ): rospy . loginfo ( msg ) def listener (): face_sub = rospy . Subscriber ( '/qt_nuitrack_app/faces' , Faces , face_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot. When QTrobot detects you it will print something like this in terminal: [ INFO ] [ 1620743499 .741132 ] : faces: - id: 2 gender: \"male\" age_years: 16 age_type: \"kid\" emotion_neutral: 0 .398449302 emotion_angry: 0 .59506011 emotion_happy: 0 .00152670825 emotion_surprise: 0 .00496386969 rectangle: [ -0.0218749996, 0 .00208333344, 0 .1609375, 0 .21666666666666667 ] left_eye: [ 0 .0281414744, 0 .0821291357 ] right_eye: [ 0 .0766262114, 0 .0779390857 ] angles: [ -20.7057095, -0.0146303261, -4.30262899 ] Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_gesture_example/","text":"Reading QTrobot nuitrack topics with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack gesture topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Gesture topic publishes some basic human gestures recognition (SWIPE UP, SWIPE DOWN, SWIPE LEFT, SWIPE RIGHT, WAVING, PUSH). ROS Gestures message structure qt_nuitrack_app/GestureInfo [] gestures int32 id string name You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/gestures' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Gestures data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Gestures Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def gesture_callback ( msg ): rospy . loginfo ( msg ) def listener (): gesture_sub = rospy . Subscriber ( '/qt_nuitrack_app/gestures' , Gestures , gesture_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot and try to do one of the gestures. If QTrobot detects your gesture you will see something like this in your terminal: [ INFO ] [ 1620741522 .673911 ] : gestures: - id: 2 name: \"SWIPE UP\" Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_gesture_example/#reading-qtrobot-nuitrack-topics-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack gesture topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Gesture topic publishes some basic human gestures recognition (SWIPE UP, SWIPE DOWN, SWIPE LEFT, SWIPE RIGHT, WAVING, PUSH). ROS Gestures message structure qt_nuitrack_app/GestureInfo [] gestures int32 id string name You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/gestures' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Gestures data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Gestures Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def gesture_callback ( msg ): rospy . loginfo ( msg ) def listener (): gesture_sub = rospy . Subscriber ( '/qt_nuitrack_app/gestures' , Gestures , gesture_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot and try to do one of the gestures. If QTrobot detects your gesture you will see something like this in your terminal: [ INFO ] [ 1620741522 .673911 ] : gestures: - id: 2 name: \"SWIPE UP\" Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_gspeech_service/","text":"QTrobot Google Speech-To-Text ROS Service Get the full code in our github tutorial repository. Notice Everything should be installed on QTRP (head) - 192.168.100.1 1. QTrobot with pre-installed google speech interface Notice Please follow the instruction bellow if your QTrobot came with pre-installed google speech recognition 1.1 Setup Google Cloud and download your private key Follow the 1. Step from instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application. 1.2 Connect to QTrobot hotspot and ssh to QTrobot and initalize Google cloud ssh qtrobot@192.168.100.1 gcloud init When initializing google cloud just follow the steps from command line (login to your google account, select your google-cloud-server) 1.3 Enable qt_gspeech_interface sudo nano ~/robot/autostart/autostart_screens.sh Uncomment the last line in autostart script \"run_script \"start_qt_gspeech_interface.sh\"\". Save it (Ctrl+O), exit (Ctrl+X) and reboot the QTrobot. 1.4 Running the QTrobot Google Speech Ros service To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\" 2. Full installation of google speech interface Connect to QTrobot hotspot and ssh to QTrobot ssh qtrobot@192.168.100.1 2.1 Prepare QTrobot (get files and setup the environment) Clone the github repository into \"robot/code\" folder cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git Copy the autostart script to autostart folder cp ~/robot/code/tutorials/examples/qt_gspeech_interface/autostart/start_qt_gspeech_interface.sh ~/robot/autostart Enable the qt_gspeech_interface in autostart_screen.sh sudo nano ~/robot/autostart/autostart_screens.sh run_script \"start_qt_gspeech_interface.sh\" Below the last command \"run_script\" copy and paste the command above. Save it (Ctrl+O) and exit (Ctrl+X). 2.2 Install python3 virtualenv and portaudio sudo apt-get update && sudo apt-get install python3-venv portaudio19-dev 2.3 Create Python3 virtualenv and install requirements Install everything inside project folder (qt_gspeech_interface) python3 -m venv .venv && source .venv/bin/activate pip3 install --upgrade \"pip < 21.0\" pip3 install -r requirements.txt 2.4 Setup Google Cloud Follow the instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application. Install Google Cloud SDK and Google Cloud Python SDK (4.step) 2.5 Link your folder and build catkin qt_gspeech_interface package cd ~/catkin_ws/src && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface . cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface 2.6 Running the QTrobot Google Speech Ros service Make sure that you have uncommented \"run_script \"start_qt_gspeech_interface.sh\"\" in autostart_screen.sh. After that just reboot the QTrobot. To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\" 2.7 Additional If you want to call this service outside of QTRP environment then you should copy content of qtpc_gspeech to your catkin workspace copy qtpc_gspeech to your PC or QTPC These commands include \"robot/code\" folder just for the purpose of the example. Include your folder name in the command. cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git cd ~/catkin_ws/src/ && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface/qtpc_gspeech qt_gspeech_interface cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface","title":"QTrobot Google Speech-To-Text ROS Service"},{"location":"examples/qt_gspeech_service/#qtrobot-google-speech-to-text-ros-service","text":"Get the full code in our github tutorial repository. Notice Everything should be installed on QTRP (head) - 192.168.100.1","title":"QTrobot Google Speech-To-Text ROS Service"},{"location":"examples/qt_gspeech_service/#1-qtrobot-with-pre-installed-google-speech-interface","text":"Notice Please follow the instruction bellow if your QTrobot came with pre-installed google speech recognition","title":"1. QTrobot with pre-installed google speech interface"},{"location":"examples/qt_gspeech_service/#11-setup-google-cloud-and-download-your-private-key","text":"Follow the 1. Step from instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application.","title":"1.1 Setup Google Cloud and download your private key"},{"location":"examples/qt_gspeech_service/#12-connect-to-qtrobot-hotspot-and-ssh-to-qtrobot-and-initalize-google-cloud","text":"ssh qtrobot@192.168.100.1 gcloud init When initializing google cloud just follow the steps from command line (login to your google account, select your google-cloud-server)","title":"1.2 Connect to QTrobot hotspot and ssh to QTrobot and initalize Google cloud"},{"location":"examples/qt_gspeech_service/#13-enable-qt_gspeech_interface","text":"sudo nano ~/robot/autostart/autostart_screens.sh Uncomment the last line in autostart script \"run_script \"start_qt_gspeech_interface.sh\"\". Save it (Ctrl+O), exit (Ctrl+X) and reboot the QTrobot.","title":"1.3 Enable qt_gspeech_interface"},{"location":"examples/qt_gspeech_service/#14-running-the-qtrobot-google-speech-ros-service","text":"To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\"","title":"1.4 Running the QTrobot Google Speech Ros service"},{"location":"examples/qt_gspeech_service/#2-full-installation-of-google-speech-interface","text":"Connect to QTrobot hotspot and ssh to QTrobot ssh qtrobot@192.168.100.1","title":"2. Full installation of google speech interface"},{"location":"examples/qt_gspeech_service/#21-prepare-qtrobot-get-files-and-setup-the-environment","text":"Clone the github repository into \"robot/code\" folder cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git Copy the autostart script to autostart folder cp ~/robot/code/tutorials/examples/qt_gspeech_interface/autostart/start_qt_gspeech_interface.sh ~/robot/autostart Enable the qt_gspeech_interface in autostart_screen.sh sudo nano ~/robot/autostart/autostart_screens.sh run_script \"start_qt_gspeech_interface.sh\" Below the last command \"run_script\" copy and paste the command above. Save it (Ctrl+O) and exit (Ctrl+X).","title":"2.1 Prepare QTrobot (get files and setup the environment)"},{"location":"examples/qt_gspeech_service/#22-install-python3-virtualenv-and-portaudio","text":"sudo apt-get update && sudo apt-get install python3-venv portaudio19-dev","title":"2.2 Install python3 virtualenv and portaudio"},{"location":"examples/qt_gspeech_service/#23-create-python3-virtualenv-and-install-requirements","text":"Install everything inside project folder (qt_gspeech_interface) python3 -m venv .venv && source .venv/bin/activate pip3 install --upgrade \"pip < 21.0\" pip3 install -r requirements.txt","title":"2.3 Create Python3 virtualenv and install requirements"},{"location":"examples/qt_gspeech_service/#24-setup-google-cloud","text":"Follow the instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application. Install Google Cloud SDK and Google Cloud Python SDK (4.step)","title":"2.4 Setup Google Cloud"},{"location":"examples/qt_gspeech_service/#25-link-your-folder-and-build-catkin-qt_gspeech_interface-package","text":"cd ~/catkin_ws/src && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface . cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface","title":"2.5 Link your folder and build catkin qt_gspeech_interface package"},{"location":"examples/qt_gspeech_service/#26-running-the-qtrobot-google-speech-ros-service","text":"Make sure that you have uncommented \"run_script \"start_qt_gspeech_interface.sh\"\" in autostart_screen.sh. After that just reboot the QTrobot. To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\"","title":"2.6 Running the QTrobot Google Speech Ros service"},{"location":"examples/qt_gspeech_service/#27-additional","text":"If you want to call this service outside of QTRP environment then you should copy content of qtpc_gspeech to your catkin workspace copy qtpc_gspeech to your PC or QTPC These commands include \"robot/code\" folder just for the purpose of the example. Include your folder name in the command. cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git cd ~/catkin_ws/src/ && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface/qtpc_gspeech qt_gspeech_interface cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface","title":"2.7 Additional"},{"location":"examples/qt_hands_example/","text":"Reading QTrobot nuitrack topics with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack hands topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Hands topic publishes human 3D hand pos and state ROS Skeletons message structure qt_nuitrack_app/HandInfo [] hands int32 id float32 [] right_projection float32 [] right_real bool right_click int32 right_pressure float32 [] left_projection float32 [] left_real bool left_click int32 left_pressure You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/hands' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Skeletons data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Hands Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def hands_callback ( msg ): rospy . loginfo ( msg ) def listener (): hands_sub = rospy . Subscriber ( '/qt_nuitrack_app/hands' , Hands , hands_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot. When QTrobot detects you it will print something like this in terminal: hands: - id: 2 right_projection: [ -1.0, -1.0 ] right_real: [ 0 .0, 0 .0, 0 .0 ] right_click: False right_pressure: 0 left_projection: [ -1.0, -1.0 ] left_real: [ 0 .0, 0 .0, 0 .0 ] left_click: False left_pressure: 0 Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_hands_example/#reading-qtrobot-nuitrack-topics-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack hands topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Hands topic publishes human 3D hand pos and state ROS Skeletons message structure qt_nuitrack_app/HandInfo [] hands int32 id float32 [] right_projection float32 [] right_real bool right_click int32 right_pressure float32 [] left_projection float32 [] left_real bool left_click int32 left_pressure You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/hands' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Skeletons data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Hands Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def hands_callback ( msg ): rospy . loginfo ( msg ) def listener (): hands_sub = rospy . Subscriber ( '/qt_nuitrack_app/hands' , Hands , hands_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot. When QTrobot detects you it will print something like this in terminal: hands: - id: 2 right_projection: [ -1.0, -1.0 ] right_real: [ 0 .0, 0 .0, 0 .0 ] right_click: False right_pressure: 0 left_projection: [ -1.0, -1.0 ] left_real: [ 0 .0, 0 .0, 0 .0 ] left_click: False left_pressure: 0 Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_motors_command/","text":"QTrobot Motors command Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"QTrobot Motors command"},{"location":"examples/qt_motors_command/#qtrobot-motors-command","text":"Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"QTrobot Motors command"},{"location":"examples/qt_motors_custom_controller/","text":"QTrobot Motors ROS controller Get the full code in our github tutorial repository. This example shows how to write a new ROS controller for QTrobot. Before getting into that, make sure you know and understand what is ROS Conrol . What it does? this controller simply play a predefined trajectory directly by commanding the QTrobot motors at hardware level. You can use it as a simple template to implement more sophisticate controllers. Compilation and build copy motors_custom_controller to the ~/catckin_ws on the QTRP of QTrobot build the example code: cd ~/catckin_ws catkin_make --pkg motors_custom_controller Configuration and launching modify the qt_motor config file to define your new controller. to do that open /opt/ros/kinetic/share/qt_motor/config/qtrobot-controller.yaml file and add the following lines to the end of the file. custom: type: motors_custom_controller/QTCustomController myparam: bar modify qt_motor launch file to load and run your controller. to do that, open /opt/ros/kinetic/share/qt_motor/launch/qt_motor.launch and add /qt_robot/custom controller to controller_manager node. it should look like this: <!-- load the controllers --> <node pkg=\"controller_manager\" name=\"controller\" type=\"spawner\" launch-prefix=\"bash -c 'sleep 15; $0 $@' \" respawn=\"false\" output=\"screen\" clear_params=\"true\" args=\"/qt_robot/head_position /qt_robot/right_arm_position /qt_robot/left_arm_position /qt_robot/joints_state /qt_robot/motors /qt_robot/gesture /qt_robot/custom\"/> Re-launch qt_motor node you can simply reboot the robot or launch the qt_motor node from terminal to see its output. * stop the running instance: ps -aux | grep qt_motor | grep SCREEN ... kill xxx launch qt_motor node roslaunch qt_motor qt_motor.launch you should see your controller ( QTCustomController ) running by checking the terminal output: ... [INFO] [1576498210.924577]: Controller Spawner: Loaded controllers: /qt_robot/head_position, /qt_robot/right_arm_position, /qt_robot/left_arm_position, /qt_robot/joints_state, /qt_robot/motors, /qt_robot/gesture, /qt_robot/custom [ INFO] [1576498210.984396566]: QTMotorsController: starting [ INFO] [1576498210.985034385]: QTGestureController: starting [ INFO] [1576498210.985518609]: QTCustomController: starting ... Testing your custom controller implement a service to Start/Stop it. to start the trajectory player, open another termianl and rosservice call /qt_robot/custom/startstop \"command: true\" to stop the trajectory player: rosservice call /qt_robot/custom/startstop \"command: false\"","title":"QTrobot Motors ROS controller"},{"location":"examples/qt_motors_custom_controller/#qtrobot-motors-ros-controller","text":"Get the full code in our github tutorial repository. This example shows how to write a new ROS controller for QTrobot. Before getting into that, make sure you know and understand what is ROS Conrol .","title":"QTrobot Motors ROS controller"},{"location":"examples/qt_motors_custom_controller/#what-it-does","text":"this controller simply play a predefined trajectory directly by commanding the QTrobot motors at hardware level. You can use it as a simple template to implement more sophisticate controllers.","title":"What it does?"},{"location":"examples/qt_motors_custom_controller/#compilation-and-build","text":"copy motors_custom_controller to the ~/catckin_ws on the QTRP of QTrobot build the example code: cd ~/catckin_ws catkin_make --pkg motors_custom_controller","title":"Compilation and build"},{"location":"examples/qt_motors_custom_controller/#configuration-and-launching","text":"modify the qt_motor config file to define your new controller. to do that open /opt/ros/kinetic/share/qt_motor/config/qtrobot-controller.yaml file and add the following lines to the end of the file. custom: type: motors_custom_controller/QTCustomController myparam: bar modify qt_motor launch file to load and run your controller. to do that, open /opt/ros/kinetic/share/qt_motor/launch/qt_motor.launch and add /qt_robot/custom controller to controller_manager node. it should look like this: <!-- load the controllers --> <node pkg=\"controller_manager\" name=\"controller\" type=\"spawner\" launch-prefix=\"bash -c 'sleep 15; $0 $@' \" respawn=\"false\" output=\"screen\" clear_params=\"true\" args=\"/qt_robot/head_position /qt_robot/right_arm_position /qt_robot/left_arm_position /qt_robot/joints_state /qt_robot/motors /qt_robot/gesture /qt_robot/custom\"/>","title":"Configuration and launching"},{"location":"examples/qt_motors_custom_controller/#re-launch-qt_motor-node","text":"you can simply reboot the robot or launch the qt_motor node from terminal to see its output. * stop the running instance: ps -aux | grep qt_motor | grep SCREEN ... kill xxx launch qt_motor node roslaunch qt_motor qt_motor.launch you should see your controller ( QTCustomController ) running by checking the terminal output: ... [INFO] [1576498210.924577]: Controller Spawner: Loaded controllers: /qt_robot/head_position, /qt_robot/right_arm_position, /qt_robot/left_arm_position, /qt_robot/joints_state, /qt_robot/motors, /qt_robot/gesture, /qt_robot/custom [ INFO] [1576498210.984396566]: QTMotorsController: starting [ INFO] [1576498210.985034385]: QTGestureController: starting [ INFO] [1576498210.985518609]: QTCustomController: starting ...","title":"Re-launch qt_motor node"},{"location":"examples/qt_motors_custom_controller/#testing","text":"your custom controller implement a service to Start/Stop it. to start the trajectory player, open another termianl and rosservice call /qt_robot/custom/startstop \"command: true\" to stop the trajectory player: rosservice call /qt_robot/custom/startstop \"command: false\"","title":"Testing"},{"location":"examples/qt_motors_gesture/","text":"QTrobot Motors gesture Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from qt_gesture_controller.srv import * def print_help (): print ( \"QTrobot gesture player example\" ) print ( \"Usage:\" ) print ( \" qt_motors_gesture <name> play a gesture given by its <name>\" ) print ( \"\" ) # main if __name__ == '__main__' : # check the params if len ( sys . argv ) < 2 : print_help () sys . exit ( 1 ) # call the relevant service rospy . init_node ( 'qt_motors_gesture' ) try : gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) res = gesturePlay ( sys . argv [ 1 ], 1.0 ) if not res . status : print ( \"Could not play gesture ' %s '.\" % sys . argv [ 1 ]) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e","title":"QTrobot Motors gesture"},{"location":"examples/qt_motors_gesture/#qtrobot-motors-gesture","text":"Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from qt_gesture_controller.srv import * def print_help (): print ( \"QTrobot gesture player example\" ) print ( \"Usage:\" ) print ( \" qt_motors_gesture <name> play a gesture given by its <name>\" ) print ( \"\" ) # main if __name__ == '__main__' : # check the params if len ( sys . argv ) < 2 : print_help () sys . exit ( 1 ) # call the relevant service rospy . init_node ( 'qt_motors_gesture' ) try : gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) res = gesturePlay ( sys . argv [ 1 ], 1.0 ) if not res . status : print ( \"Could not play gesture ' %s '.\" % sys . argv [ 1 ]) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e","title":"QTrobot Motors gesture"},{"location":"examples/qt_motors_moveit/","text":"Controlling QTrobot arms using MoveIt Get the full code in our github tutorial repository. This advanced example demonstrates how to use ROS MoveIT to control QTrobot arms. The example draw some shapes (i,e. rectangle and spirals) on the XY plane in robot frame. Preparation and requirements Before running the example, please ensure that following setups of your QTrobot and the machine which you are running the example. QTrobot setup By default QTrobot motors interface runs in 'normal' mode. in normal mode, the motors control loop and joints state publisher run in low frequency (2-5hz). More importantly the joint position values are in degree . To be able to use MoveIt with QTrobot, you need to configure it to run in 'advanced' mode: - joints position value is in radian - motors main controller loop and joints state publisher runs in 30hz. - required interfaces and controller such as robot_state_publisher and JointTrajectoryController are available Update ros-kinetic-qt-motor package if necessary: First check and update (if necessary) the ros-kinetic-qt-motor package on the QTrobot head machine (QTRP). ssh to the QTRP (e.g. via QTPC or your desktop) and qtrobot@QTXXX: apt list ros-kinetic-qt-motor ros-kinetic-qt-motor/now 1.2.0-0xenial armhf [installed,local] if the version of ros-kinetic-qt-motor is older then 1.2.0 , you NEED to update it: make a copy of your current QTrobot configuration ( qtrobot-hardware.yaml ). You need this file to put it back after updating the package: qtrobot@QTXXX: cp /opt/ros/kinetic/share/qt_motor/config/qtrobot-hardware.yaml ~/ install the latest version of ros-kinetic-qt-motor . in my case it is 1.2.0-0 : qtrobot@QTXXX: cd ~/robot/packages/dep qtrobot@QTXXX: git pull qtrobot@QTXXX: sudo apt remove ros-kinetic-qt-motor qtrobot@QTXXX: sudo dpkg -i ros-kinetic-qt-motor_1.2.0-0xenial_armhf.deb put back your QTrobot configuration ( qtrobot-hardware.yaml ): qtrobot@QTXXX: sudo cp ~/qtrobot-hardware.yaml /opt/ros/kinetic/share/qt_motor/config/ Change motor launcher autostart script to run in advanced mode: qtrobot@QTXXX: nano ~/robot/autostart/start_qt_motor.sh and change the corresponding line to look like the following and save and exit: roslaunch qt_motor qt_motor_advanced.launch Reboot the robot to run the advance motor interface. Check the advanced mode setup: After rebooting the reboot, you can check (from QTPC, QTRP or your machine) if the motor interface is running in the advanced mode: joint state publisher frequency: qtrobot@QTXXX: rostopic hz /qt_robot/joints/state average rate: 30.041 min: 0.029s max: 0.047s std dev: 0.00391s window: 29 joints value should be in radian: qtrobot@QTXXX: rostopic echo /qt_robot/joints/state position: [0.015707962851830046, 0.0, -0.6073745663782212, 1.569051024174513, -0.9896016991965904, -0.5689773095185405, -0.3455751785790718, -0.8360127383368947] trajectory controller is running: qtrobot@QTXXX: rostopic type /qt_robot/left_arm_controller/follow_joint_trajectory/goal control_msgs/FollowJointTrajectoryActionGoal Your machine setup (e.g. QTPC) After checking and updating the QTrobot setup, you can install the iKfast solver plugin for MoveIt on the machine which you plan to run the example: clone the QTrobot open software repository: cd ~/ git clone https://github.com/luxai-qtrobot/software.git build the plugins: cd ~/catkin_ws ln -s ~/software/qtrobot_ikfast_right_arm_plugin/ ./ ln -s ~/software/qtrobot_ikfast_right_left_plugin/ ./ cd ../ catkin_make clone and build the motors_moveit Assuming that you have cloned the tutorial repository somewhere on your home folder (e.g. ~/tutorials ): cd ~/catkin_ws ln -s ~/tutorials/examples/motors_moveit ./ cd ../ catkin_make or copy the motors_moveit to your catkin_ws. How to run the examples Launch moveit_qtrobot.launch to start move_group planner and rviz: roslaunch motors_moveit moveit_qtrobot.launch wait until rviz shows up, then run one of the following demos: Drawing rectangle rosrun motors_moveit draw_rectangle.py joint_states:=/qt_robot/joints/state Drawing spiral rosrun motors_moveit draw_spiral.py joint_states:=/qt_robot/joints/state","title":"Controlling QTrobot arms using MoveIt"},{"location":"examples/qt_motors_moveit/#controlling-qtrobot-arms-using-moveit","text":"Get the full code in our github tutorial repository. This advanced example demonstrates how to use ROS MoveIT to control QTrobot arms. The example draw some shapes (i,e. rectangle and spirals) on the XY plane in robot frame.","title":"Controlling QTrobot arms using MoveIt"},{"location":"examples/qt_motors_moveit/#preparation-and-requirements","text":"Before running the example, please ensure that following setups of your QTrobot and the machine which you are running the example.","title":"Preparation and requirements"},{"location":"examples/qt_motors_moveit/#qtrobot-setup","text":"By default QTrobot motors interface runs in 'normal' mode. in normal mode, the motors control loop and joints state publisher run in low frequency (2-5hz). More importantly the joint position values are in degree . To be able to use MoveIt with QTrobot, you need to configure it to run in 'advanced' mode: - joints position value is in radian - motors main controller loop and joints state publisher runs in 30hz. - required interfaces and controller such as robot_state_publisher and JointTrajectoryController are available Update ros-kinetic-qt-motor package if necessary: First check and update (if necessary) the ros-kinetic-qt-motor package on the QTrobot head machine (QTRP). ssh to the QTRP (e.g. via QTPC or your desktop) and qtrobot@QTXXX: apt list ros-kinetic-qt-motor ros-kinetic-qt-motor/now 1.2.0-0xenial armhf [installed,local] if the version of ros-kinetic-qt-motor is older then 1.2.0 , you NEED to update it: make a copy of your current QTrobot configuration ( qtrobot-hardware.yaml ). You need this file to put it back after updating the package: qtrobot@QTXXX: cp /opt/ros/kinetic/share/qt_motor/config/qtrobot-hardware.yaml ~/ install the latest version of ros-kinetic-qt-motor . in my case it is 1.2.0-0 : qtrobot@QTXXX: cd ~/robot/packages/dep qtrobot@QTXXX: git pull qtrobot@QTXXX: sudo apt remove ros-kinetic-qt-motor qtrobot@QTXXX: sudo dpkg -i ros-kinetic-qt-motor_1.2.0-0xenial_armhf.deb put back your QTrobot configuration ( qtrobot-hardware.yaml ): qtrobot@QTXXX: sudo cp ~/qtrobot-hardware.yaml /opt/ros/kinetic/share/qt_motor/config/ Change motor launcher autostart script to run in advanced mode: qtrobot@QTXXX: nano ~/robot/autostart/start_qt_motor.sh and change the corresponding line to look like the following and save and exit: roslaunch qt_motor qt_motor_advanced.launch Reboot the robot to run the advance motor interface. Check the advanced mode setup: After rebooting the reboot, you can check (from QTPC, QTRP or your machine) if the motor interface is running in the advanced mode: joint state publisher frequency: qtrobot@QTXXX: rostopic hz /qt_robot/joints/state average rate: 30.041 min: 0.029s max: 0.047s std dev: 0.00391s window: 29 joints value should be in radian: qtrobot@QTXXX: rostopic echo /qt_robot/joints/state position: [0.015707962851830046, 0.0, -0.6073745663782212, 1.569051024174513, -0.9896016991965904, -0.5689773095185405, -0.3455751785790718, -0.8360127383368947] trajectory controller is running: qtrobot@QTXXX: rostopic type /qt_robot/left_arm_controller/follow_joint_trajectory/goal control_msgs/FollowJointTrajectoryActionGoal","title":"QTrobot setup"},{"location":"examples/qt_motors_moveit/#your-machine-setup-eg-qtpc","text":"After checking and updating the QTrobot setup, you can install the iKfast solver plugin for MoveIt on the machine which you plan to run the example: clone the QTrobot open software repository: cd ~/ git clone https://github.com/luxai-qtrobot/software.git build the plugins: cd ~/catkin_ws ln -s ~/software/qtrobot_ikfast_right_arm_plugin/ ./ ln -s ~/software/qtrobot_ikfast_right_left_plugin/ ./ cd ../ catkin_make","title":"Your machine setup (e.g. QTPC)"},{"location":"examples/qt_motors_moveit/#clone-and-build-the-motors_moveit","text":"Assuming that you have cloned the tutorial repository somewhere on your home folder (e.g. ~/tutorials ): cd ~/catkin_ws ln -s ~/tutorials/examples/motors_moveit ./ cd ../ catkin_make or copy the motors_moveit to your catkin_ws.","title":"clone and build the motors_moveit"},{"location":"examples/qt_motors_moveit/#how-to-run-the-examples","text":"Launch moveit_qtrobot.launch to start move_group planner and rviz: roslaunch motors_moveit moveit_qtrobot.launch wait until rviz shows up, then run one of the following demos:","title":"How to run the examples"},{"location":"examples/qt_motors_moveit/#drawing-rectangle","text":"rosrun motors_moveit draw_rectangle.py joint_states:=/qt_robot/joints/state","title":"Drawing rectangle"},{"location":"examples/qt_motors_moveit/#drawing-spiral","text":"rosrun motors_moveit draw_spiral.py joint_states:=/qt_robot/joints/state","title":"Drawing spiral"},{"location":"examples/qt_motors_state/","text":"QTrobot Motors state Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from sensor_msgs.msg import JointState def joint_states_callback ( msg ): strmsg = \"\" for i , joint_name in enumerate ( msg . name ): strmsg += \" %s : %.2f , \" % ( joint_name , msg . position [ i ]) rospy . loginfo ( strmsg ) # main if __name__ == '__main__' : # call the relevant service rospy . init_node ( 'qt_motors_state' ) rospy . Subscriber ( '/qt_robot/joints/state' , JointState , joint_states_callback ) rospy . spin ()","title":"QTrobot Motors state"},{"location":"examples/qt_motors_state/#qtrobot-motors-state","text":"Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from sensor_msgs.msg import JointState def joint_states_callback ( msg ): strmsg = \"\" for i , joint_name in enumerate ( msg . name ): strmsg += \" %s : %.2f , \" % ( joint_name , msg . position [ i ]) rospy . loginfo ( strmsg ) # main if __name__ == '__main__' : # call the relevant service rospy . init_node ( 'qt_motors_state' ) rospy . Subscriber ( '/qt_robot/joints/state' , JointState , joint_states_callback ) rospy . spin ()","title":"QTrobot Motors state"},{"location":"examples/qt_skeleton_example/","text":"Reading QTrobot nuitrack topics with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack skeletons topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Skeletons topic publishes human full body skeleton information ROS Skeletons message structure qt_nuitrack_app/SkeletonInfo [] skeletons int32 id qt_nuitrack_app/JointInfo [] joints uint8 type float32 confidence float32 [] real float32 [] projection float32 [] orientation You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/skeletons' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Skeletons data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Skeletons Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def skeletons_callback ( msg ): rospy . loginfo ( msg ) def listener (): skeletons_sub = rospy . Subscriber ( '/qt_nuitrack_app/skeletons' , Skeletons , skeletons_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot. When QTrobot detects you it will print something like this in terminal: skeletons: - id: 2 joints: - type: 0 confidence: 0 .0 real: [ 0 .0, 0 .0, 0 .0 ] projection: [ 0 .0, 0 .0, 0 .0 ] orientation: [ 1 .0, 0 .0, 0 .0, 0 .0, 1 .0, 0 .0, 0 .0, 0 .0, 1 .0 ] - type: 1 confidence: 0 .0 real: [ 624 .39306640625, 346 .18487548828125, 1269 .763671875 ] projection: [ 0 .9711607098579407, 0 .15169653296470642, 1269 .763671875 ] orientation: [ 0 .7399715781211853, -0.08104655146598816, -0.667737603187561, 0 .07703026384115219, 0 .9963939189910889, -0.03557398170232773, 0 .6682128310203552, -0.025112269446253777, 0 .743546187877655 ] - type: 2 confidence: 0 .75 real: [ 574 .6754150390625, 257 .53973388671875, 1317 .256591796875 ] projection: [ 0 .9180094599723816, 0 .2502264380455017, 1317 .256591796875 ] orientation: [ 0 .7399715781211853, -0.08104655146598816, -0.667737603187561, 0 .07703026384115219, 0 .9963939189910889, -0.03557398170232773, 0 .6682128310203552, -0.025112269446253777, 0 .743546187877655 ] - type: 3 confidence: 0 .75 real: [ 569 .5018310546875, -44.684295654296875, 1389 .437744140625 ] projection: [ 0 .8927262425422668, 0 .5410854816436768, 1389 .437744140625 ] orientation: [ 0 .7399715781211853, 0 .04483749717473984, -0.671142041683197, 0 .07703026384115219, 0 .9855626225471497, 0 .15077339112758636, 0 .6682128310203552, -0.16326627135276794, 0 .7258344888687134 ] - Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_skeleton_example/#reading-qtrobot-nuitrack-topics-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack skeletons topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Skeletons topic publishes human full body skeleton information ROS Skeletons message structure qt_nuitrack_app/SkeletonInfo [] skeletons int32 id qt_nuitrack_app/JointInfo [] joints uint8 type float32 confidence float32 [] real float32 [] projection float32 [] orientation You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/skeletons' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Skeletons data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Skeletons Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def skeletons_callback ( msg ): rospy . loginfo ( msg ) def listener (): skeletons_sub = rospy . Subscriber ( '/qt_nuitrack_app/skeletons' , Skeletons , skeletons_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot. When QTrobot detects you it will print something like this in terminal: skeletons: - id: 2 joints: - type: 0 confidence: 0 .0 real: [ 0 .0, 0 .0, 0 .0 ] projection: [ 0 .0, 0 .0, 0 .0 ] orientation: [ 1 .0, 0 .0, 0 .0, 0 .0, 1 .0, 0 .0, 0 .0, 0 .0, 1 .0 ] - type: 1 confidence: 0 .0 real: [ 624 .39306640625, 346 .18487548828125, 1269 .763671875 ] projection: [ 0 .9711607098579407, 0 .15169653296470642, 1269 .763671875 ] orientation: [ 0 .7399715781211853, -0.08104655146598816, -0.667737603187561, 0 .07703026384115219, 0 .9963939189910889, -0.03557398170232773, 0 .6682128310203552, -0.025112269446253777, 0 .743546187877655 ] - type: 2 confidence: 0 .75 real: [ 574 .6754150390625, 257 .53973388671875, 1317 .256591796875 ] projection: [ 0 .9180094599723816, 0 .2502264380455017, 1317 .256591796875 ] orientation: [ 0 .7399715781211853, -0.08104655146598816, -0.667737603187561, 0 .07703026384115219, 0 .9963939189910889, -0.03557398170232773, 0 .6682128310203552, -0.025112269446253777, 0 .743546187877655 ] - type: 3 confidence: 0 .75 real: [ 569 .5018310546875, -44.684295654296875, 1389 .437744140625 ] projection: [ 0 .8927262425422668, 0 .5410854816436768, 1389 .437744140625 ] orientation: [ 0 .7399715781211853, 0 .04483749717473984, -0.671142041683197, 0 .07703026384115219, 0 .9855626225471497, 0 .15077339112758636, 0 .6682128310203552, -0.16326627135276794, 0 .7258344888687134 ] - Get the full code in our github tutorial repository.","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_speech_interface/","text":"Accessing QTrobot speech interface with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot speech interface with rospy module.This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot, but for this example we will use American English (en-US). Speech interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.1 Speech Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot speech interface is \"/qt_robot/speech/say\" and data type is text \"String\". Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a ros publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( speechSay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To publish text to the QTrobot speech interface we need to call publish function of the ROS publisher(speechSay_pub) that we created. Step 3 # publish a text message to TTS speechSay_pub . publish ( \"Hello!\" ) NOTICE Default language is set in '/opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml'. the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them. Get the full code in our github tutorial repository.","title":"Accessing QTrobot speech interface with Python"},{"location":"examples/qt_speech_interface/#accessing-qtrobot-speech-interface-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot speech interface with rospy module.This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot, but for this example we will use American English (en-US). Speech interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.1 Speech Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot speech interface is \"/qt_robot/speech/say\" and data type is text \"String\". Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a ros publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( speechSay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To publish text to the QTrobot speech interface we need to call publish function of the ROS publisher(speechSay_pub) that we created. Step 3 # publish a text message to TTS speechSay_pub . publish ( \"Hello!\" ) NOTICE Default language is set in '/opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml'. the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them. Get the full code in our github tutorial repository.","title":"Accessing QTrobot speech interface with Python"},{"location":"examples/qt_voice_activity/","text":"QTrobot Voice activity Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : print ( \"Voice activity %d \" % Mic_tuning . is_voice ()) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice activity"},{"location":"examples/qt_voice_activity/#qtrobot-voice-activity","text":"Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : print ( \"Voice activity %d \" % Mic_tuning . is_voice ()) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice activity"},{"location":"examples/qt_voice_direction/","text":"QTrobot Voice direction Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : if Mic_tuning . is_voice (): print ( \"I detected voice activity at angle %d \" % Mic_tuning . direction ) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice direction"},{"location":"examples/qt_voice_direction/#qtrobot-voice-direction","text":"Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : if Mic_tuning . is_voice (): print ( \"I detected voice activity at angle %d \" % Mic_tuning . direction ) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice direction"}]}