{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! This is the documentation page for all QTrobot developers out there. This page was made for helping our QTrobot community to have easier access to user manual, examples, demos, API reference and FAQ. .md-container { background-image: url(/img/luxai_illustration.png); height: 100%; background-position: center; background-repeat: no-repeat; background-color: rgba(255,255,255,0.88); background-attachment: fixed; background-blend-mode: lighten; } Quick Start Face, Age & Gender recognition 3D Skeleton Tracking","title":"Home"},{"location":"#welcome","text":"This is the documentation page for all QTrobot developers out there. This page was made for helping our QTrobot community to have easier access to user manual, examples, demos, API reference and FAQ. .md-container { background-image: url(/img/luxai_illustration.png); height: 100%; background-position: center; background-repeat: no-repeat; background-color: rgba(255,255,255,0.88); background-attachment: fixed; background-blend-mode: lighten; } Quick Start","title":"Welcome!"},{"location":"about/","text":"About","title":"**About**"},{"location":"about/#about","text":"","title":"About"},{"location":"api/","text":"API Reference QTrobot Interface The QTrobot (LuxAI) interface aims to facilitate accessing basic robot functionalities leveraging a set of user-friendly ROS interfaces. Each robot\u2019s functionality can be accessed in blocking and non-blocking mode using ROS publish/subscribe and Service/Client interfaces. Following are the main interfaces of QTrobot: QTrobot Emotion : implements robot facial emotion QTrobot Speech : implements robot text to speech functionality QTrobot Audio : implement a simple player to play standard audio files QTrobot Gesture : implements robot gesture control QTrobot Behavior : implements more complex behaviors by combining the robot basic functionality QTrobot Motor : implements robot motor controls using standard ros_control QTrobot 3D Camera : implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK QTrobot Settings : implements some basic setting of robot such as speaker volume control and some extra interfaces which are available for some models of QTrobot: For each of the main interfaces, there are two different way of accessing them: Using ROS Publisher/Subscribers to allow non-blocking call to the interfaces Using ROS Services for blocking and accessing more sophisticated interfaces 1. Naming convention All the QTRobot ROS interfaces have '/qt_robot/...' prefix in front of their names. The word follows the prefix is the name of the main service (e.g. '/qt_robot/speech/...' ). Each interface may have more sub-services (methods) which come after the main service name. For example: /qt_robot/speech/say : implements say() method of QTrobot TTS /qt_robot/speech/config : implements configure() method of QTrobot TTS For user\u2019s convenience we have given the same name to the service and subscriber for each QTrobot ROS interface. That means one can access, for example, the speech functionality using ROS service call or publish/subscribe via the same interface name (e.g. '/qt_robot/speech/say' ). Please notice that some of the complex services (e.g. speech configuration for language, pitch,\u2026) are accessible only via ROS service call. 2. Accessing QTrobot interface from bash You can access each Robot functionality via its publish/subscribe or Service/Client interfaces. For example to use robot 'Speech' functionality you can try the following: Using ROS Publisher $ rostopic pub /qt_robot/speech/say std_msgs/String \"data: 'I am QT'\" Using ROS Service $ rosservice call /qt_robot/speech/say \"message: 'I am QT.'\" 3. Accessing QTrobot interface from a python script [Non-blocking mode] The following example shows how to access QTrobot Speech functionality using ROS publish/subscribe method from python: import rospy from std_msgs.msg import String # create a publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) ... # publish a text message to TTS (non-blocking) speechSay_pub . publish ( \"Hello! I am QT!\" ) [Blocking mode] And the following example, re-implements the previous one using ROS Service/Client method from python: import rospy from std_msgs.msg import String from qt_robot_interface.srv import * # create a service clients speechSay = rospy . ServiceProxy ( '/qt_robot/speech/say' , speech_say ) ... # call the service (blocking) resp = speechSay ( \"Hello! I am QT.\" ) Notice All QTrobot service interfaces returns the status of the call upun success or failure. For example to check whether a call to a service was successful in python, you can check the return value resp.status. 4. List of available interfaces Currently the following interfaces have been implemented: INTERFACES Functionality Interface prefix Description Speech /qt_robot/speech/... robot text to speech functionality Audio /qt_robot/audio/... simple standard audio file player Emotion /qt_robot/emotion/... robot facial emotion Gesture /qt_robot/gesture/... robot gesture control Behavior /qt_robot/behavior/... more complex behaviors by combining the robot basic functionality Motor /qt_robot/motor/... robot motor controls using standard ros_control Setting /qt_robot/setting/... basic setting of robot such as speaker volume control Human 3D Skeleton /qt_nuitrack_app/skeletons Human full body skeleton information Human 3D Hand Pos /qt_nuitrack_app/hands Human 3D hand pos and state Human Hand Gestures /qt_nuitrack_app/gestures Some basic human gestures recognition Human facial /qt_nuitrack_app/faces human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition 4.1 Speech Interface This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot. following are some standard supported languages: en-US (american english) en-GB (british english) fr-FR (French) de-DE (German) SUBSCRIBERS Interface Name Data Type Description /qt_robot/speech/say 'std_msgs/String' (text) Read a text using built-in TTS SERVICES Interface Name Service Name Parameters Description /qt_robot/speech/say 'speech_say' message Read a text using built-in TTS /qt_robot/speech/config 'speech_config' 'language', 'pitch', 'speed' Configure TTS /qt_robot/speech/stop 'speech_stop' Stops current speech activity. Info Default language is set in '/opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml'. the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them. 4.2 Audio Interface Play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy . In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wavand so on. SUBSCRIBERS Interface Name Data Type Description /qt_robot/audio/play 'std_msgs/String' (audio file name) Play an sudio file SERVICES Interface Name Service Name Parameters Description /qt_robot/audio/play 'audio_play' 'filename', 'filepath' Play an audio file given by its filename and filepath /qt_robot/audio/stop 'audio_stop' Stops current audio activity. Info The default path for the audio files is '~/robot/data/audios/'. To play the audio file from the default path, pass an empty string to filepath parameter. 4.3 Emotion Interface Change the robot facial emotions such as 'QT/happy', 'QT/sad', etc. SUBSCRIBERS Interface Name Data Type Description /qt_robot/emotion/show 'std_msgs/String' (emotion name) Show a facial emotion given by its name SERVICES Interface Name Service Name Parameters Description /qt_robot/emotion/show 'emotion_show' 'name' Show a facial emotion given by its name /qt_robot/emotion/stop 'emotion_stop' Stops current emotion activity. Info The complete list of emotion files can be found in '~/robot/data/emotions/'. 4.4 Gesture Interface Plays recored robot gesture (arms, head) such as 'happy', 'discust', etc. The complete list of gesture files can be found in ~/robot/data/gestures/ . SUBSCRIBERS Interface Name Data Type Description /qt_robot/gesture/play 'std_msgs/String' (gesture name) Play a robot gesture given by its name SERVICES Interface Name Service Name Parameters Description /qt_robot/gesture/play 'gesture_play' 'name', 'speed' Play a robot gesture given by its name and speed (default 1.0) /qt_robot/gesture/record 'gesture_record' 'parts', 'idleParts' Start recording a new gesture. 'parts' is a string array of parts name ('head', 'left_arm','right_arm') which specifies which robot part will be used for recording the gesture. 'idleParts' must be set to 'true' to release the motor PWM and put them in idle mode. If not you must put them in idle mode using '/qt_robot/motors/setControlMode' interface /qt_robot/gesture/save 'gesture_save' 'name', 'path' stops the current recording process and save the recorded gesture given by its 'name'. 'path' specifies where to save the gesture instead of the default path /qt_robot/gesture/list 'gesture_list' return a list of a the available gestures within the default gesture path /qt_robot/gesture/stop 'gesture_stop' Stops current gesture activity. Info The default value for speed is 1.0 and it is the default speed with which the gesture got recorded. Notice If the speed param value is 0 the default speed will be used to play the gestures. Default path to record/play the gesture is ~/robot/data/gestures/ . 4.5 Behavior Interface This interface implements higher-level and more complex behaviors by combing robot basic functionality. SUBSCRIBERS Interface Name Data Type Description /qt_robot/behavior/talkText 'std_msgs/String' (message) Read a text using TTS and show talking emotion /qt_robot/behavior/talkAudio 'std_msgs/String' (audio filename) Play an audio file and show talking emotion SERVICES Interface Name Service Name Parameters Description /qt_robot/behavior/talkText 'behavior_talk_text' 'message' Read a text using TTS and show talking emotion /qt_robot/behavior/talkAudio 'behavior_talk_audio' 'filename', 'filepath' Play an audio file and show talking emotion Info To play the audio file from the default path, pass an empty string to filepath param. Notice The talkAudio and talkText services are mutually exclusive and cannot be used with Emotion Interface at the same time. 4.6 Motor Interface Motor interface provide access to the robot actuators using standard ros_control system. Currently the interface implements ROS 'JointStateController', 'JointGroupPositionController' and a custom 'QTMotorsController' controllers. Notice Before using the Motor interface, ensure that you fully understood ros_control system and have a clear understanding of what you do at the motor joint level. 4.6.1 QTrobot parts The robot joints are structured as different parts as shown bellow: head HeadYaw HeadPitch right_arm RightShoulderPitch RightShoulderRoll RightElbowRoll left_arm LeftShoulderPitch LeftShoulderRoll LeftElbowRoll SUBSCRIBERS Interface Name Data Type Description /qt_robot/joints/state sensor_msgs/JointState publishes joint states (currently only positions) /qt_robot/head_position/command std_msgs/Float64MultiArray move the robot head to desired position given by (HeadYaw, HeadPitch). /qt_robot/right_arm_position/command std_msgs/Float64MultiArray move the right_arm to desired position given by (RightShoulderPitch, RightShoulderRoll, RightElbowRoll). /qt_robot/left_arm_position/command std_msgs/Float64MultiArray move the left_arm to desired position given by (LeftShoulderPitch, LeftShoulderRoll, LeftElbowRoll). SERVICES Interface Name Service Name Parameters Description /qt_robot/motors/home 'home' 'parts' moves the desired robot part to the home position. 'parts' is an array of robot parts and/or single joint name (e.g.['left_arm', 'right_arm', 'HeadPitch']) /qt_robot/motors/setControlMode 'set_control_mode' 'parts' set the control mode (M_ON=0,M_OFF=1, M_BRAKE=2) of desired robot part. 'parts' is an array of robot parts and/or single joint name (e.g. ['left_arm', 'right_arm', 'HeadPitch']). M_ON: motor is controlled. M_OFF: motor is idle and M_BRAKE: motor is in brake mode (not controlled) /qt_robot/motors/setVelocity 'set_velocity' 'parts', 'velocity' sets the moving velocity of the desired robot part. 'parts' is an array of robot parts and/or single joint name (e.g. ['left_arm', 'right_arm', 'HeadPitch']). 'velocity' is given as percentage. Notice For safety purpose, every joint has a maximum velocity limits. For example you cannot run 'HeadPitch' joint with more than 20% of the maximum velocity. 4.7 Setting Interface This interface provides some basic setting of robot such as speaker volume control. SERVICES Interface Name Service Name Parameters Description /qt_robot/setting/setVolume 'setting_setVolume' 'volume' set the robot speaker volume to the desired level (in percentage) 4.8 Human 3D Tracking Interface This interface implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK . PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image 5. Tutorials Please check our Demo and Examples sections. You can also check our repository for the full list of examples and interesting demos for QTrobot.","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#qtrobot-interface","text":"The QTrobot (LuxAI) interface aims to facilitate accessing basic robot functionalities leveraging a set of user-friendly ROS interfaces. Each robot\u2019s functionality can be accessed in blocking and non-blocking mode using ROS publish/subscribe and Service/Client interfaces. Following are the main interfaces of QTrobot: QTrobot Emotion : implements robot facial emotion QTrobot Speech : implements robot text to speech functionality QTrobot Audio : implement a simple player to play standard audio files QTrobot Gesture : implements robot gesture control QTrobot Behavior : implements more complex behaviors by combining the robot basic functionality QTrobot Motor : implements robot motor controls using standard ros_control QTrobot 3D Camera : implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK QTrobot Settings : implements some basic setting of robot such as speaker volume control and some extra interfaces which are available for some models of QTrobot: For each of the main interfaces, there are two different way of accessing them: Using ROS Publisher/Subscribers to allow non-blocking call to the interfaces Using ROS Services for blocking and accessing more sophisticated interfaces","title":"QTrobot Interface"},{"location":"api/#1-naming-convention","text":"All the QTRobot ROS interfaces have '/qt_robot/...' prefix in front of their names. The word follows the prefix is the name of the main service (e.g. '/qt_robot/speech/...' ). Each interface may have more sub-services (methods) which come after the main service name. For example: /qt_robot/speech/say : implements say() method of QTrobot TTS /qt_robot/speech/config : implements configure() method of QTrobot TTS For user\u2019s convenience we have given the same name to the service and subscriber for each QTrobot ROS interface. That means one can access, for example, the speech functionality using ROS service call or publish/subscribe via the same interface name (e.g. '/qt_robot/speech/say' ). Please notice that some of the complex services (e.g. speech configuration for language, pitch,\u2026) are accessible only via ROS service call.","title":"1. Naming convention"},{"location":"api/#2-accessing-qtrobot-interface-from-bash","text":"You can access each Robot functionality via its publish/subscribe or Service/Client interfaces. For example to use robot 'Speech' functionality you can try the following: Using ROS Publisher $ rostopic pub /qt_robot/speech/say std_msgs/String \"data: 'I am QT'\" Using ROS Service $ rosservice call /qt_robot/speech/say \"message: 'I am QT.'\"","title":"2. Accessing QTrobot interface from bash"},{"location":"api/#3-accessing-qtrobot-interface-from-a-python-script","text":"[Non-blocking mode] The following example shows how to access QTrobot Speech functionality using ROS publish/subscribe method from python: import rospy from std_msgs.msg import String # create a publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) ... # publish a text message to TTS (non-blocking) speechSay_pub . publish ( \"Hello! I am QT!\" ) [Blocking mode] And the following example, re-implements the previous one using ROS Service/Client method from python: import rospy from std_msgs.msg import String from qt_robot_interface.srv import * # create a service clients speechSay = rospy . ServiceProxy ( '/qt_robot/speech/say' , speech_say ) ... # call the service (blocking) resp = speechSay ( \"Hello! I am QT.\" ) Notice All QTrobot service interfaces returns the status of the call upun success or failure. For example to check whether a call to a service was successful in python, you can check the return value resp.status.","title":"3. Accessing QTrobot interface from a python script"},{"location":"api/#4-list-of-available-interfaces","text":"Currently the following interfaces have been implemented: INTERFACES Functionality Interface prefix Description Speech /qt_robot/speech/... robot text to speech functionality Audio /qt_robot/audio/... simple standard audio file player Emotion /qt_robot/emotion/... robot facial emotion Gesture /qt_robot/gesture/... robot gesture control Behavior /qt_robot/behavior/... more complex behaviors by combining the robot basic functionality Motor /qt_robot/motor/... robot motor controls using standard ros_control Setting /qt_robot/setting/... basic setting of robot such as speaker volume control Human 3D Skeleton /qt_nuitrack_app/skeletons Human full body skeleton information Human 3D Hand Pos /qt_nuitrack_app/hands Human 3D hand pos and state Human Hand Gestures /qt_nuitrack_app/gestures Some basic human gestures recognition Human facial /qt_nuitrack_app/faces human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition","title":"4. List of available interfaces"},{"location":"api/#41-speech-interface","text":"This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot. following are some standard supported languages: en-US (american english) en-GB (british english) fr-FR (French) de-DE (German) SUBSCRIBERS Interface Name Data Type Description /qt_robot/speech/say 'std_msgs/String' (text) Read a text using built-in TTS SERVICES Interface Name Service Name Parameters Description /qt_robot/speech/say 'speech_say' message Read a text using built-in TTS /qt_robot/speech/config 'speech_config' 'language', 'pitch', 'speed' Configure TTS /qt_robot/speech/stop 'speech_stop' Stops current speech activity. Info Default language is set in '/opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml'. the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them.","title":"4.1 Speech Interface"},{"location":"api/#42-audio-interface","text":"Play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy . In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wavand so on. SUBSCRIBERS Interface Name Data Type Description /qt_robot/audio/play 'std_msgs/String' (audio file name) Play an sudio file SERVICES Interface Name Service Name Parameters Description /qt_robot/audio/play 'audio_play' 'filename', 'filepath' Play an audio file given by its filename and filepath /qt_robot/audio/stop 'audio_stop' Stops current audio activity. Info The default path for the audio files is '~/robot/data/audios/'. To play the audio file from the default path, pass an empty string to filepath parameter.","title":"4.2 Audio Interface"},{"location":"api/#43-emotion-interface","text":"Change the robot facial emotions such as 'QT/happy', 'QT/sad', etc. SUBSCRIBERS Interface Name Data Type Description /qt_robot/emotion/show 'std_msgs/String' (emotion name) Show a facial emotion given by its name SERVICES Interface Name Service Name Parameters Description /qt_robot/emotion/show 'emotion_show' 'name' Show a facial emotion given by its name /qt_robot/emotion/stop 'emotion_stop' Stops current emotion activity. Info The complete list of emotion files can be found in '~/robot/data/emotions/'.","title":"4.3 Emotion Interface"},{"location":"api/#44-gesture-interface","text":"Plays recored robot gesture (arms, head) such as 'happy', 'discust', etc. The complete list of gesture files can be found in ~/robot/data/gestures/ . SUBSCRIBERS Interface Name Data Type Description /qt_robot/gesture/play 'std_msgs/String' (gesture name) Play a robot gesture given by its name SERVICES Interface Name Service Name Parameters Description /qt_robot/gesture/play 'gesture_play' 'name', 'speed' Play a robot gesture given by its name and speed (default 1.0) /qt_robot/gesture/record 'gesture_record' 'parts', 'idleParts' Start recording a new gesture. 'parts' is a string array of parts name ('head', 'left_arm','right_arm') which specifies which robot part will be used for recording the gesture. 'idleParts' must be set to 'true' to release the motor PWM and put them in idle mode. If not you must put them in idle mode using '/qt_robot/motors/setControlMode' interface /qt_robot/gesture/save 'gesture_save' 'name', 'path' stops the current recording process and save the recorded gesture given by its 'name'. 'path' specifies where to save the gesture instead of the default path /qt_robot/gesture/list 'gesture_list' return a list of a the available gestures within the default gesture path /qt_robot/gesture/stop 'gesture_stop' Stops current gesture activity. Info The default value for speed is 1.0 and it is the default speed with which the gesture got recorded. Notice If the speed param value is 0 the default speed will be used to play the gestures. Default path to record/play the gesture is ~/robot/data/gestures/ .","title":"4.4 Gesture Interface"},{"location":"api/#45-behavior-interface","text":"This interface implements higher-level and more complex behaviors by combing robot basic functionality. SUBSCRIBERS Interface Name Data Type Description /qt_robot/behavior/talkText 'std_msgs/String' (message) Read a text using TTS and show talking emotion /qt_robot/behavior/talkAudio 'std_msgs/String' (audio filename) Play an audio file and show talking emotion SERVICES Interface Name Service Name Parameters Description /qt_robot/behavior/talkText 'behavior_talk_text' 'message' Read a text using TTS and show talking emotion /qt_robot/behavior/talkAudio 'behavior_talk_audio' 'filename', 'filepath' Play an audio file and show talking emotion Info To play the audio file from the default path, pass an empty string to filepath param. Notice The talkAudio and talkText services are mutually exclusive and cannot be used with Emotion Interface at the same time.","title":"4.5 Behavior Interface"},{"location":"api/#46-motor-interface","text":"Motor interface provide access to the robot actuators using standard ros_control system. Currently the interface implements ROS 'JointStateController', 'JointGroupPositionController' and a custom 'QTMotorsController' controllers. Notice Before using the Motor interface, ensure that you fully understood ros_control system and have a clear understanding of what you do at the motor joint level.","title":"4.6 Motor Interface"},{"location":"api/#461-qtrobot-parts","text":"The robot joints are structured as different parts as shown bellow: head HeadYaw HeadPitch right_arm RightShoulderPitch RightShoulderRoll RightElbowRoll left_arm LeftShoulderPitch LeftShoulderRoll LeftElbowRoll SUBSCRIBERS Interface Name Data Type Description /qt_robot/joints/state sensor_msgs/JointState publishes joint states (currently only positions) /qt_robot/head_position/command std_msgs/Float64MultiArray move the robot head to desired position given by (HeadYaw, HeadPitch). /qt_robot/right_arm_position/command std_msgs/Float64MultiArray move the right_arm to desired position given by (RightShoulderPitch, RightShoulderRoll, RightElbowRoll). /qt_robot/left_arm_position/command std_msgs/Float64MultiArray move the left_arm to desired position given by (LeftShoulderPitch, LeftShoulderRoll, LeftElbowRoll). SERVICES Interface Name Service Name Parameters Description /qt_robot/motors/home 'home' 'parts' moves the desired robot part to the home position. 'parts' is an array of robot parts and/or single joint name (e.g.['left_arm', 'right_arm', 'HeadPitch']) /qt_robot/motors/setControlMode 'set_control_mode' 'parts' set the control mode (M_ON=0,M_OFF=1, M_BRAKE=2) of desired robot part. 'parts' is an array of robot parts and/or single joint name (e.g. ['left_arm', 'right_arm', 'HeadPitch']). M_ON: motor is controlled. M_OFF: motor is idle and M_BRAKE: motor is in brake mode (not controlled) /qt_robot/motors/setVelocity 'set_velocity' 'parts', 'velocity' sets the moving velocity of the desired robot part. 'parts' is an array of robot parts and/or single joint name (e.g. ['left_arm', 'right_arm', 'HeadPitch']). 'velocity' is given as percentage. Notice For safety purpose, every joint has a maximum velocity limits. For example you cannot run 'HeadPitch' joint with more than 20% of the maximum velocity.","title":"4.6.1 QTrobot parts"},{"location":"api/#47-setting-interface","text":"This interface provides some basic setting of robot such as speaker volume control. SERVICES Interface Name Service Name Parameters Description /qt_robot/setting/setVolume 'setting_setVolume' 'volume' set the robot speaker volume to the desired level (in percentage)","title":"4.7 Setting Interface"},{"location":"api/#48-human-3d-tracking-interface","text":"This interface implements different human 3D body and facial tracking including human full body skeleton, hands pos and gestures, facial and emotion recognition using Intel Realsense camera and Nuitrack SDK . PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image","title":"4.8 Human 3D Tracking Interface"},{"location":"api/#5-tutorials","text":"Please check our Demo and Examples sections. You can also check our repository for the full list of examples and interesting demos for QTrobot.","title":"5. Tutorials"},{"location":"demos/","text":"Demos About Here is the list of all demos with QTrobot. All demos and code you can find in our github tutorial repository. Range of Motion & DOF contains motor movement demo source code! Emotion Game Image recognition emotion demo Download a PDF file for the demo source code! Memory Game Image recognition memory demo Download a PDF file for the demo source code! Gesture recognition Gesture recognition with 3D camera source code! Face, Age & Gender recognition Face, Age & Gender recognition with 3D camera source code! Voice interaction Voice interaction with Far-Field Microphone Array source code! Voice activity & direction detection Voice activity & direction detection with Far-Field Microphone Array source code! Skeleton Tracking Skeleton Tracking with 3D camera","title":"Demos"},{"location":"demos/#demos","text":"About Here is the list of all demos with QTrobot. All demos and code you can find in our github tutorial repository.","title":"Demos"},{"location":"examples/","text":"Examples About Here is the list of all examples with QTrobot . All examples and code you can find in our github tutorial repository . Interfaces Audio interface : demonstrates how to make robot sing something Emotion interface : demonstrates how to make robot show some emotion Speech interface : demonstrates how to make robot say something Motor Motors command : demonstrates how to command robot motors positions Motors gesture : demonstrates how to run prerecorded gestures Motors state : demonstrates how to read robot joints state such as positions Motors ROS Controller : demonstrates how to implement ROS controller for QTrobot Motors MoveIt : demonstrates how to use ROS MoveIT to control QTrobot arms Voice Speech Recognition - Google SpeechToText : example of google speech-to-text ros service Voice activity : demonstrates how to show voice activity Voice direction : demonstrates how to read direction of voice Vision Gesture detection : demonstrates how to read from qtrobot nuitrack gesture topic","title":"Examples"},{"location":"examples/#examples","text":"About Here is the list of all examples with QTrobot . All examples and code you can find in our github tutorial repository .","title":"Examples"},{"location":"examples/#interfaces","text":"Audio interface : demonstrates how to make robot sing something Emotion interface : demonstrates how to make robot show some emotion Speech interface : demonstrates how to make robot say something","title":"Interfaces"},{"location":"examples/#motor","text":"Motors command : demonstrates how to command robot motors positions Motors gesture : demonstrates how to run prerecorded gestures Motors state : demonstrates how to read robot joints state such as positions Motors ROS Controller : demonstrates how to implement ROS controller for QTrobot Motors MoveIt : demonstrates how to use ROS MoveIT to control QTrobot arms","title":"Motor"},{"location":"examples/#voice","text":"Speech Recognition - Google SpeechToText : example of google speech-to-text ros service Voice activity : demonstrates how to show voice activity Voice direction : demonstrates how to read direction of voice","title":"Voice"},{"location":"examples/#vision","text":"Gesture detection : demonstrates how to read from qtrobot nuitrack gesture topic","title":"Vision"},{"location":"meet_qtrobot/","text":"Hi, I'm QTrobot ( @qtrobot ) I'm toddler-like humanoid robot by LuxAI S.A . I'm a socially engaging and interactive robot with a wide areas of application. I'm helping with emotional training of children with autism, post-stroke rehabilitation and elderly cognitive and physical rehabilitation. If you want to know more about me please visit LuxAI S.A website.","title":"Meet QTrobot"},{"location":"meet_qtrobot/#hi-im-qtrobot-qtrobot","text":"I'm toddler-like humanoid robot by LuxAI S.A . I'm a socially engaging and interactive robot with a wide areas of application. I'm helping with emotional training of children with autism, post-stroke rehabilitation and elderly cognitive and physical rehabilitation. If you want to know more about me please visit LuxAI S.A website.","title":"Hi, I'm QTrobot (@qtrobot)"},{"location":"user-manual/","text":"User Manual In the following sections you will find user manual of the QTrobot. 1. QTrobot Power QTrobot power button has three color states, each indicating the robot current power state: LED OFF: robot is disconnected from its main power supply. LED with Dim Light: robot is connected to its power supply but is not running. LED Fully ON: robot is running. Power ON To power the robot on, simply connect the robot's power supply 1 .This triggers the power on process and your robot becomes ready less than a minute. After powering on, you can see the robot face is on and the motors are in their home position. If the robot is off and it is connected to the power supply 2 simply push the power button to turn on the robot. Power OFF To power off the robot, simply press the power button when it is on. You can see that the robot motors move to the parking position and the robot display turns off in less than one minute. The Dim light at the power button indicates that it is safe to disconnect the power supply. Warning Avoid turning off the robot by cutting the power supply. This may damage your product. Methods to power off the QTrobot Apart from using the power button, you can power off the robot in different ways: Powering off using the robot control panel Connect to robot WIFI hotspot Access the control panel using http://qtrrobot or http://192.168.100.1 in your web browser Click on \"Power off\" button Power off using QTrobot tablet App Connect to robot WIFI hotspot Open the Tablet App Shutdown the robot from the main menu Power off using developer terminal Connect to robot WIFI hotspot Login into robot via SSH using provided credentials Power the robot off ssh qtrobot@qtrobot sudo poweroff 2. Connecting to QTrobot WiFi Hotspot QTrobot comes with integrated WIFI interfaces. One of them provide the hotspot to remotely connect to the robot and the other one can be used to connect the robot to public WIF and internet. From your device/PC/tablet, just look for the robot WIFI hotspot SSID and connect to it. The hotspot name is your robot name (e.g. QT100). 3. Connect to QTrobot via ssh Your QTrobot comes with two integrated computers both running Ubuntu 16.04 LTS operating system. QTROBOT (ARM Cortex-A53) : This is the embedded computer of QTrobot which runs the robot main software such as motors controller, gestures, emotions and implement most of the ROS interfaces. The ROS server is running on this machine. QTPC (Intel NUC i5) : This is the high-performance computer of the robot which allows you to develop and run your high computational codes. QTPC is connected to QTROBOT via LAN and communicates to all robot modules using ROS. This machine also implements some of the QTrobot ROS interfaces such as 3D camera images. Accessing QTROBOT via SSH After connecting to the robot WIFI hotspot, open a terminal from your PC and SSH to the robot using 'qtrobot' as hostname or using IP 192.168.100.1 3 . ssh qtrobot@qtrobot qtrobot@qt password:******* Accessing QTPC via SSH There are two ways to SSH into QTPC: Via QTROBOT: after connecting to robot WIFI hotspot, first SSH to QTRBOT, then from QTROBOT terminal SSH into QTPC. ssh qtrobot@qtrobot qtrobot@qt password:******* ssh qtpc Directly SSH to QTPC: after connecting to robot WIFI hotspot, open a terminal from your PC and ssh to QTPC using 192.168.100.2 ssh qtrobot@192.168.100.2 qtrobot@qt password:******* 4. Accessing The Control Panel QTrobot has two web-based control panels one for QTROBOT and the other for QTPCP. These panels facilitate robot configuration such as connecting to the internet or to enable/disable robot startup scripts (see section Robot Autostart Scripts ). You can connect separately to each control panel using their corresponding IPs. To connect to the QTROBOT or QTPC web control panel, after connecting to the robot WIFI hotspot: For QTROBOT, open a browser and type http://192.168.100.1:8080 or http://qtrobot For QTPC, open a browser and type http://192.168.100.2:8080 Log in to the panel using the QTrobot's username and password provided to you. 5. Connecting to a Home Network and Internet You can connect the robot to your home network and internet using the WIFI from QTPC either via terminal or using the control panel of QTPC Warning Do not use the QTROBOT WIFI to connect to a network. Instead use WIFI from QTPC! Connecting to the Internet from Terminal Open a terminal and SSH to QTPC and use 'nmtui' tool: qtrobot@qtpc:~$ sudo nmtui Connecting to the Internet from Control panel Connect to the QTPC control panel and navigate to \"Configure WIFI\". Then simply follow the instructions. 6. Robot Autostart scripts Warning Assure that you completely understood the purpose of each script and you are aware of what you are doing Autostart scripts are simply some bash scripts which are executed at robot startup time. These scripts prepare robot network, setup ROS environment, launch QTrobot motor and other controllers, setup log files and etc. Where are they located? there are two sets of autostart scripts on QTrobot: 1. Autostart scripts on RPI 2. Autostart scripts on NUC Both sets are located under ~/robot/autostart folder. How they are executed at startup time? The scripts are run by linux Cron job scheduler. In fact, a specific corn job is configured to run autostart_screens.sh script at linux boot. The other scripts are launched by autostart_screens.sh and their output are redirected to the corresponding log files. To add your own script (e.g. start_my_script.sh ) you can simply add the following line to the autostart_screens.sh : { wait_for_network run_script \"start_qtroutes.sh\" ... ... run_script \"start_my_script.sh\" } & >> ${ LOG_FILE } How can I access them? You can access them via terminal of NUC/RPI or via QTrobot web interface for enable/disable them: Web interface on RPI: http://192.168.100.1:8080 Web interface on NUC: http://192.168.100.2:8080 What are the main autostart scripts? As I explained above, there are some important scripts which prepare the robot network, ROS environment and launch the motor controller and other robot interfaces. All of these scripts are running on RPI and are necessary for functionality of QTrobot. Here are the main startup scripts on RPI : start_qt_routes.sh : prepares network routing between RPI and NUC for internet sharing and etc. start_roscore.sh : launches roscore start_qtpc.sh : power on NUC via wake-on-lan. start_qt_motor.sh : launch qt_motor node including motor and gesture controllers. start_qt_robot_interface.sh : launch qt_robot_interface node which implements QTrobot emotion (face), audio, speech, behavior and other interfaces. - start_qt_webconfing.sh : launch QTrobot web config interface on RPI Here are the main startup scripts on NUC : The NUC pc is most convenient place to develop code and run your own autostart scripts and apps. Most of the default scripts are simply running the QTrobot open source demos and examples and can be safely disabled and/or modified. There is nothing especial running by autostart scripts on NUC except: - start_qt_nuitrack_app.sh : which run Nuitrack skeleton tracking and camera interfaces. - start_qt_webconfing.sh : launch QTrobot web config interface on NUC Where are the log files? All programs/ROS nodes which are run by QTrobot autostart scripts redirect their standard output (info/war/error messages) to their corresponding log file. These log files can be found under ~/robot/autostart/logs folder. 7. Working directly on QTPC Your robot has a standard Ubuntu-based PC. You can use QTPC in the same way you use a standard desktop. That greatly facilitates and accelerates your development cycle as you have a full-featured desktop directly with your QTrobot. To do that: Connect a USB-C extension (or USB-C to HDMI cable) to the USB-C port of QTrobot. Connect your mouse and keyboard to your USB-C extension. Alternatively you can use weirless mouse/keyboard and connect the dongle directly to USB3 port of QTrobot. Connect the HDMI cable of your display to the USB-C extension or use a USB-C to HDMI cable. You will see Ubuntu desktop running Notice You may need to connect the HDMI cable before powering up the robot to make QTPC aware of any external display. 8. Working on your PC Warning You don NOT need this installation if you are programming directly on the QTrobot. See previous section Working directly on QTPC. Installing ROS The following steps guide you through the installation of ROS Kinetic. For the complete installation guide of ROS see Install ROS Notice Please notice that QTrobot can be used with other versions of ROS such as Lunar and Melodic. Installing ROS 1. Configure your Ubuntu repositories You need to configure your Ubuntu repositories to allow \"restricted\" \"universe\" and \"multiverse\". You can check Ubuntu documentation for doing this. 2. Setup your sources.list Setup your computer to accept software from packages.ros.org. sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' 3. Set up your keys sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 4. Install Now you can update and install ros-kinetic-desktop-full sudo apt-get update && sudo apt-get install ros-kinetic-desktop-full 5. Setup Catkin workspace source /opt/ros/kinetic/setup.bash mkdir -p ~/catkin_ws/src && cd ~/catkin_ws/ && catkin_make 6. Install QTrobot ROS header files First you need to clone our github Software repository. You can do that with this command: git clone https://github.com/luxai-qtrobot/software.git After that go in the folder and copy the content of the headers folder to your Catkin devel folder. cp -r headers/* ~/catkin_ws/devel/ Now you can access all QTrobot ROS services and messages from your PC. Configure ROS environment for QTrobot The following steps guide you through the configuration of your ROS environment for QTrobot. ROS environment for QTrobot You need to edit your ~/.bash_aliases and add following lines. Change ROS_IP to your PC IP. source /opt/ros/kinetic/setup.bash source ~/catkin_ws/devel/setup.bash ## QTrobot export ROS_IP = <your PC IP address> export ROS_MASTER_URI = http://192.168.100.1:11311 Which is my IP address ? To get your IP address, First you need to connect to the QTrobot WiFi hotspot. Then open a terminal on your PC and run the following command: ifconfig To apply this configuration you can open a new terminal or run this command: source ~/.bash_aliases Check your ROS setup Last step is to connect to the QTrobot WiFi hotspot and test your ROS environment. Test To test your environment you can list all rostopics from QTrobot. You can do that with running this command. rostopic list If everything is working you should see output similar to this one. /rosout /qt_robot/audio/play /qt_robot/behavior/talkAudio /qt_robot/emotion/show /qt_robot/gesture/play /qt_robot/head_position/command /qt_robot/joints/state ... Now you can start having fun. 9. QTrobot ROS API The QTrobot (LuxAI) interface aims to facilitate accessing basic robot functionalities leveraging a set of user-friendly ROS interfaces. Each robot\u2019s functionality can be accessed in blocking and non-blocking mode using ROS publish/subscribe and Service/Client interfaces. Info Visit QTrobot ROS wiki for ROS API Demos & Examples Please check our Demos and Examples to check how to use and implement QTrobot interfaces 10. Calling QTrobot APIs You can access each robot's functionality via its publish/subscribe or service/client interfaces. For example, to use the robot 'Speech' functionality from terminal using ROS publisher, you can try: rostopic pub /qt_robot/speech/say std_msgs/String \"data: 'Hello!'\" or by using ROS service call: rosservice call /qt_robot/speech/say \"message: 'Hello!'\" or from a python script: import rospy from std_msgs.msg import String # create a publisher pub = rospy.Publisher ( '/qt_robot/speech/say' , String, queue_size = 10 ) # publish a text message to TTS (non-blocking) pub.publish ( \"Hello! I am QT!\" ) 11. Updating QTrobot packages Notice To update QTrobot packages your robot should be connected to the internet. For that you can follow instructions in 5. Connecting to a Home Network and Internet Connect to the robot WIFI hotspot, open a terminal from your PC and SSH to the QTrobot using qtrobot as hostname or using IP 192.168.100.1 3 For the ros-kinetic-qt-motor and ros-kinetic-qt-robot-interface packages you need to do a backup of the configuration before updating them. Following steps will explain how to update packages. Please follow the instructions carefully. How to update ros-kinetic-qt-motor package . make a copy of your current QTrobot hardware configuration ( qtrobot-hardware.yaml ) You need this file to put it back after updating the package: cp /opt/ros/kinetic/share/qt_motor/config/qtrobot-hardware.yaml ~/ Move to the packages folder and pull new updates from that folder. cd ~/robot/packages && git pull Install the latest version of ros-kinetic-qt-motor , in my case it is 1.2.0-0 : sudo dpkg -i deb/ros-kinetic-qt-motor_1.2.0-0xenial_armhf.deb Put back your QTrobot hardware configuration ( qtrobot-hardware.yaml ): sudo cp ~/qtrobot-hardware.yaml /opt/ros/kinetic/share/qt_motor/config/ How to update ros-kinetic-qt-robot-interface . make a copy of your current QTrobot interface configuration ( qtrobot-interface.yaml ) You need this file to put it back after updating the package: cp /opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml ~/ Move to the packages folder and pull new updates from that folder. cd ~/robot/packages && git pull Install the latest version of ros-kinetic-qt-robot-interface , in my case it is 1.2.0-0 : sudo dpkg -i deb/ros-kinetic-qt-robot-interface_1.2.0-0xenial_armhf.deb Put back your QTrobot interface configuration ( qtrobot-interface.yaml ): sudo cp ~/qtrobot-interface.yaml /opt/ros/kinetic/share/qt_robot_interface/config/ How to update other packages. Move to the packages folder and pull new updates from that folder. cd ~/robot/packages && git pull Install new packages (from robot/packages/deb ) with this command: sudo dpkg -i deb/ros-kinetic-<name-of-the-package>.deb Example For example to install latest version of ros-kinetic-qt-gesture-controller version 1.3.0-0 : sudo dpkg -i deb/ros-kinetic-qt-gesture-controller_1.3.0-0xenial_armhf.deb Only use the provided power supply with your robot. \u21a9 In this case you can notice a dim light on the power button. \u21a9 Windows users can use Putty ( www.putty.org ) \u21a9 \u21a9","title":"Quick Start"},{"location":"user-manual/#user-manual","text":"In the following sections you will find user manual of the QTrobot.","title":"User Manual"},{"location":"user-manual/#1-qtrobot-power","text":"QTrobot power button has three color states, each indicating the robot current power state: LED OFF: robot is disconnected from its main power supply. LED with Dim Light: robot is connected to its power supply but is not running. LED Fully ON: robot is running.","title":"1. QTrobot Power"},{"location":"user-manual/#power-on","text":"To power the robot on, simply connect the robot's power supply 1 .This triggers the power on process and your robot becomes ready less than a minute. After powering on, you can see the robot face is on and the motors are in their home position. If the robot is off and it is connected to the power supply 2 simply push the power button to turn on the robot.","title":"Power ON"},{"location":"user-manual/#power-off","text":"To power off the robot, simply press the power button when it is on. You can see that the robot motors move to the parking position and the robot display turns off in less than one minute. The Dim light at the power button indicates that it is safe to disconnect the power supply. Warning Avoid turning off the robot by cutting the power supply. This may damage your product.","title":"Power OFF"},{"location":"user-manual/#methods-to-power-off-the-qtrobot","text":"Apart from using the power button, you can power off the robot in different ways: Powering off using the robot control panel Connect to robot WIFI hotspot Access the control panel using http://qtrrobot or http://192.168.100.1 in your web browser Click on \"Power off\" button Power off using QTrobot tablet App Connect to robot WIFI hotspot Open the Tablet App Shutdown the robot from the main menu Power off using developer terminal Connect to robot WIFI hotspot Login into robot via SSH using provided credentials Power the robot off ssh qtrobot@qtrobot sudo poweroff","title":"Methods to power off the QTrobot"},{"location":"user-manual/#2-connecting-to-qtrobot-wifi-hotspot","text":"QTrobot comes with integrated WIFI interfaces. One of them provide the hotspot to remotely connect to the robot and the other one can be used to connect the robot to public WIF and internet. From your device/PC/tablet, just look for the robot WIFI hotspot SSID and connect to it. The hotspot name is your robot name (e.g. QT100).","title":"2. Connecting to QTrobot WiFi Hotspot"},{"location":"user-manual/#3-connect-to-qtrobot-via-ssh","text":"Your QTrobot comes with two integrated computers both running Ubuntu 16.04 LTS operating system. QTROBOT (ARM Cortex-A53) : This is the embedded computer of QTrobot which runs the robot main software such as motors controller, gestures, emotions and implement most of the ROS interfaces. The ROS server is running on this machine. QTPC (Intel NUC i5) : This is the high-performance computer of the robot which allows you to develop and run your high computational codes. QTPC is connected to QTROBOT via LAN and communicates to all robot modules using ROS. This machine also implements some of the QTrobot ROS interfaces such as 3D camera images.","title":"3. Connect to QTrobot via ssh"},{"location":"user-manual/#accessing-qtrobot-via-ssh","text":"After connecting to the robot WIFI hotspot, open a terminal from your PC and SSH to the robot using 'qtrobot' as hostname or using IP 192.168.100.1 3 . ssh qtrobot@qtrobot qtrobot@qt password:*******","title":"Accessing QTROBOT via SSH"},{"location":"user-manual/#accessing-qtpc-via-ssh","text":"There are two ways to SSH into QTPC: Via QTROBOT: after connecting to robot WIFI hotspot, first SSH to QTRBOT, then from QTROBOT terminal SSH into QTPC. ssh qtrobot@qtrobot qtrobot@qt password:******* ssh qtpc Directly SSH to QTPC: after connecting to robot WIFI hotspot, open a terminal from your PC and ssh to QTPC using 192.168.100.2 ssh qtrobot@192.168.100.2 qtrobot@qt password:*******","title":"Accessing QTPC via SSH"},{"location":"user-manual/#4-accessing-the-control-panel","text":"QTrobot has two web-based control panels one for QTROBOT and the other for QTPCP. These panels facilitate robot configuration such as connecting to the internet or to enable/disable robot startup scripts (see section Robot Autostart Scripts ). You can connect separately to each control panel using their corresponding IPs. To connect to the QTROBOT or QTPC web control panel, after connecting to the robot WIFI hotspot: For QTROBOT, open a browser and type http://192.168.100.1:8080 or http://qtrobot For QTPC, open a browser and type http://192.168.100.2:8080 Log in to the panel using the QTrobot's username and password provided to you.","title":"4. Accessing The Control Panel"},{"location":"user-manual/#5-connecting-to-a-home-network-and-internet","text":"You can connect the robot to your home network and internet using the WIFI from QTPC either via terminal or using the control panel of QTPC Warning Do not use the QTROBOT WIFI to connect to a network. Instead use WIFI from QTPC!","title":"5. Connecting to a Home Network and Internet"},{"location":"user-manual/#connecting-to-the-internet-from-terminal","text":"Open a terminal and SSH to QTPC and use 'nmtui' tool: qtrobot@qtpc:~$ sudo nmtui","title":"Connecting to the Internet from Terminal"},{"location":"user-manual/#connecting-to-the-internet-from-control-panel","text":"Connect to the QTPC control panel and navigate to \"Configure WIFI\". Then simply follow the instructions.","title":"Connecting to the Internet from Control panel"},{"location":"user-manual/#6-robot-autostart-scripts","text":"Warning Assure that you completely understood the purpose of each script and you are aware of what you are doing Autostart scripts are simply some bash scripts which are executed at robot startup time. These scripts prepare robot network, setup ROS environment, launch QTrobot motor and other controllers, setup log files and etc.","title":"6. Robot Autostart scripts"},{"location":"user-manual/#where-are-they-located","text":"there are two sets of autostart scripts on QTrobot: 1. Autostart scripts on RPI 2. Autostart scripts on NUC Both sets are located under ~/robot/autostart folder.","title":"Where are they located?"},{"location":"user-manual/#how-they-are-executed-at-startup-time","text":"The scripts are run by linux Cron job scheduler. In fact, a specific corn job is configured to run autostart_screens.sh script at linux boot. The other scripts are launched by autostart_screens.sh and their output are redirected to the corresponding log files. To add your own script (e.g. start_my_script.sh ) you can simply add the following line to the autostart_screens.sh : { wait_for_network run_script \"start_qtroutes.sh\" ... ... run_script \"start_my_script.sh\" } & >> ${ LOG_FILE }","title":"How they are executed at startup time?"},{"location":"user-manual/#how-can-i-access-them","text":"You can access them via terminal of NUC/RPI or via QTrobot web interface for enable/disable them: Web interface on RPI: http://192.168.100.1:8080 Web interface on NUC: http://192.168.100.2:8080","title":"How can I access them?"},{"location":"user-manual/#what-are-the-main-autostart-scripts","text":"As I explained above, there are some important scripts which prepare the robot network, ROS environment and launch the motor controller and other robot interfaces. All of these scripts are running on RPI and are necessary for functionality of QTrobot.","title":"What are the main autostart scripts?"},{"location":"user-manual/#here-are-the-main-startup-scripts-on-rpi","text":"start_qt_routes.sh : prepares network routing between RPI and NUC for internet sharing and etc. start_roscore.sh : launches roscore start_qtpc.sh : power on NUC via wake-on-lan. start_qt_motor.sh : launch qt_motor node including motor and gesture controllers. start_qt_robot_interface.sh : launch qt_robot_interface node which implements QTrobot emotion (face), audio, speech, behavior and other interfaces. - start_qt_webconfing.sh : launch QTrobot web config interface on RPI","title":"Here are the main startup scripts on RPI:"},{"location":"user-manual/#here-are-the-main-startup-scripts-on-nuc","text":"The NUC pc is most convenient place to develop code and run your own autostart scripts and apps. Most of the default scripts are simply running the QTrobot open source demos and examples and can be safely disabled and/or modified. There is nothing especial running by autostart scripts on NUC except: - start_qt_nuitrack_app.sh : which run Nuitrack skeleton tracking and camera interfaces. - start_qt_webconfing.sh : launch QTrobot web config interface on NUC","title":"Here are the main startup scripts on NUC:"},{"location":"user-manual/#where-are-the-log-files","text":"All programs/ROS nodes which are run by QTrobot autostart scripts redirect their standard output (info/war/error messages) to their corresponding log file. These log files can be found under ~/robot/autostart/logs folder.","title":"Where are the log files?"},{"location":"user-manual/#7-working-directly-on-qtpc","text":"Your robot has a standard Ubuntu-based PC. You can use QTPC in the same way you use a standard desktop. That greatly facilitates and accelerates your development cycle as you have a full-featured desktop directly with your QTrobot. To do that: Connect a USB-C extension (or USB-C to HDMI cable) to the USB-C port of QTrobot. Connect your mouse and keyboard to your USB-C extension. Alternatively you can use weirless mouse/keyboard and connect the dongle directly to USB3 port of QTrobot. Connect the HDMI cable of your display to the USB-C extension or use a USB-C to HDMI cable. You will see Ubuntu desktop running Notice You may need to connect the HDMI cable before powering up the robot to make QTPC aware of any external display.","title":"7. Working directly on QTPC"},{"location":"user-manual/#8-working-on-your-pc","text":"Warning You don NOT need this installation if you are programming directly on the QTrobot. See previous section Working directly on QTPC.","title":"8. Working on your PC"},{"location":"user-manual/#installing-ros","text":"The following steps guide you through the installation of ROS Kinetic. For the complete installation guide of ROS see Install ROS Notice Please notice that QTrobot can be used with other versions of ROS such as Lunar and Melodic. Installing ROS 1. Configure your Ubuntu repositories You need to configure your Ubuntu repositories to allow \"restricted\" \"universe\" and \"multiverse\". You can check Ubuntu documentation for doing this. 2. Setup your sources.list Setup your computer to accept software from packages.ros.org. sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' 3. Set up your keys sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 4. Install Now you can update and install ros-kinetic-desktop-full sudo apt-get update && sudo apt-get install ros-kinetic-desktop-full 5. Setup Catkin workspace source /opt/ros/kinetic/setup.bash mkdir -p ~/catkin_ws/src && cd ~/catkin_ws/ && catkin_make 6. Install QTrobot ROS header files First you need to clone our github Software repository. You can do that with this command: git clone https://github.com/luxai-qtrobot/software.git After that go in the folder and copy the content of the headers folder to your Catkin devel folder. cp -r headers/* ~/catkin_ws/devel/ Now you can access all QTrobot ROS services and messages from your PC.","title":"Installing ROS"},{"location":"user-manual/#configure-ros-environment-for-qtrobot","text":"The following steps guide you through the configuration of your ROS environment for QTrobot. ROS environment for QTrobot You need to edit your ~/.bash_aliases and add following lines. Change ROS_IP to your PC IP. source /opt/ros/kinetic/setup.bash source ~/catkin_ws/devel/setup.bash ## QTrobot export ROS_IP = <your PC IP address> export ROS_MASTER_URI = http://192.168.100.1:11311 Which is my IP address ? To get your IP address, First you need to connect to the QTrobot WiFi hotspot. Then open a terminal on your PC and run the following command: ifconfig To apply this configuration you can open a new terminal or run this command: source ~/.bash_aliases","title":"Configure ROS environment for QTrobot"},{"location":"user-manual/#check-your-ros-setup","text":"Last step is to connect to the QTrobot WiFi hotspot and test your ROS environment. Test To test your environment you can list all rostopics from QTrobot. You can do that with running this command. rostopic list If everything is working you should see output similar to this one. /rosout /qt_robot/audio/play /qt_robot/behavior/talkAudio /qt_robot/emotion/show /qt_robot/gesture/play /qt_robot/head_position/command /qt_robot/joints/state ... Now you can start having fun.","title":"Check your ROS setup"},{"location":"user-manual/#9-qtrobot-ros-api","text":"The QTrobot (LuxAI) interface aims to facilitate accessing basic robot functionalities leveraging a set of user-friendly ROS interfaces. Each robot\u2019s functionality can be accessed in blocking and non-blocking mode using ROS publish/subscribe and Service/Client interfaces. Info Visit QTrobot ROS wiki for ROS API Demos & Examples Please check our Demos and Examples to check how to use and implement QTrobot interfaces","title":"9. QTrobot ROS API"},{"location":"user-manual/#10-calling-qtrobot-apis","text":"You can access each robot's functionality via its publish/subscribe or service/client interfaces. For example, to use the robot 'Speech' functionality from terminal using ROS publisher, you can try: rostopic pub /qt_robot/speech/say std_msgs/String \"data: 'Hello!'\" or by using ROS service call: rosservice call /qt_robot/speech/say \"message: 'Hello!'\" or from a python script: import rospy from std_msgs.msg import String # create a publisher pub = rospy.Publisher ( '/qt_robot/speech/say' , String, queue_size = 10 ) # publish a text message to TTS (non-blocking) pub.publish ( \"Hello! I am QT!\" )","title":"10. Calling QTrobot APIs"},{"location":"user-manual/#11-updating-qtrobot-packages","text":"Notice To update QTrobot packages your robot should be connected to the internet. For that you can follow instructions in 5. Connecting to a Home Network and Internet Connect to the robot WIFI hotspot, open a terminal from your PC and SSH to the QTrobot using qtrobot as hostname or using IP 192.168.100.1 3 For the ros-kinetic-qt-motor and ros-kinetic-qt-robot-interface packages you need to do a backup of the configuration before updating them. Following steps will explain how to update packages. Please follow the instructions carefully.","title":"11. Updating QTrobot packages"},{"location":"user-manual/#how-to-update-ros-kinetic-qt-motor-package","text":"make a copy of your current QTrobot hardware configuration ( qtrobot-hardware.yaml ) You need this file to put it back after updating the package: cp /opt/ros/kinetic/share/qt_motor/config/qtrobot-hardware.yaml ~/ Move to the packages folder and pull new updates from that folder. cd ~/robot/packages && git pull Install the latest version of ros-kinetic-qt-motor , in my case it is 1.2.0-0 : sudo dpkg -i deb/ros-kinetic-qt-motor_1.2.0-0xenial_armhf.deb Put back your QTrobot hardware configuration ( qtrobot-hardware.yaml ): sudo cp ~/qtrobot-hardware.yaml /opt/ros/kinetic/share/qt_motor/config/","title":"How to update ros-kinetic-qt-motor package."},{"location":"user-manual/#how-to-update-ros-kinetic-qt-robot-interface","text":"make a copy of your current QTrobot interface configuration ( qtrobot-interface.yaml ) You need this file to put it back after updating the package: cp /opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml ~/ Move to the packages folder and pull new updates from that folder. cd ~/robot/packages && git pull Install the latest version of ros-kinetic-qt-robot-interface , in my case it is 1.2.0-0 : sudo dpkg -i deb/ros-kinetic-qt-robot-interface_1.2.0-0xenial_armhf.deb Put back your QTrobot interface configuration ( qtrobot-interface.yaml ): sudo cp ~/qtrobot-interface.yaml /opt/ros/kinetic/share/qt_robot_interface/config/","title":"How to update ros-kinetic-qt-robot-interface."},{"location":"user-manual/#how-to-update-other-packages","text":"Move to the packages folder and pull new updates from that folder. cd ~/robot/packages && git pull Install new packages (from robot/packages/deb ) with this command: sudo dpkg -i deb/ros-kinetic-<name-of-the-package>.deb Example For example to install latest version of ros-kinetic-qt-gesture-controller version 1.3.0-0 : sudo dpkg -i deb/ros-kinetic-qt-gesture-controller_1.3.0-0xenial_armhf.deb Only use the provided power supply with your robot. \u21a9 In this case you can notice a dim light on the power button. \u21a9 Windows users can use Putty ( www.putty.org ) \u21a9 \u21a9","title":"How to update other packages."},{"location":"FAQ/Audio/","text":"Audio - FAQ Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1.Can I stream microphone audio? When using audio_common you just need to configure capture.launch file to use proper device, so our mic is hw:1,0. Running audio_capture it will create \"/audio/audio\" topic and on that topic you can listen to the mic, read data etc. There is already tutorial on that page how to set it up. You can also try this respeaker_ros and use some of this topics : /sound_direction # Result of DoA /sound_localization # Result of DoA as Pose /is_speeching # Result of VAD /audio # Raw audio /speech_audio # Audio data while speeching","title":"Audio"},{"location":"FAQ/Audio/#audio-faq","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them.","title":"Audio - FAQ"},{"location":"FAQ/Audio/#1can-i-stream-microphone-audio","text":"When using audio_common you just need to configure capture.launch file to use proper device, so our mic is hw:1,0. Running audio_capture it will create \"/audio/audio\" topic and on that topic you can listen to the mic, read data etc. There is already tutorial on that page how to set it up. You can also try this respeaker_ros and use some of this topics : /sound_direction # Result of DoA /sound_localization # Result of DoA as Pose /is_speeching # Result of VAD /audio # Raw audio /speech_audio # Audio data while speeching","title":"1.Can I stream microphone audio?"},{"location":"FAQ/Autostart/","text":"Autostart - FAQ Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1. What are autostart scripts? Please check our User manual about Robot Autostart Scripts","title":"Autostart"},{"location":"FAQ/Autostart/#autostart-faq","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them.","title":"Autostart - FAQ"},{"location":"FAQ/Autostart/#1-what-are-autostart-scripts","text":"Please check our User manual about Robot Autostart Scripts","title":"1. What are autostart scripts?"},{"location":"FAQ/Miscellaneous/","text":"Miscellaneous - FAQ Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1. Can I do a backup of QTrobot? QTrobot has two computational components: RPI (Ubuntu 16.04 LTS) Intel NUC PC (Ubuntu 16.04 LTS) Except Nuitrack ROS interface (qt_nuitrack_app) all other qt software are running on RPI. qt_nuitrack_app and other demo apps are all available on our github and you can recover/update it whenever you want. You can backup the config files of RPI for your own recovery. All QTrobot startup scripts, gestures, emotions, etc. are located under ~/robot folder in RPI. In case of emergency recovery, we can send you the SD card image. we can prepare this image somehow you can flash it to a USB stick, plug it into QTrobot (USB) to recover the image. 2. Can I use C# to program QTrobot? QTrobot APIs are based on the most popular software framework in robotic, ROS \u2013 a publish/subscribe middleware. ROS by default supports C++ and Python but other languages are covered via different open-source and well-maintained client libraries. Indeed you can use C# to program QTrobot. You can refer to ros-sharp open-source library which leverages websockets for underlying communication and cover all QTrobot APIs. It has good integration with Unity3D too. We have also developed different Android apps using websockets in Jscript and using native Java APIs for our robot. 3. How to connect Bluetooth mouse/keyboard to QTrobot? This tutorial explain how to pair and connect a Bluetooth mouse/keyboard to QTrobot NUC pc via terminal/ssh. The same procedure works also for RPI. Notice : for RPI you may need to run the bluetoothctl with sudo ! Step 1: ssh to QTPC connect to the QTrobot wifi and ssh into QTPC: $ ssh qtrobot @192.168.100.2 Step 2: launch bluetoothctl qtrobot @QTPC : ~ $ bluetoothctl [ NEW ] Controller F8 : 63 : 3 F : 40 : 61 : B2 QTPC [ default ] Step 3: turn bluetooth power on and register the agent [ bluetooth ] # power on Changing power on succeeded [ bluetooth ] # agent on Agent registered [ bluetooth ] # default - agent Default agent request successful Step 4: scan and pair the bluetooth device to scan the bluettoth devices: [ bluetooth ] # scan on Discovery started [ CHG ] Controller F8 : 63 : 3 F : 40 : 61 : B2 Discovering : yes [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Name : Bluetooth 3.0 Keyboard ... in my case Device 17:13:00:00:8A:04 Name: Bluetooth 3.0 Keyboard is what I am looking for. Now to pair the bluetooth device: [ bluetooth ] # pair 17 : 13 : 00 : 00 : 8 A : 04 Attempting to pair with 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Connected : yes [ agent ] PIN code : xxxx ... [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Paired : yes Pairing successful Some keyboards require to type a PIN code for pairing. Just type the code using your bluetooth keyboard . You do not see anything on the terminal. The code is sent directly by your keyboard while you are typing it. Step 5: trust and connect to the device: trust the device: [ bluetooth ] # trust 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Trusted : yes and finally connect to the bluettoh/keyboard mouse: [ bluetooth ] # connect 17 : 13 : 00 : 00 : 8 A : 04 Attempting to connect to 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Connected : yes Connection successful 4. Is QTrobot SDK open source? QTrobot SDK software is not licensed as ope source. For the time being, we do not have any plan to make them open source. But these are just few software components which implement the required basic ROS interfaces such as motor control, gesture, speech, emotion. The main components (implemented as ROS node) are: qt_motor including motor_controller and gesture_controller plugins implement robot motor joints control and robot gesture record/play functionalities. qt_robot_interface implements speech, audio, basic behavior and setting functionalities. You can always open a new issue/feature request for those interfaces and we do our best to fix/implement them. The rest of the codes including all demo application are open source and you are more than welcome to contribute to those. :)","title":"Miscellaneous"},{"location":"FAQ/Miscellaneous/#miscellaneous-faq","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them.","title":"Miscellaneous - FAQ"},{"location":"FAQ/Miscellaneous/#1-can-i-do-a-backup-of-qtrobot","text":"QTrobot has two computational components: RPI (Ubuntu 16.04 LTS) Intel NUC PC (Ubuntu 16.04 LTS) Except Nuitrack ROS interface (qt_nuitrack_app) all other qt software are running on RPI. qt_nuitrack_app and other demo apps are all available on our github and you can recover/update it whenever you want. You can backup the config files of RPI for your own recovery. All QTrobot startup scripts, gestures, emotions, etc. are located under ~/robot folder in RPI. In case of emergency recovery, we can send you the SD card image. we can prepare this image somehow you can flash it to a USB stick, plug it into QTrobot (USB) to recover the image.","title":"1. Can I do a backup of QTrobot?"},{"location":"FAQ/Miscellaneous/#2-can-i-use-c-to-program-qtrobot","text":"QTrobot APIs are based on the most popular software framework in robotic, ROS \u2013 a publish/subscribe middleware. ROS by default supports C++ and Python but other languages are covered via different open-source and well-maintained client libraries. Indeed you can use C# to program QTrobot. You can refer to ros-sharp open-source library which leverages websockets for underlying communication and cover all QTrobot APIs. It has good integration with Unity3D too. We have also developed different Android apps using websockets in Jscript and using native Java APIs for our robot.","title":"2. Can I use C# to program QTrobot?"},{"location":"FAQ/Miscellaneous/#3-how-to-connect-bluetooth-mousekeyboard-to-qtrobot","text":"This tutorial explain how to pair and connect a Bluetooth mouse/keyboard to QTrobot NUC pc via terminal/ssh. The same procedure works also for RPI. Notice : for RPI you may need to run the bluetoothctl with sudo !","title":"3. How to connect Bluetooth mouse/keyboard to QTrobot?"},{"location":"FAQ/Miscellaneous/#step-1-ssh-to-qtpc","text":"connect to the QTrobot wifi and ssh into QTPC: $ ssh qtrobot @192.168.100.2","title":"Step 1: ssh to QTPC"},{"location":"FAQ/Miscellaneous/#step-2-launch-bluetoothctl","text":"qtrobot @QTPC : ~ $ bluetoothctl [ NEW ] Controller F8 : 63 : 3 F : 40 : 61 : B2 QTPC [ default ]","title":"Step 2: launch bluetoothctl"},{"location":"FAQ/Miscellaneous/#step-3-turn-bluetooth-power-on-and-register-the-agent","text":"[ bluetooth ] # power on Changing power on succeeded [ bluetooth ] # agent on Agent registered [ bluetooth ] # default - agent Default agent request successful","title":"Step 3: turn bluetooth power on and register the agent"},{"location":"FAQ/Miscellaneous/#step-4-scan-and-pair-the-bluetooth-device","text":"to scan the bluettoth devices: [ bluetooth ] # scan on Discovery started [ CHG ] Controller F8 : 63 : 3 F : 40 : 61 : B2 Discovering : yes [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Name : Bluetooth 3.0 Keyboard ... in my case Device 17:13:00:00:8A:04 Name: Bluetooth 3.0 Keyboard is what I am looking for. Now to pair the bluetooth device: [ bluetooth ] # pair 17 : 13 : 00 : 00 : 8 A : 04 Attempting to pair with 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Connected : yes [ agent ] PIN code : xxxx ... [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Paired : yes Pairing successful Some keyboards require to type a PIN code for pairing. Just type the code using your bluetooth keyboard . You do not see anything on the terminal. The code is sent directly by your keyboard while you are typing it.","title":"Step 4: scan and pair the bluetooth device"},{"location":"FAQ/Miscellaneous/#step-5-trust-and-connect-to-the-device","text":"trust the device: [ bluetooth ] # trust 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Trusted : yes and finally connect to the bluettoh/keyboard mouse: [ bluetooth ] # connect 17 : 13 : 00 : 00 : 8 A : 04 Attempting to connect to 17 : 13 : 00 : 00 : 8 A : 04 [ CHG ] Device 17 : 13 : 00 : 00 : 8 A : 04 Connected : yes Connection successful","title":"Step 5: trust and connect to the device:"},{"location":"FAQ/Miscellaneous/#4-is-qtrobot-sdk-open-source","text":"QTrobot SDK software is not licensed as ope source. For the time being, we do not have any plan to make them open source. But these are just few software components which implement the required basic ROS interfaces such as motor control, gesture, speech, emotion. The main components (implemented as ROS node) are: qt_motor including motor_controller and gesture_controller plugins implement robot motor joints control and robot gesture record/play functionalities. qt_robot_interface implements speech, audio, basic behavior and setting functionalities. You can always open a new issue/feature request for those interfaces and we do our best to fix/implement them. The rest of the codes including all demo application are open source and you are more than welcome to contribute to those. :)","title":"4. Is QTrobot SDK open source?"},{"location":"FAQ/Motors/","text":"Motors - FAQ Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1. Does the QT Motor interface implement joint limits? The short answer is Yes . QTrobot motor interface sets all joint, torque limits of each motor independently at the startup. Each motor separately does the joint limit check on each position command that it receives. NOTICE : This does not mean that the QTrobot has self collision awareness when moving its joints! 2. Gesture kinematic diagram/QTrobot URDF Gesture files the gesture are simply xml files containing robots joint waypoints and their timestamps. They are located under ~/robot/data/gestures/ folder in QTRP (head) board. here is an example: <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\" ?> <gesture> <name> QT/bye </name> <parts> <part> right_arm </part> <part> left_arm </part> </parts> <duration> 5.54 </duration> <waypoints count= \"97\" > <point time= \"1558971402671926152\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -62.90 </LeftShoulderRoll> <RightElbowRoll> -31.90 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -59.30 </RightShoulderRoll> </point> <point time= \"1558971402704626777\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -64.80 </LeftShoulderRoll> <RightElbowRoll> -30.60 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -60.60 </RightShoulderRoll> </point> ... QTrobot Joints configuration in QTRP, under /opt/ros/kinetic/share/qt_motor/ there are motor controller config files: qtrobot-hardware.yaml : contains each motor configuration such as joints limit, PID, calibration offsets, etc. qtrobot-controller.yaml : contains QTrobot controller interface configuration such as joint_state , gesture , etc. These interface are implemented using ROS control system. QTRobot URDF You can find the QTrobot URDF fils in the following links: - qtrobot.pdf - qtrobot_urdf.zip You can also load them in Gazebo to visualize joint configurations and test them: $ roslaunch urdf_tutorial display.launch model: = qtrobot.urdf you need to copy the qtrobot.urdf file to the proper folder in your PC (where you have installed Gazebo). Check the display.launch to find the path.","title":"Motors"},{"location":"FAQ/Motors/#motors-faq","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them.","title":"Motors - FAQ"},{"location":"FAQ/Motors/#1-does-the-qt-motor-interface-implement-joint-limits","text":"The short answer is Yes . QTrobot motor interface sets all joint, torque limits of each motor independently at the startup. Each motor separately does the joint limit check on each position command that it receives. NOTICE : This does not mean that the QTrobot has self collision awareness when moving its joints!","title":"1. Does the QT Motor interface implement joint limits?"},{"location":"FAQ/Motors/#2-gesture-kinematic-diagramqtrobot-urdf","text":"","title":"2. Gesture kinematic diagram/QTrobot URDF"},{"location":"FAQ/Motors/#gesture-files","text":"the gesture are simply xml files containing robots joint waypoints and their timestamps. They are located under ~/robot/data/gestures/ folder in QTRP (head) board. here is an example: <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\" ?> <gesture> <name> QT/bye </name> <parts> <part> right_arm </part> <part> left_arm </part> </parts> <duration> 5.54 </duration> <waypoints count= \"97\" > <point time= \"1558971402671926152\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -62.90 </LeftShoulderRoll> <RightElbowRoll> -31.90 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -59.30 </RightShoulderRoll> </point> <point time= \"1558971402704626777\" > <LeftElbowRoll> -33.50 </LeftElbowRoll> <LeftShoulderPitch> 89.90 </LeftShoulderPitch> <LeftShoulderRoll> -64.80 </LeftShoulderRoll> <RightElbowRoll> -30.60 </RightElbowRoll> <RightShoulderPitch> -88.60 </RightShoulderPitch> <RightShoulderRoll> -60.60 </RightShoulderRoll> </point> ...","title":"Gesture files"},{"location":"FAQ/Motors/#qtrobot-joints-configuration","text":"in QTRP, under /opt/ros/kinetic/share/qt_motor/ there are motor controller config files: qtrobot-hardware.yaml : contains each motor configuration such as joints limit, PID, calibration offsets, etc. qtrobot-controller.yaml : contains QTrobot controller interface configuration such as joint_state , gesture , etc. These interface are implemented using ROS control system.","title":"QTrobot Joints configuration"},{"location":"FAQ/Motors/#qtrobot-urdf","text":"You can find the QTrobot URDF fils in the following links: - qtrobot.pdf - qtrobot_urdf.zip You can also load them in Gazebo to visualize joint configurations and test them: $ roslaunch urdf_tutorial display.launch model: = qtrobot.urdf you need to copy the qtrobot.urdf file to the proper folder in your PC (where you have installed Gazebo). Check the display.launch to find the path.","title":"QTRobot URDF"},{"location":"FAQ/Network/","text":"Network - FAQ Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them. 1. How to disable port forward on QTrobot? Okay. here is the instruction how to disable port forwarding on QTrobot: - access QTrobot RPI via ssh. - navigate to ~/robot/autostarts and open start_qt_routes.sh for editing (e.g. using vim/nano) - comment this line exec echo \"$QT_USERNAME\" | sudo -kS iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080 - reboot the robot. NOTICE : Doing this, you need to access QTrobot RPI web-config interface using http://qtrobot:8080 or http://192.168.100.1:8080 2.How do I setup Wifi on QTrobot? Let me clarify the QTrobot WIFIs and web interface. As it has been explained in different places (#7) QTrobot has an RPI and one NUC PC. RPI WIFI is configured as Hotsopt with unique ssid for each robot (e.g. QT100). the web interface is accessible via http://192.168.100.1:8080 NUC WIFI is free and you can use it to connect to the internet either via it's Ubuntu desktop, terminal programs via ssh or using NUC web interface via http://192.168.100.2:8080 The reason that web interface of RPI does not allow you to change its WIFI configuration is intuitive: you cannot access it remotely any more via its hotspot! QTrobot gave you three simple options to connect it to the internet: Using NUC WIFI Using an external WIFI dongle attached to RPI USB port (back of the robot) Using external Ethernet adapter attached to NUC USB-C port (see the attached picture) Notice: It does not matter which option you use to connect the robot to the internet. The internet will be always shared and routed between RPI and NUC! Notice: If you connect external WIFI dongle to the RPI 's USB port, you need to reboot the robot and then it will be shown up in the RPI web interface!","title":"Network"},{"location":"FAQ/Network/#network-faq","text":"Can't find what you're looking for? ASK YOUR QUESTION HERE This QTrobot FAQ is a summary of the issues from our Github repo . You can always open a new issue/feature request for any interfaces and we do our best to fix/implement them.","title":"Network - FAQ"},{"location":"FAQ/Network/#1-how-to-disable-port-forward-on-qtrobot","text":"Okay. here is the instruction how to disable port forwarding on QTrobot: - access QTrobot RPI via ssh. - navigate to ~/robot/autostarts and open start_qt_routes.sh for editing (e.g. using vim/nano) - comment this line exec echo \"$QT_USERNAME\" | sudo -kS iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080 - reboot the robot. NOTICE : Doing this, you need to access QTrobot RPI web-config interface using http://qtrobot:8080 or http://192.168.100.1:8080","title":"1. How to disable port forward on QTrobot?"},{"location":"FAQ/Network/#2how-do-i-setup-wifi-on-qtrobot","text":"Let me clarify the QTrobot WIFIs and web interface. As it has been explained in different places (#7) QTrobot has an RPI and one NUC PC. RPI WIFI is configured as Hotsopt with unique ssid for each robot (e.g. QT100). the web interface is accessible via http://192.168.100.1:8080 NUC WIFI is free and you can use it to connect to the internet either via it's Ubuntu desktop, terminal programs via ssh or using NUC web interface via http://192.168.100.2:8080 The reason that web interface of RPI does not allow you to change its WIFI configuration is intuitive: you cannot access it remotely any more via its hotspot! QTrobot gave you three simple options to connect it to the internet: Using NUC WIFI Using an external WIFI dongle attached to RPI USB port (back of the robot) Using external Ethernet adapter attached to NUC USB-C port (see the attached picture) Notice: It does not matter which option you use to connect the robot to the internet. The internet will be always shared and routed between RPI and NUC! Notice: If you connect external WIFI dongle to the RPI 's USB port, you need to reboot the robot and then it will be shown up in the RPI web interface!","title":"2.How do I setup Wifi on QTrobot?"},{"location":"demos/qt_emotion_game/","text":"Emotion Game Demo About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_emotion_app/qt_emotion_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_emotion_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_emotion_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_emotion_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { inState = true ; ROS_INFO ( \"Play.entry()\" ); ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language to English std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } ROS_INFO_STREAM ( \"emotion: \" << shown_lables ); if ( shown_lables == \"happy\" ) { talk ( \"emotion_recognition_001\" ); serviceHelper . showEmotionPlayGesture ( \"QT/happy\" , \"QT/happy\" , 0.5 , true ); } else if ( shown_lables == \"sad\" ) { talk ( \"emotion_recognition_002\" ); serviceHelper . showEmotionPlayGesture ( \"QT/sad\" , \"QT/sad\" , 1.0 , true ); talk ( \"emotion_recognition_003\" ); } else if ( shown_lables == \"angry\" ) { talk ( \"emotion_recognition_004\" ); serviceHelper . showEmotionPlayGesture ( \"QT/angry\" , \"QT/angry\" , 0.5 , true ); talk ( \"emotion_recognition_005\" ); serviceHelper . showEmotionPlayGesture ( \"QT/breathing_exercise\" , \"QT/breathing_exercise\" , 0.5 , true ); } else if ( shown_lables == \"disgusted\" ) { talk ( \"emotion_recognition_006\" ); serviceHelper . showEmotion ( \"QT/disgusted\" ); talk ( \"emotion_recognition_007\" ); } else if ( shown_lables == \"surprised\" ) { talk ( \"emotion_recognition_008\" ); serviceHelper . showEmotionPlayGesture ( \"QT/surprise\" , \"QT/surprise\" , 2.0 , true ); talk ( \"emotion_recognition_009\" ); } } virtual void exit () { ROS_INFO ( \"Play.exit()\" ); serviceHelper . homeAll (); ros :: Duration ( 1.0 ). sleep (); inState = false ; if ( ! with_audio_files ) { // set speech language to English if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set back speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); if ( ! inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool inState ; bool interrupted ; std :: string shown_lables ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTEmotionApp :: QTEmotionApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_emotion_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_emotion_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_emotion_app/suspend\", &QTEmotionApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTEmotionApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTEmotionApp :: timerCallback , this ); } QTEmotionApp ::~ QTEmotionApp () { timer . stop (); } void QTEmotionApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTEmotionApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTEmotionApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 3 ) lables = \"surprised\" ; else if ( objects -> data [ i ] == 4 ) lables = \"disgusted\" ; else if ( objects -> data [ i ] == 5 ) lables = \"angry\" ; else if ( objects -> data [ i ] == 6 ) lables = \"sad\" ; else if ( objects -> data [ i ] == 7 ) lables = \"happy\" ; } // call related callbacks if ( lables . size ()) { dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"**Emotion Game Demo**"},{"location":"demos/qt_emotion_game/#emotion-game-demo","text":"About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Emotion Game Demo"},{"location":"demos/qt_emotion_game/#video","text":"","title":"Video"},{"location":"demos/qt_emotion_game/#code","text":"Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_emotion_app/qt_emotion_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_emotion_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_emotion_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_emotion_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_emotion_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { inState = true ; ROS_INFO ( \"Play.entry()\" ); ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language to English std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } ROS_INFO_STREAM ( \"emotion: \" << shown_lables ); if ( shown_lables == \"happy\" ) { talk ( \"emotion_recognition_001\" ); serviceHelper . showEmotionPlayGesture ( \"QT/happy\" , \"QT/happy\" , 0.5 , true ); } else if ( shown_lables == \"sad\" ) { talk ( \"emotion_recognition_002\" ); serviceHelper . showEmotionPlayGesture ( \"QT/sad\" , \"QT/sad\" , 1.0 , true ); talk ( \"emotion_recognition_003\" ); } else if ( shown_lables == \"angry\" ) { talk ( \"emotion_recognition_004\" ); serviceHelper . showEmotionPlayGesture ( \"QT/angry\" , \"QT/angry\" , 0.5 , true ); talk ( \"emotion_recognition_005\" ); serviceHelper . showEmotionPlayGesture ( \"QT/breathing_exercise\" , \"QT/breathing_exercise\" , 0.5 , true ); } else if ( shown_lables == \"disgusted\" ) { talk ( \"emotion_recognition_006\" ); serviceHelper . showEmotion ( \"QT/disgusted\" ); talk ( \"emotion_recognition_007\" ); } else if ( shown_lables == \"surprised\" ) { talk ( \"emotion_recognition_008\" ); serviceHelper . showEmotionPlayGesture ( \"QT/surprise\" , \"QT/surprise\" , 2.0 , true ); talk ( \"emotion_recognition_009\" ); } } virtual void exit () { ROS_INFO ( \"Play.exit()\" ); serviceHelper . homeAll (); ros :: Duration ( 1.0 ). sleep (); inState = false ; if ( ! with_audio_files ) { // set speech language to English if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set back speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); if ( ! inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool inState ; bool interrupted ; std :: string shown_lables ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTEmotionApp :: QTEmotionApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_emotion_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_emotion_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_emotion_app/suspend\", &QTEmotionApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTEmotionApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTEmotionApp :: timerCallback , this ); } QTEmotionApp ::~ QTEmotionApp () { timer . stop (); } void QTEmotionApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTEmotionApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTEmotionApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 3 ) lables = \"surprised\" ; else if ( objects -> data [ i ] == 4 ) lables = \"disgusted\" ; else if ( objects -> data [ i ] == 5 ) lables = \"angry\" ; else if ( objects -> data [ i ] == 6 ) lables = \"sad\" ; else if ( objects -> data [ i ] == 7 ) lables = \"happy\" ; } // call related callbacks if ( lables . size ()) { dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"Code"},{"location":"demos/qt_face_recognition/","text":"Face recognition, Age & Gender About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #!/usr/bin/env python from __future__ import print_function # import sys import rospy import cv2 import threading from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError from qt_nuitrack_app.msg import Faces , FaceInfo class image_converter : faces = None faces_time = None def __init__ ( self ): self . lock = threading . Lock () self . bridge = CvBridge () self . image_pub = rospy . Publisher ( \"/face_recognition/out\" , Image , queue_size = 1 ) self . image_sub = rospy . Subscriber ( \"/camera/color/image_raw\" , Image , self . image_callback ) self . face_sub = rospy . Subscriber ( \"/qt_nuitrack_app/faces\" , Faces , self . face_callback ) def face_callback ( self , data ): # print(\"face_callback\") self . lock . acquire () self . faces = data . faces self . faces_time = rospy . Time . now () self . lock . release () def image_callback ( self , data ): try : cv_image = self . bridge . imgmsg_to_cv2 ( data , \"bgr8\" ) except CvBridgeError as e : print ( e ) ( rows , cols , channels ) = cv_image . shape self . lock . acquire () new_faces = self . faces new_faces_time = self . faces_time self . lock . release () if new_faces and ( rospy . Time . now () - new_faces_time ) < rospy . Duration ( 5.0 ): for face in new_faces : rect = face . rectangle cv2 . rectangle ( cv_image , ( int ( rect [ 0 ] * cols ), int ( rect [ 1 ] * rows )), ( int ( rect [ 0 ] * cols + rect [ 2 ] * cols ), int ( rect [ 1 ] * rows + rect [ 3 ] * rows )), ( 0 , 255 , 0 ), 2 ) x = int ( rect [ 0 ] * cols ) y = int ( rect [ 1 ] * rows ) w = int ( rect [ 2 ] * cols ) h = int ( rect [ 3 ] * rows ) #cv2.putText(cv_image, \"Gender:\", (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), lineType=cv2.LINE_AA) cv2 . putText ( cv_image , \"Gender: %s \" % face . gender , ( x , y + h + 20 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . putText ( cv_image , \"Age: %d \" % face . age_years , ( x , y + h + 40 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) # Neutral cv2 . putText ( cv_image , \"Neutral:\" , ( x , y + h + 60 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + int ( face . emotion_neutral * 100 ), y + h + 10 + 50 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + 100 , y + h + 10 + 50 ), ( 255 , 255 , 255 ), 1 ) # Angry cv2 . putText ( cv_image , \"Angry:\" , ( x , y + h + 80 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + int ( face . emotion_angry * 100 ), y + h + 10 + 70 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + 100 , y + h + 10 + 70 ), ( 255 , 255 , 255 ), 1 ) # Happy cv2 . putText ( cv_image , \"Happy:\" , ( x , y + h + 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + int ( face . emotion_happy * 100 ), y + h + 10 + 90 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + 100 , y + h + 10 + 90 ), ( 255 , 255 , 255 ), 1 ) cv2 . putText ( cv_image , \"Surprise:\" , ( x , y + h + 120 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + int ( face . emotion_surprise * 100 ), y + h + 10 + 110 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + 100 , y + h + 10 + 110 ), ( 255 , 255 , 255 ), 1 ) try : self . image_pub . publish ( self . bridge . cv2_to_imgmsg ( cv_image , \"bgr8\" )) except CvBridgeError as e : print ( e ) if __name__ == '__main__' : rospy . init_node ( 'qt_face_recognition' , anonymous = True ) ic = image_converter () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" )","title":"**Face recognition, Age & Gender**"},{"location":"demos/qt_face_recognition/#face-recognition-age-gender","text":"About This is face recognition, age and gender demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Face recognition, Age &amp; Gender"},{"location":"demos/qt_face_recognition/#video","text":"","title":"Video"},{"location":"demos/qt_face_recognition/#code","text":"Check code here #!/usr/bin/env python from __future__ import print_function # import sys import rospy import cv2 import threading from sensor_msgs.msg import Image from cv_bridge import CvBridge , CvBridgeError from qt_nuitrack_app.msg import Faces , FaceInfo class image_converter : faces = None faces_time = None def __init__ ( self ): self . lock = threading . Lock () self . bridge = CvBridge () self . image_pub = rospy . Publisher ( \"/face_recognition/out\" , Image , queue_size = 1 ) self . image_sub = rospy . Subscriber ( \"/camera/color/image_raw\" , Image , self . image_callback ) self . face_sub = rospy . Subscriber ( \"/qt_nuitrack_app/faces\" , Faces , self . face_callback ) def face_callback ( self , data ): # print(\"face_callback\") self . lock . acquire () self . faces = data . faces self . faces_time = rospy . Time . now () self . lock . release () def image_callback ( self , data ): try : cv_image = self . bridge . imgmsg_to_cv2 ( data , \"bgr8\" ) except CvBridgeError as e : print ( e ) ( rows , cols , channels ) = cv_image . shape self . lock . acquire () new_faces = self . faces new_faces_time = self . faces_time self . lock . release () if new_faces and ( rospy . Time . now () - new_faces_time ) < rospy . Duration ( 5.0 ): for face in new_faces : rect = face . rectangle cv2 . rectangle ( cv_image , ( int ( rect [ 0 ] * cols ), int ( rect [ 1 ] * rows )), ( int ( rect [ 0 ] * cols + rect [ 2 ] * cols ), int ( rect [ 1 ] * rows + rect [ 3 ] * rows )), ( 0 , 255 , 0 ), 2 ) x = int ( rect [ 0 ] * cols ) y = int ( rect [ 1 ] * rows ) w = int ( rect [ 2 ] * cols ) h = int ( rect [ 3 ] * rows ) #cv2.putText(cv_image, \"Gender:\", (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), lineType=cv2.LINE_AA) cv2 . putText ( cv_image , \"Gender: %s \" % face . gender , ( x , y + h + 20 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . putText ( cv_image , \"Age: %d \" % face . age_years , ( x , y + h + 40 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) # Neutral cv2 . putText ( cv_image , \"Neutral:\" , ( x , y + h + 60 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + int ( face . emotion_neutral * 100 ), y + h + 10 + 50 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 50 ), ( x + 80 + 100 , y + h + 10 + 50 ), ( 255 , 255 , 255 ), 1 ) # Angry cv2 . putText ( cv_image , \"Angry:\" , ( x , y + h + 80 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + int ( face . emotion_angry * 100 ), y + h + 10 + 70 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 70 ), ( x + 80 + 100 , y + h + 10 + 70 ), ( 255 , 255 , 255 ), 1 ) # Happy cv2 . putText ( cv_image , \"Happy:\" , ( x , y + h + 100 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + int ( face . emotion_happy * 100 ), y + h + 10 + 90 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 90 ), ( x + 80 + 100 , y + h + 10 + 90 ), ( 255 , 255 , 255 ), 1 ) cv2 . putText ( cv_image , \"Surprise:\" , ( x , y + h + 120 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.5 , ( 0 , 255 , 0 ), 1 , lineType = cv2 . LINE_AA ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + int ( face . emotion_surprise * 100 ), y + h + 10 + 110 ), ( 0 , 255 , 0 ), cv2 . FILLED ) cv2 . rectangle ( cv_image , ( x + 80 , y + h + 110 ), ( x + 80 + 100 , y + h + 10 + 110 ), ( 255 , 255 , 255 ), 1 ) try : self . image_pub . publish ( self . bridge . cv2_to_imgmsg ( cv_image , \"bgr8\" )) except CvBridgeError as e : print ( e ) if __name__ == '__main__' : rospy . init_node ( 'qt_face_recognition' , anonymous = True ) ic = image_converter () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" )","title":"Code"},{"location":"demos/qt_gesture_recognition/","text":"Gesture Game Demo About This is gesture game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #include \"qt_gesturegame_app/qt_gesturegame_app.h\" #include \"qt_idle_app/suspend.h\" #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'U' , 'R' , 'L' }; static int current_user_id = - 1 ; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_gesturegame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_gesturegame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = - 1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 3 )]; } ROS_INFO_STREAM ( \"Show \" << robotMemory ); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'U' ) serviceHelper . playGesture ( \"QT/up_right\" , 1.2 ); else if ( robotMemory [ i ] == 'R' ) serviceHelper . playGesture ( \"QT/swipe_right\" , 1.2 ); else if ( robotMemory [ i ] == 'L' ) serviceHelper . playGesture ( \"QT/swipe_left\" , 1.2 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } //ros::Duration(2.0).sleep(); // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_gesture . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_gesture . size ()) { ROS_INFO_STREAM ( \"shown \" << shown_gesture ); read = true ; if ( shown_gesture == \"SWIPE UP\" ) { playerMemory += \"U\" ; talk ( \"memory_game_015\" ); } else if ( shown_gesture == \"SWIPE RIGHT\" ) { playerMemory += \"R\" ; talk ( \"memory_game_007\" ); } else if ( shown_gesture == \"SWIPE LEFT\" ) { playerMemory += \"L\" ; talk ( \"memory_game_008\" ); } else { read = false ; } } mutexLables . unlock (); } //ros::Duration(0.5).sleep(); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != - 1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onGestureDetected ( const std :: string & gesture , int id ) { if (( gesture == \"SWIPE DOWN\" ) || ( gesture == \"WAVING\" )) return ; if ( id != current_user_id ) return ; mutexLables . lock (); shown_gesture = gesture ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_gesture ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onGestureDetected ( const std :: string & gesture , int id ) { // TODO check if both Q & T are shown if ( inState && gesture == \"SWIPE UP\" ) { /* static int gesture_count = 0; static ros::Time prev_gesture_time = ros::Time::now(); if((ros::Time::now()-prev_gesture_time).toSec() > 3.0) { gesture_count = 0; prev_gesture_time = ros::Time::now(); return; } if(gesture_count >= 1) { gesture_count = 0; current_user_id = id; rfsm.sendEvent(\"e_start_game\"); return; } if(gesture_count == 0) { gesture_count++; prev_gesture_time = ros::Time::now(); return; } gesture_count++; */ current_user_id = id ; rfsm . sendEvent ( \"e_start_game\" ); } } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper & serviceHelper ; }; QTGestureGameApp :: QTGestureGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_gesturegame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_gesturegame_app/suspend\", &QTGestureGameApp::suspendCB, this); subGestures = nh . subscribe ( \"/qt_nuitrack_app/gestures\" , 10 , & QTGestureGameApp :: gestureSubCB , this ); // set speech language to English if ( ! serviceHelper . speechConfig ( \"en-US\" )) { ROS_ERROR_STREAM ( \"Cannot set speech language to 'en-US'\" ); ros :: shutdown (); } // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } fsmTimer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTGestureGameApp :: fsmTimerCallback , this ); } QTGestureGameApp ::~ QTGestureGameApp () { fsmTimer . stop (); } void QTGestureGameApp :: fsmTimerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } /* void QTGestureGameApp::onNewGestures(const tdv::nuitrack::GestureData::Ptr gesture_data) { //ROS_INFO_STREAM(\"onNewGestures...\"); const std::vector<tdv::nuitrack::Gesture> gestures = gesture_data->getGestures(); for( const tdv::nuitrack::Gesture& gesture : gestures ){ ROS_INFO_STREAM(gesture.userId << \": \" << type2string(gesture.type)); if(gesture.type == GestureType::GESTURE_SWIPE_UP || gesture.type == GestureType::GESTURE_SWIPE_RIGHT || gesture.type == GestureType::GESTURE_SWIPE_LEFT) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onGestureDetected(type2string(gesture.type)); dynamic_cast<PlayStateCB*>(playStateCB)->onGestureDetected(type2string(gesture.type)); return; } } } */ bool QTGestureGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTGestureGameApp :: gestureSubCB ( const qt_nuitrack_app :: Gestures :: ConstPtr & gestures ) { for ( int i = 0 ; i < gestures -> gestures . size (); i ++ ) { ROS_INFO_STREAM ( \"gesture: \" << gestures -> gestures [ i ]. name << \", id: \" << gestures -> gestures [ i ]. id ); dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); } } /* void QTGestureGameApp::objectsSubCB(const std_msgs::Float32MultiArray::ConstPtr& objects) { if(objects->data.size() <= 0 ) return; std::string lables; for(size_t i=0; i<objects->data.size(); i+=12) { if(objects->data[i] == 1) lables.push_back('Q'); else if(objects->data[i] == 2) lables.push_back('T'); } // call related callbacks if(lables.size()) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onObjectsDetected(lables); dynamic_cast<PlayStateCB*>(playStateCB)->onObjectsDetected(lables); } } */","title":"**Gesture Game Demo**"},{"location":"demos/qt_gesture_recognition/#gesture-game-demo","text":"About This is gesture game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Gesture Game Demo"},{"location":"demos/qt_gesture_recognition/#video","text":"","title":"Video"},{"location":"demos/qt_gesture_recognition/#code","text":"Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #include \"qt_gesturegame_app/qt_gesturegame_app.h\" #include \"qt_idle_app/suspend.h\" #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'U' , 'R' , 'L' }; static int current_user_id = - 1 ; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ){ srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_gesturegame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_gesturegame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_gesturegame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = - 1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 3 )]; } ROS_INFO_STREAM ( \"Show \" << robotMemory ); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'U' ) serviceHelper . playGesture ( \"QT/up_right\" , 1.2 ); else if ( robotMemory [ i ] == 'R' ) serviceHelper . playGesture ( \"QT/swipe_right\" , 1.2 ); else if ( robotMemory [ i ] == 'L' ) serviceHelper . playGesture ( \"QT/swipe_left\" , 1.2 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } //ros::Duration(2.0).sleep(); // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_gesture . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_gesture . size ()) { ROS_INFO_STREAM ( \"shown \" << shown_gesture ); read = true ; if ( shown_gesture == \"SWIPE UP\" ) { playerMemory += \"U\" ; talk ( \"memory_game_015\" ); } else if ( shown_gesture == \"SWIPE RIGHT\" ) { playerMemory += \"R\" ; talk ( \"memory_game_007\" ); } else if ( shown_gesture == \"SWIPE LEFT\" ) { playerMemory += \"L\" ; talk ( \"memory_game_008\" ); } else { read = false ; } } mutexLables . unlock (); } //ros::Duration(0.5).sleep(); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != - 1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onGestureDetected ( const std :: string & gesture , int id ) { if (( gesture == \"SWIPE DOWN\" ) || ( gesture == \"WAVING\" )) return ; if ( id != current_user_id ) return ; mutexLables . lock (); shown_gesture = gesture ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_gesture ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onGestureDetected ( const std :: string & gesture , int id ) { // TODO check if both Q & T are shown if ( inState && gesture == \"SWIPE UP\" ) { /* static int gesture_count = 0; static ros::Time prev_gesture_time = ros::Time::now(); if((ros::Time::now()-prev_gesture_time).toSec() > 3.0) { gesture_count = 0; prev_gesture_time = ros::Time::now(); return; } if(gesture_count >= 1) { gesture_count = 0; current_user_id = id; rfsm.sendEvent(\"e_start_game\"); return; } if(gesture_count == 0) { gesture_count++; prev_gesture_time = ros::Time::now(); return; } gesture_count++; */ current_user_id = id ; rfsm . sendEvent ( \"e_start_game\" ); } } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper & serviceHelper ; }; QTGestureGameApp :: QTGestureGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_gesturegame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_gesturegame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_gesturegame_app/suspend\", &QTGestureGameApp::suspendCB, this); subGestures = nh . subscribe ( \"/qt_nuitrack_app/gestures\" , 10 , & QTGestureGameApp :: gestureSubCB , this ); // set speech language to English if ( ! serviceHelper . speechConfig ( \"en-US\" )) { ROS_ERROR_STREAM ( \"Cannot set speech language to 'en-US'\" ); ros :: shutdown (); } // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } fsmTimer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTGestureGameApp :: fsmTimerCallback , this ); } QTGestureGameApp ::~ QTGestureGameApp () { fsmTimer . stop (); } void QTGestureGameApp :: fsmTimerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } /* void QTGestureGameApp::onNewGestures(const tdv::nuitrack::GestureData::Ptr gesture_data) { //ROS_INFO_STREAM(\"onNewGestures...\"); const std::vector<tdv::nuitrack::Gesture> gestures = gesture_data->getGestures(); for( const tdv::nuitrack::Gesture& gesture : gestures ){ ROS_INFO_STREAM(gesture.userId << \": \" << type2string(gesture.type)); if(gesture.type == GestureType::GESTURE_SWIPE_UP || gesture.type == GestureType::GESTURE_SWIPE_RIGHT || gesture.type == GestureType::GESTURE_SWIPE_LEFT) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onGestureDetected(type2string(gesture.type)); dynamic_cast<PlayStateCB*>(playStateCB)->onGestureDetected(type2string(gesture.type)); return; } } } */ bool QTGestureGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTGestureGameApp :: gestureSubCB ( const qt_nuitrack_app :: Gestures :: ConstPtr & gestures ) { for ( int i = 0 ; i < gestures -> gestures . size (); i ++ ) { ROS_INFO_STREAM ( \"gesture: \" << gestures -> gestures [ i ]. name << \", id: \" << gestures -> gestures [ i ]. id ); dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onGestureDetected ( gestures -> gestures [ i ]. name , gestures -> gestures [ i ]. id ); } } /* void QTGestureGameApp::objectsSubCB(const std_msgs::Float32MultiArray::ConstPtr& objects) { if(objects->data.size() <= 0 ) return; std::string lables; for(size_t i=0; i<objects->data.size(); i+=12) { if(objects->data[i] == 1) lables.push_back('Q'); else if(objects->data[i] == 2) lables.push_back('T'); } // call related callbacks if(lables.size()) { dynamic_cast<WaitingStateCB*>(waitingStateCB)->onObjectsDetected(lables); dynamic_cast<PlayStateCB*>(playStateCB)->onObjectsDetected(lables); } } */","title":"Code"},{"location":"demos/qt_memory_game/","text":"Memory Game Demo About This is memory game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_memgame_app/qt_memgame_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'Q' , 'T' }; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_memgame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_memgame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_memgame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = - 1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 2 )]; } //ROS_INFO_STREAM(\"Show \"<<robotMemory); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'Q' ) serviceHelper . playGesture ( \"QT/show_right\" , 1.0 ); else if ( robotMemory [ i ] == 'T' ) serviceHelper . playGesture ( \"QT/show_left\" , 1.0 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_lables . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_lables . size ()) { //ROS_INFO_STREAM(\"shown \"<< shown_lables[0]); talk (( shown_lables [ 0 ] == 'Q' ) ? \"memory_game_007\" : \"memory_game_008\" ); playerMemory += shown_lables [ 0 ]; read = true ; } mutexLables . unlock (); } ros :: Duration ( 0.5 ). sleep (); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != - 1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_lables ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onObjectsDetected ( const std :: string & lables ) { // TODO check if both Q & T are shown if ( inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTMemGameApp :: QTMemGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_memgame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_memgame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_memgame_app/suspend\", &QTMemGameApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTMemGameApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTMemGameApp :: timerCallback , this ); } QTMemGameApp ::~ QTMemGameApp () { timer . stop (); } void QTMemGameApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTMemGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTMemGameApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 1 ) lables . push_back ( 'Q' ); else if ( objects -> data [ i ] == 2 ) lables . push_back ( 'T' ); } // call related callbacks if ( lables . size ()) { dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onObjectsDetected ( lables ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"**Memory Game Demo**"},{"location":"demos/qt_memory_game/#memory-game-demo","text":"About This is memory game demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Memory Game Demo"},{"location":"demos/qt_memory_game/#video","text":"","title":"Video"},{"location":"demos/qt_memory_game/#code","text":"Check code here #include <ctime> #include <cstdlib> #include <sstream> #include <std_msgs/Float64MultiArray.h> #include \"qt_memgame_app/qt_memgame_app.h\" #include \"qt_idle_app/suspend.h\" #include <boost/thread/thread.hpp> #include <yaml-cpp/yaml.h> #define ROBOT_MEMORY_INCREMENT 1 static char GestureTable [] = { 'Q' , 'T' }; /** * @brief The PlayStateCB class */ class PlayStateCB : public rfsm :: StateCallback { public : PlayStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), serviceHelper ( srvHelper ), with_audio_files ( false ) { srand ( time ( 0 )); if ( ! nh . getParam ( \"/qt_memgame_app/with_audio_files\" , with_audio_files )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/with_audio_files.\" ); } if ( with_audio_files ) { if ( ! nh . getParam ( \"/qt_memgame_app/audio_path\" , audio_path )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/audio_path.\" ); } ROS_INFO_STREAM ( \"Using audios from '\" << audio_path << \"'\" ); } else { std :: string speech_message_file ; if ( ! nh . getParam ( \"/qt_memgame_app/speech_message_file\" , speech_message_file )) { ROS_WARN_STREAM ( \"Cannot find param /qt_memgame_app/speech_message_file.\" ); } ROS_INFO_STREAM ( \"Loading speech messages from '\" << speech_message_file << \"'\" ); try { speechMessages = YAML :: LoadFile ( speech_message_file ); } catch ( YAML :: BadFile ) { ROS_ERROR_STREAM ( \"Cannot load speech file '\" << speech_message_file << \"'\" ); ros :: shutdown (); return ; } ROS_INFO_STREAM ( \"Using speech in \" << speechMessages [ \"language\" ]. as < std :: string > ()); } } virtual void entry () { ROS_INFO ( \"Play.entry()\" ); if ( ! with_audio_files ) { // set speech language std :: string lang = speechMessages [ \"language\" ]. as < std :: string > (); if ( ! serviceHelper . speechConfig ( lang )) { ROS_WARN_STREAM ( \"Cannot set speech language to \" << lang ); } } prevLevel = - 1 ; if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_001\" , \"QT/challenge\" , 2.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_001\" ]. as < std :: string > (), \"QT/challenge\" , 2.0 , true ); ros :: Duration ( 1.0 ). sleep (); // doo interrupted = false ; robotMemory . clear (); playerMemory . clear (); start_time = ros :: Time :: now (); while ( ! ros :: isShuttingDown () && ! interrupted ) { // motivate player if ( robotMemory . size () == 4 ) talk ( \"memory_game_002\" ); // update robot memory for ( size_t rep = 0 ; rep < ROBOT_MEMORY_INCREMENT ; rep ++ ) { robotMemory += GestureTable [( rand () % 2 )]; } //ROS_INFO_STREAM(\"Show \"<<robotMemory); if ( robotMemory . size () == ROBOT_MEMORY_INCREMENT ) talk ( \"memory_game_003\" ); else talk ( \"memory_game_004\" ); for ( size_t i = 0 ; i < robotMemory . size (); i ++ ) { if ( robotMemory [ i ] == 'Q' ) serviceHelper . playGesture ( \"QT/show_right\" , 1.0 ); else if ( robotMemory [ i ] == 'T' ) serviceHelper . playGesture ( \"QT/show_left\" , 1.0 ); else ROS_WARN_STREAM ( \"Unknown gesture \" << robotMemory [ i ]); } // clear player memory playerMemory . clear (); ROS_INFO_STREAM ( \"Your turn...\" ); talk ( \"memory_game_005\" ); // loop until player take enough moves size_t robotMemorySize = robotMemory . size (); while (( playerMemory . size () <= robotMemorySize ) && ! ros :: isShuttingDown () && ! interrupted ) { mutexLables . lock (); shown_lables . clear (); mutexLables . unlock (); // wait until user show a lable (read from user shwon image) ROS_INFO ( \"Reading...\" ); ros :: Time start_read_time = ros :: Time :: now (); bool read = false ; while ( ! read && ! ros :: isShuttingDown () && ! interrupted ) { if (( ros :: Time :: now () - start_read_time ). toSec () > 10.0 ) { ROS_INFO ( \"timeout! Okay! it seems you do not want to play anymore!\" ); if ( with_audio_files ) serviceHelper . talkAudioPlayGesture ( \"memory_game_006\" , \"QT/angry\" , 1.0 , true , audio_path ); else serviceHelper . talkTextPlayGesture ( speechMessages [ \"memory_game_006\" ]. as < std :: string > (), \"QT/angry\" , 1.0 , true ); interrupted = true ; break ; } mutexLables . lock (); if ( shown_lables . size ()) { //ROS_INFO_STREAM(\"shown \"<< shown_lables[0]); talk (( shown_lables [ 0 ] == 'Q' ) ? \"memory_game_007\" : \"memory_game_008\" ); playerMemory += shown_lables [ 0 ]; read = true ; } mutexLables . unlock (); } ros :: Duration ( 0.5 ). sleep (); if ( playerMemory . size () <= robotMemorySize ) { // ignore the last one if ( playerMemory . size () && ( playerMemory != robotMemory . substr ( 0 , playerMemory . size ()))) break ; } else // add the last gesture to robot memory robotMemory . push_back ( playerMemory . back ()); } // end of moves loop ROS_INFO_STREAM ( \"Player memory: \" << playerMemory ); ROS_INFO_STREAM ( \"Robot memory : \" << robotMemory ); if (( playerMemory != robotMemory ) && ! interrupted ) { //ros::Duration(1.0).sleep(); talk ( \"memory_game_009\" ); serviceHelper . showEmotion ( \"QT/shy\" ); break ; } } // end of main loop } virtual void exit () { int level = robotMemory . size () / 2 ; ros :: Duration game_duration = ros :: Time :: now () - start_time ; if ( robotMemory . size () < 5 ) { ROS_INFO ( \"It was a short game!\" ); talk ( \"memory_game_010\" ); } else if ( robotMemory . size () >= 5 ) { ROS_INFO ( \"Well Done! It was a good game!\" ); talk ( \"memory_game_011\" ); } else if ( robotMemory . size () >= 8 ) { talk ( \"memory_game_012\" ); } if ( prevLevel != - 1 ) { if ( level < prevLevel ) talk ( \"memory_game_013\" ); else talk ( \"memory_game_014\" ); } serviceHelper . showEmotionPlayGesture ( \"QT/kiss\" , \"QT/kiss\" , 2.0 , true ); if ( ! with_audio_files ) { // set speech language to default if ( ! serviceHelper . speechConfig ( \"default\" )) { ROS_WARN_STREAM ( \"Cannot set speech language to 'default'\" ); } } } void onObjectsDetected ( const std :: string & lables ) { mutexLables . lock (); shown_lables = lables ; mutexLables . unlock (); } void interrupt () { interrupted = true ; } bool talk ( std :: string message ){ bool ret = true ; if ( with_audio_files ) { ret = serviceHelper . talkAudio ( message , audio_path ); if ( ! ret ) ROS_WARN_STREAM ( \"Could not play audio \" << message ); } else { try { ROS_INFO_STREAM ( \"talking '\" << speechMessages [ message ]. as < std :: string > () << \"'\" ); ret = serviceHelper . talkText ( speechMessages [ message ]. as < std :: string > ()); } catch (...) { ret = false ; } if ( ! ret ) ROS_WARN_STREAM ( \"Could not talk message \" << message ); } return ret ; } private : bool interrupted ; int prevLevel ; ros :: Time start_time ; std :: string shown_lables ; std :: string robotMemory ; std :: string playerMemory ; boost :: mutex mutexLables ; rfsm :: StateMachine & rfsm ; std :: string audio_path ; bool with_audio_files ; YAML :: Node speechMessages ; QTrobotServiceHelper & serviceHelper ; }; /** * @brief The WaitingStateCB class */ class WaitingStateCB : public rfsm :: StateCallback { public : WaitingStateCB ( ros :: NodeHandle & nh , rfsm :: StateMachine & fsm , QTrobotServiceHelper & srvHelper ) : rfsm ( fsm ), inState ( false ), serviceHelper ( srvHelper ){ idleAppClient = nh . serviceClient < qt_idle_app :: suspend > ( \"/qt_idle_app/suspend\" ); if ( ! idleAppClient . exists ()) { ROS_WARN ( \"Could connect to /qt_idle_app/suspend\" ); } } virtual void entry () { ROS_INFO ( \"Waiting.entry()\" ); // resume qt_idle_app suspendIdleApp ( false ); inState = true ; } virtual void exit () { ROS_INFO ( \"Waiting.exit()\" ); // suspend qt_idle_app suspendIdleApp ( true ); // home all robot serviceHelper . homeAll (); inState = false ; } void onObjectsDetected ( const std :: string & lables ) { // TODO check if both Q & T are shown if ( inState && lables . size ()) rfsm . sendEvent ( \"e_start_game\" ); } private : bool suspendIdleApp ( bool flag ) { qt_idle_app :: suspend cmdSuspend ; cmdSuspend . request . flag = flag ; if ( ! idleAppClient . call ( cmdSuspend )) { ROS_WARN ( \"Could not call service qt_idle_app::suspend\" ); return false ; } return cmdSuspend . response . status ; } private : bool inState ; rfsm :: StateMachine & rfsm ; ros :: ServiceClient idleAppClient ; QTrobotServiceHelper serviceHelper ; }; QTMemGameApp :: QTMemGameApp ( ros :: NodeHandle & nh ) : serviceHelper ( nh ) { std :: string fsm_file ; if ( ! nh . getParam ( \"/qt_memgame_app/fsm\" , fsm_file )) { ROS_ERROR_STREAM ( \"Cannot find param /qt_memgame_app/fsm\" ); ros :: shutdown (); } if ( ! rfsm . load ( fsm_file )) { ROS_ERROR_STREAM ( \"Cannot load \" << fsm_file ); ros :: shutdown (); } //serviceSuspend = nh.advertiseService(\"qt_memgame_app/suspend\", &QTMemGameApp::suspendCB, this); subObjects = nh . subscribe ( \"/find_object/objects\" , 10 , & QTMemGameApp :: objectsSubCB , this ); // Waiting callback waitingStateCB = new WaitingStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Waiting\" , * waitingStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } // Play callback playStateCB = new PlayStateCB ( nh , rfsm , serviceHelper ); if ( ! rfsm . setStateCallback ( \"Game.Play\" , * playStateCB )) { ROS_ERROR_STREAM ( \"Cannot set callback for Waiting from \" << fsm_file ); ros :: shutdown (); } timer = nh . createTimer ( ros :: Duration ( 0.3 ), & QTMemGameApp :: timerCallback , this ); } QTMemGameApp ::~ QTMemGameApp () { timer . stop (); } void QTMemGameApp :: timerCallback ( const ros :: TimerEvent & event ) { mutexRFSM . lock (); rfsm . run (); mutexRFSM . unlock (); } bool QTMemGameApp :: suspend ( bool flag ) { if ( flag ) { //dynamic_cast<LookAroundStateCB*>(lookAroundStateCB)->interrupt(); dynamic_cast < PlayStateCB *> ( playStateCB ) -> interrupt (); rfsm . sendEvent ( \"e_suspend\" ); mutexRFSM . lock (); rfsm . run (); // to ensure being in suspended state mutexRFSM . unlock (); } else { rfsm . sendEvent ( \"e_resume\" ); } return true ; } void QTMemGameApp :: objectsSubCB ( const std_msgs :: Float32MultiArray :: ConstPtr & objects ) { if ( objects -> data . size () <= 0 ) return ; std :: string lables ; for ( size_t i = 0 ; i < objects -> data . size (); i += 12 ) { if ( objects -> data [ i ] == 1 ) lables . push_back ( 'Q' ); else if ( objects -> data [ i ] == 2 ) lables . push_back ( 'T' ); } // call related callbacks if ( lables . size ()) { dynamic_cast < WaitingStateCB *> ( waitingStateCB ) -> onObjectsDetected ( lables ); dynamic_cast < PlayStateCB *> ( playStateCB ) -> onObjectsDetected ( lables ); } }","title":"Code"},{"location":"demos/qt_microphone_detection/","text":"Microphone Voice Activity & Direction Detection About This is microphone voice activity and direction detection demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Setup NOTICE You must run this demo on QTRP (head) How to set it up? This demo needs some prerequisites: sudo apt-get update sudo pip install pyusb click OR sudo apt-get update sudo `which pip` install pyusb click PyUSB needs root privileges, if you run the script without root you will see something like this: usb.core.USBError: [Errno 13] Access denied (insufficient permissions) To fix this error we need to set up a udev rule file for the microphone to be able to access it with normal user. * Create a udev rules file: ACTION==\"add\", SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2886\", ATTRS{idProduct}==\"0018\", MODE=\"660\", GROUP=\"plugdev\" Create this file in folder /etc/udev/rules.d/ . For example usual structure of the file name can be Number-Name.rules Add the user to the plugdev group: adduser username plugdev Reload udev system to see your changes: sudo udevadm control --reload sudo udevadm trigger Reboot QTrobot: sudo reboot To run the demo go to qt_microphone_detection folder and run voice_direction python script: cd qt_microphone_detection/ python voice_direction.py Now you can speak or sing around the QTrobot and he will follow your voice. Code Check code here from tuning import Tuning import usb.core import usb.util import time import sys import rospy from std_msgs.msg import Float64MultiArray # microphone orientation MICROPHONE_ANGLE_OFFSET = 90 if __name__ == '__main__' : rospy . init_node ( 'voice_direction' , anonymous = True ) dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if not dev : rospy . logerr ( \"Cannot establish connection!\" ) sys . exit () head_pub = rospy . Publisher ( '/qt_robot/head_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher connections wtime_begin = rospy . get_time () while ( head_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) # centering the head href = Float64MultiArray ( data = [ 0 , 0 ]) head_pub . publish ( href ) rospy . sleep ( 1 ) microphone = Tuning ( dev ) while not rospy . is_shutdown (): if not microphone . is_voice (): continue mic = abs ( microphone . direction - 180 ) angle = mic - MICROPHONE_ANGLE_OFFSET rospy . loginfo ( \"mic: %d , head: %d \" % ( mic , angle )) href . data = [ angle , 0 ] head_pub . publish ( href ) rospy . loginfo ( \"shutdowned!\" )","title":"**Microphone Voice Activity & Direction Detection**"},{"location":"demos/qt_microphone_detection/#microphone-voice-activity-direction-detection","text":"About This is microphone voice activity and direction detection demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Microphone Voice Activity &amp; Direction Detection"},{"location":"demos/qt_microphone_detection/#video","text":"","title":"Video"},{"location":"demos/qt_microphone_detection/#setup","text":"NOTICE You must run this demo on QTRP (head) How to set it up? This demo needs some prerequisites: sudo apt-get update sudo pip install pyusb click OR sudo apt-get update sudo `which pip` install pyusb click PyUSB needs root privileges, if you run the script without root you will see something like this: usb.core.USBError: [Errno 13] Access denied (insufficient permissions) To fix this error we need to set up a udev rule file for the microphone to be able to access it with normal user. * Create a udev rules file: ACTION==\"add\", SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"2886\", ATTRS{idProduct}==\"0018\", MODE=\"660\", GROUP=\"plugdev\" Create this file in folder /etc/udev/rules.d/ . For example usual structure of the file name can be Number-Name.rules Add the user to the plugdev group: adduser username plugdev Reload udev system to see your changes: sudo udevadm control --reload sudo udevadm trigger Reboot QTrobot: sudo reboot To run the demo go to qt_microphone_detection folder and run voice_direction python script: cd qt_microphone_detection/ python voice_direction.py Now you can speak or sing around the QTrobot and he will follow your voice.","title":"Setup"},{"location":"demos/qt_microphone_detection/#code","text":"Check code here from tuning import Tuning import usb.core import usb.util import time import sys import rospy from std_msgs.msg import Float64MultiArray # microphone orientation MICROPHONE_ANGLE_OFFSET = 90 if __name__ == '__main__' : rospy . init_node ( 'voice_direction' , anonymous = True ) dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if not dev : rospy . logerr ( \"Cannot establish connection!\" ) sys . exit () head_pub = rospy . Publisher ( '/qt_robot/head_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher connections wtime_begin = rospy . get_time () while ( head_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) # centering the head href = Float64MultiArray ( data = [ 0 , 0 ]) head_pub . publish ( href ) rospy . sleep ( 1 ) microphone = Tuning ( dev ) while not rospy . is_shutdown (): if not microphone . is_voice (): continue mic = abs ( microphone . direction - 180 ) angle = mic - MICROPHONE_ANGLE_OFFSET rospy . loginfo ( \"mic: %d , head: %d \" % ( mic , angle )) href . data = [ angle , 0 ] head_pub . publish ( href ) rospy . loginfo ( \"shutdowned!\" )","title":"Code"},{"location":"demos/qt_microphone_interaction/","text":"Microphone Voice Interaction About This is microphone voice interaction demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Setup NOTICE You must read and fully understand Snips voice system before starting this Please read the documentation of snips to understand it https://console.snips.ai/login How to set it up? This demo is already installed on your QTrobot. you can find it under ~/robot/code/qt_app Snips already installed on QTRP (head) how to run the snip servers: sudo systemctl start snips-asr.service snips-audio-server.service snips-dialogue.service snips-hotword.service snips-nlu.service how to run qt_voice_app demo rosrun qt_voice_app qt_voice_app.py Voice commands Hey QT, show me your happy emotion Hey QT, play happy gesture Snips configuration file /etc/snips.toml Code Check code here #!/usr/bin/env python2 # -*- coding: utf-8 -*- import time import threading from hermes_python.hermes import Hermes import rospy from std_msgs.msg import String from qt_gesture_controller.srv import * from qt_motors_controller.srv import * from qt_robot_interface.srv import * talkText = rospy . ServiceProxy ( '/qt_robot/behavior/talkText' , behavior_talk_text ) gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) gestureList = rospy . ServiceProxy ( '/qt_robot/gesture/list' , gesture_list ) gestureSave = rospy . ServiceProxy ( '/qt_robot/gesture/save' , gesture_save ) setControlMode = rospy . ServiceProxy ( '/qt_robot/motors/setControlMode' , set_control_mode ) gestureRecord = rospy . ServiceProxy ( '/qt_robot/gesture/record' , gesture_record ) #gesturePlay_pub = rospy.Publisher('/qt_robot/gesture/play', String, queue_size=10) emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) action_thread = None def emotion_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in emotion_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"ShowEmotion\" : emotion_name = slot [ 0 ] . slot_value . value . value try : print ( \"Showing emotion %s \" % \"QT/ %s \" % emotion_name ) talkText ( \"This is my %s face.\" % emotion_name . replace ( \"_\" , \" \" )) emotionShow_pub . publish ( \"QT/ %s \" % emotion_name ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for showing emotion %s \" % emotion_name ) else : talkText ( \"I did not understand which emotion.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def play_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) # action code goes here... print ( \"in play_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"GastureName\" : gesture_name = slot [ 0 ] . slot_value . value . value try : res = gestureList () if gesture_name not in res . gestures : talkText ( \"I do not know this gesture. But you can always record your gesture. Just ask me, hey Q.T. . record, new gesture!\" ) else : if gesture_name != \"my\" : gesture_name = \"QT/\" + gesture_name print ( \"playing gesture ' %s '.\" % gesture_name ) res = gesturePlay ( gesture_name , 1.0 ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for playing gesture %s \" % gesture_name ) else : talkText ( \"I did not understand which gesture.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def record_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in record_intent_callback\" ) talkText ( \"You can start recording your gesture.\" ) res = gestureRecord ([ \"right_arm\" , \"left_arm\" ], True , 0 , 0 ) print ( \"done!\" ) def stop_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in stop_intent_callback\" ) res = gestureSave ( \"my\" , \"\" ) if not res . status : talkText ( \"I have a problem with recording the gesture\" ) setControlMode ([ \"right_arm\" , \"left_arm\" ], 1 ) talkText ( \"Gesture recorded!\" ) print ( \"done!\" ) def unknown_intent_callback ( hermes , intent_message ): talkText ( \"I do not know this. But you can ask me to play gesture or show emotions.\" ) print ( \"done\" ) def intent_received ( hermes , intent_message ): hermes . publish_end_session ( intent_message . session_id , None ) coming_intent = intent_message . intent . intent_name print ( intent_message . intent . confidence_score ) if intent_message . intent . confidence_score < 0.5 : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () return print if coming_intent == 'apaikan:Play' : action_thread = threading . Thread ( target = play_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Record' : action_thread = threading . Thread ( target = record_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Emotion' : action_thread = threading . Thread ( target = emotion_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Stop' : action_thread = threading . Thread ( target = stop_intent_callback , args = ( hermes , intent_message )) action_thread . start () else : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () def intent_not_recognized ( hermes , intent_nr_message ): print ( \"not recogniozed\" ) action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_nr_message )) action_thread . start () if __name__ == \"__main__\" : # call the relevant service rospy . init_node ( 'qt_voice_interface' , disable_signals = True ) with Hermes ( 'localhost:1883' ) as h : # h.subscribe_intent(\"apaikan:Play\", play_intent_callback) # h.subscribe_intent(\"apaikan:Record\", record_intent_callback) # h.subscribe_intent(\"apaikan:Stop\", stop_intent_callback) # h.subscribe_intent(\"apaikan:Emotion\", emotion_intent_callback) h . subscribe_intents ( intent_received ) h . subscribe_intent_not_recognized ( intent_not_recognized ) h . loop_forever () # async mode using #h.loop_start() rospy.spin() h.loop_stop()","title":"**Microphone Voice Interaction**"},{"location":"demos/qt_microphone_interaction/#microphone-voice-interaction","text":"About This is microphone voice interaction demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Microphone Voice Interaction"},{"location":"demos/qt_microphone_interaction/#video","text":"","title":"Video"},{"location":"demos/qt_microphone_interaction/#setup","text":"NOTICE You must read and fully understand Snips voice system before starting this Please read the documentation of snips to understand it https://console.snips.ai/login How to set it up? This demo is already installed on your QTrobot. you can find it under ~/robot/code/qt_app Snips already installed on QTRP (head) how to run the snip servers: sudo systemctl start snips-asr.service snips-audio-server.service snips-dialogue.service snips-hotword.service snips-nlu.service how to run qt_voice_app demo rosrun qt_voice_app qt_voice_app.py Voice commands Hey QT, show me your happy emotion Hey QT, play happy gesture Snips configuration file /etc/snips.toml","title":"Setup"},{"location":"demos/qt_microphone_interaction/#code","text":"Check code here #!/usr/bin/env python2 # -*- coding: utf-8 -*- import time import threading from hermes_python.hermes import Hermes import rospy from std_msgs.msg import String from qt_gesture_controller.srv import * from qt_motors_controller.srv import * from qt_robot_interface.srv import * talkText = rospy . ServiceProxy ( '/qt_robot/behavior/talkText' , behavior_talk_text ) gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) gestureList = rospy . ServiceProxy ( '/qt_robot/gesture/list' , gesture_list ) gestureSave = rospy . ServiceProxy ( '/qt_robot/gesture/save' , gesture_save ) setControlMode = rospy . ServiceProxy ( '/qt_robot/motors/setControlMode' , set_control_mode ) gestureRecord = rospy . ServiceProxy ( '/qt_robot/gesture/record' , gesture_record ) #gesturePlay_pub = rospy.Publisher('/qt_robot/gesture/play', String, queue_size=10) emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) action_thread = None def emotion_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in emotion_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"ShowEmotion\" : emotion_name = slot [ 0 ] . slot_value . value . value try : print ( \"Showing emotion %s \" % \"QT/ %s \" % emotion_name ) talkText ( \"This is my %s face.\" % emotion_name . replace ( \"_\" , \" \" )) emotionShow_pub . publish ( \"QT/ %s \" % emotion_name ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for showing emotion %s \" % emotion_name ) else : talkText ( \"I did not understand which emotion.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def play_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) # action code goes here... print ( \"in play_intent_callback\" ) if intent_message . slots : for ( slot_value , slot ) in intent_message . slots . items (): if slot_value == \"GastureName\" : gesture_name = slot [ 0 ] . slot_value . value . value try : res = gestureList () if gesture_name not in res . gestures : talkText ( \"I do not know this gesture. But you can always record your gesture. Just ask me, hey Q.T. . record, new gesture!\" ) else : if gesture_name != \"my\" : gesture_name = \"QT/\" + gesture_name print ( \"playing gesture ' %s '.\" % gesture_name ) res = gesturePlay ( gesture_name , 1.0 ) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e talkText ( \"I have a problem for playing gesture %s \" % gesture_name ) else : talkText ( \"I did not understand which gesture.\" ) emotionShow_pub . publish ( \"QT/confused\" ) print ( \"done!\" ) def record_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in record_intent_callback\" ) talkText ( \"You can start recording your gesture.\" ) res = gestureRecord ([ \"right_arm\" , \"left_arm\" ], True , 0 , 0 ) print ( \"done!\" ) def stop_intent_callback ( hermes , intent_message ): #hermes.publish_end_session(intent_message.session_id, None) print ( \"in stop_intent_callback\" ) res = gestureSave ( \"my\" , \"\" ) if not res . status : talkText ( \"I have a problem with recording the gesture\" ) setControlMode ([ \"right_arm\" , \"left_arm\" ], 1 ) talkText ( \"Gesture recorded!\" ) print ( \"done!\" ) def unknown_intent_callback ( hermes , intent_message ): talkText ( \"I do not know this. But you can ask me to play gesture or show emotions.\" ) print ( \"done\" ) def intent_received ( hermes , intent_message ): hermes . publish_end_session ( intent_message . session_id , None ) coming_intent = intent_message . intent . intent_name print ( intent_message . intent . confidence_score ) if intent_message . intent . confidence_score < 0.5 : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () return print if coming_intent == 'apaikan:Play' : action_thread = threading . Thread ( target = play_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Record' : action_thread = threading . Thread ( target = record_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Emotion' : action_thread = threading . Thread ( target = emotion_intent_callback , args = ( hermes , intent_message )) action_thread . start () elif coming_intent == 'apaikan:Stop' : action_thread = threading . Thread ( target = stop_intent_callback , args = ( hermes , intent_message )) action_thread . start () else : action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_message )) action_thread . start () def intent_not_recognized ( hermes , intent_nr_message ): print ( \"not recogniozed\" ) action_thread = threading . Thread ( target = unknown_intent_callback , args = ( hermes , intent_nr_message )) action_thread . start () if __name__ == \"__main__\" : # call the relevant service rospy . init_node ( 'qt_voice_interface' , disable_signals = True ) with Hermes ( 'localhost:1883' ) as h : # h.subscribe_intent(\"apaikan:Play\", play_intent_callback) # h.subscribe_intent(\"apaikan:Record\", record_intent_callback) # h.subscribe_intent(\"apaikan:Stop\", stop_intent_callback) # h.subscribe_intent(\"apaikan:Emotion\", emotion_intent_callback) h . subscribe_intents ( intent_received ) h . subscribe_intent_not_recognized ( intent_not_recognized ) h . loop_forever () # async mode using #h.loop_start() rospy.spin() h.loop_stop()","title":"Code"},{"location":"demos/qt_range_of_motion/","text":"Range of Motion and DOF About This is range of motion and DOF demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository. Video Code Check code here #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"**Range of Motion and DOF**"},{"location":"demos/qt_range_of_motion/#range-of-motion-and-dof","text":"About This is range of motion and DOF demo with QTrobot. Check the video for demonstration. Full code for this demo you can find in our github tutorial repository.","title":"Range of Motion and DOF"},{"location":"demos/qt_range_of_motion/#video","text":"","title":"Video"},{"location":"demos/qt_range_of_motion/#code","text":"Check code here #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"Code"},{"location":"examples/qt_audio_interface/","text":"Accessing QTrobot audio interface with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot audio interface with rospy module. This interface allows you to play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy. In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wav and so on. NOTICE The default path for the audio files is '~/robot/data/audios/' Audio interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.2 Audio Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot audio interface is \"/qt_robot/audio/play\" and data type is text \"String\" audio file name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a audio ros publisher audioPlay_pub = rospy . Publisher ( '/qt_robot/audio/play' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( audioPlay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To play audio file with the QTrobot audio interface we need to call publish function of the ROS publisher(audioPlay_pub) that we created with the audio file name. Step 3 # publish audio file audioPlay_pub . publish ( \"QT/5LittleBunnies\" ) NOTICE To play the audio file from the default path, pass an empty string to filepath parameter. Get the full code in our github tutorial repository.","title":"Accessing QTrobot audio interface with Python"},{"location":"examples/qt_audio_interface/#accessing-qtrobot-audio-interface-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot audio interface with rospy module. This interface allows you to play a standard audio file (e.g. wav, mp3). The audio file can be given using its complete name (with the file extension) such as happy.mp3 or simply without the file extension: happy. In the second case, the player first looks for happy.mp3 to play and if it is not found it then tries with happy.wav and so on. NOTICE The default path for the audio files is '~/robot/data/audios/' Audio interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.2 Audio Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot audio interface is \"/qt_robot/audio/play\" and data type is text \"String\" audio file name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a audio ros publisher audioPlay_pub = rospy . Publisher ( '/qt_robot/audio/play' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( audioPlay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To play audio file with the QTrobot audio interface we need to call publish function of the ROS publisher(audioPlay_pub) that we created with the audio file name. Step 3 # publish audio file audioPlay_pub . publish ( \"QT/5LittleBunnies\" ) NOTICE To play the audio file from the default path, pass an empty string to filepath parameter. Get the full code in our github tutorial repository.","title":"Accessing QTrobot audio interface with Python"},{"location":"examples/qt_emotion_interface/","text":"Accessing QTrobot Emotion interface with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot emotion interface with rospy module. This interface allows you to change the QTrobot facial emotions such as 'QT/happy', 'QT/sad', etc. NOTICE The complete list of emotion files can be found in '~/robot/data/emotions/'. Emotion interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.3 Emotion Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot emotion interface is \"/qt_robot/emotion/show\" and data type is text \"String\" emotion name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a emotion ros publisher emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( emotionShow_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To show emotion with the QTrobot emotion interface we need to call publish function of the ROS publisher(emotionShow_pub) that we created with the emotion file name. Step 3 # publish emotion to QTrobot emotionShow_pub . publish ( \"QT/happy\" ) Get the full code in our github tutorial repository.","title":"Accessing QTrobot Emotion interface with Python"},{"location":"examples/qt_emotion_interface/#accessing-qtrobot-emotion-interface-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot emotion interface with rospy module. This interface allows you to change the QTrobot facial emotions such as 'QT/happy', 'QT/sad', etc. NOTICE The complete list of emotion files can be found in '~/robot/data/emotions/'. Emotion interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.3 Emotion Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot emotion interface is \"/qt_robot/emotion/show\" and data type is text \"String\" emotion name. Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a emotion ros publisher emotionShow_pub = rospy . Publisher ( '/qt_robot/emotion/show' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( emotionShow_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To show emotion with the QTrobot emotion interface we need to call publish function of the ROS publisher(emotionShow_pub) that we created with the emotion file name. Step 3 # publish emotion to QTrobot emotionShow_pub . publish ( \"QT/happy\" ) Get the full code in our github tutorial repository.","title":"Accessing QTrobot Emotion interface with Python"},{"location":"examples/qt_gesture_example/","text":"Reading QTrobot nuitrack topics with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack gesture topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Gesture topic publishes some basic human gestures recognition (SWIPE UP, SWIPE DOWN, SWIPE LEFT, SWIPE RIGHT, WAVING, PUSH). You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/gestures' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Gestures data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Gestures Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def gesture_callback ( msg ): rospy . loginfo ( msg ) def listener (): gesture_sub = rospy . Subscriber ( '/qt_nuitrack_app/gestures' , Gestures , gesture_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot and try to do one of the gestures. If QTrobot detects your gesture you will see something like this in your terminal: [ INFO ] [ 1620741522 .673911 ] : gestures: - id: 2 name: \"SWIPE UP\"","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_gesture_example/#reading-qtrobot-nuitrack-topics-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to read from QTrobot nuitrack gesture topic with rospy module. You can easily customize this code to read from any other Qtrobot nuitrack topic. Gesture topic publishes some basic human gestures recognition (SWIPE UP, SWIPE DOWN, SWIPE LEFT, SWIPE RIGHT, WAVING, PUSH). You can also check \"4.8 Human 3D Tracking Interface\" chapter on Wiki ROS. To create a ROS subscriber we need topic name (Interface name), data type and callback for '/qt_nuitrack_app/gestures' interface. PUBLISHERS Interface Name Data Type Description /qt_nuitrack_app/skeletons 'qt_nuitrack_app/Skeletons' publishes human full body skeleton information /qt_nuitrack_app/hands 'qt_nuitrack_app/Hands' publishes human 3D hand pos and state /qt_nuitrack_app/gestures 'qt_nuitrack_app/Gestures' publishes some basic human gestures recognition /qt_nuitrack_app/faces 'qt_nuitrack_app/Faces' publishes human face detection, orientation, eyes and facial features pos, age/gender detection and facial emotion recognition /camera/color/image_raw 'sensor_msgs/Image' publishes ROS standard 2D camera image First we should import rospy and Gestures data type from qt_nuitrack_app. Step 1 import rospy from qt_nuitrack_app.msg import Gestures Next up we define callback and listener functions. To create a ROS subscriber we need topic name, data type and callback. Step 2 def gesture_callback ( msg ): rospy . loginfo ( msg ) def listener (): gesture_sub = rospy . Subscriber ( '/qt_nuitrack_app/gestures' , Gestures , gesture_callback ) In main we define ROS node which in ROS are uniquely named. If two nodes with the same name are launched, the previous one is kicked off. The anonymous=True flag means that rospy will choose a unique name for our listener node so that multiple listeners can run simultaneously. We need to run rospy.spin() which simply keeps python from exiting until this node is stopped Step 3 if __name__ == '__main__' : rospy . init_node ( 'listener' , anonymous = True ) listener () try : rospy . spin () except KeyboardInterrupt : print ( \"Shutting down\" ) When you run the code you need to stand in front of QTrobot and try to do one of the gestures. If QTrobot detects your gesture you will see something like this in your terminal: [ INFO ] [ 1620741522 .673911 ] : gestures: - id: 2 name: \"SWIPE UP\"","title":"Reading QTrobot nuitrack topics with Python"},{"location":"examples/qt_gspeech_service/","text":"QTrobot Google Speech-To-Text ROS Service Get the full code in our github tutorial repository. Notice Everything should be installed on QTRP (head) - 192.168.100.1 1. QTrobot with pre-installed google speech interface Notice Please follow the instruction bellow if your QTrobot came with pre-installed google speech recognition 1.1 Setup Google Cloud and download your private key Follow the 1. Step from instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application. 1.2 Connect to QTrobot hotspot and ssh to QTrobot and initalize Google cloud ssh qtrobot@192.168.100.1 gcloud init When initializing google cloud just follow the steps from command line (login to your google account, select your google-cloud-server) 1.3 Enable qt_gspeech_interface sudo nano ~/robot/autostart/autostart_screens.sh Uncomment the last line in autostart script \"run_script \"start_qt_gspeech_interface.sh\"\". Save it (Ctrl+O), exit (Ctrl+X) and reboot the QTrobot. 1.4 Running the QTrobot Google Speech Ros service To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\" 2. Full installation of google speech interface Connect to QTrobot hotspot and ssh to QTrobot ssh qtrobot@192.168.100.1 2.1 Prepare QTrobot (get files and setup the environment) Clone the github repository into \"robot/code\" folder cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git Copy the autostart script to autostart folder cp ~/robot/code/tutorials/examples/qt_gspeech_interface/autostart/start_qt_gspeech_interface.sh ~/robot/autostart Enable the qt_gspeech_interface in autostart_screen.sh sudo nano ~/robot/autostart/autostart_screens.sh run_script \"start_qt_gspeech_interface.sh\" Below the last command \"run_script\" copy and paste the command above. Save it (Ctrl+O) and exit (Ctrl+X). 2.2 Install python3 virtualenv and portaudio sudo apt-get update && sudo apt-get install python3-venv portaudio19-dev 2.3 Create Python3 virtualenv and install requirements Install everything inside project folder (qt_gspeech_interface) python3 -m venv .venv && source .venv/bin/activate pip3 install --upgrade \"pip < 21.0\" pip3 install -r requirements.txt 2.4 Setup Google Cloud Follow the instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application. Install Google Cloud SDK and Google Cloud Python SDK (4.step) 2.5 Link your folder and build catkin qt_gspeech_interface package cd ~/catkin_ws/src && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface . cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface 2.6 Running the QTrobot Google Speech Ros service Make sure that you have uncommented \"run_script \"start_qt_gspeech_interface.sh\"\" in autostart_screen.sh. After that just reboot the QTrobot. To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\" 2.7 Additional If you want to call this service outside of QTRP environment then you should copy content of qtpc_gspeech to your catkin workspace copy qtpc_gspeech to your PC or QTPC These commands include \"robot/code\" folder just for the purpose of the example. Include your folder name in the command. cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git cd ~/catkin_ws/src/ && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface/qtpc_gspeech qt_gspeech_interface cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface","title":"QTrobot Google Speech-To-Text ROS Service"},{"location":"examples/qt_gspeech_service/#qtrobot-google-speech-to-text-ros-service","text":"Get the full code in our github tutorial repository. Notice Everything should be installed on QTRP (head) - 192.168.100.1","title":"QTrobot Google Speech-To-Text ROS Service"},{"location":"examples/qt_gspeech_service/#1-qtrobot-with-pre-installed-google-speech-interface","text":"Notice Please follow the instruction bellow if your QTrobot came with pre-installed google speech recognition","title":"1. QTrobot with pre-installed google speech interface"},{"location":"examples/qt_gspeech_service/#11-setup-google-cloud-and-download-your-private-key","text":"Follow the 1. Step from instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application.","title":"1.1 Setup Google Cloud and download your private key"},{"location":"examples/qt_gspeech_service/#12-connect-to-qtrobot-hotspot-and-ssh-to-qtrobot-and-initalize-google-cloud","text":"ssh qtrobot@192.168.100.1 gcloud init When initializing google cloud just follow the steps from command line (login to your google account, select your google-cloud-server)","title":"1.2 Connect to QTrobot hotspot and ssh to QTrobot and initalize Google cloud"},{"location":"examples/qt_gspeech_service/#13-enable-qt_gspeech_interface","text":"sudo nano ~/robot/autostart/autostart_screens.sh Uncomment the last line in autostart script \"run_script \"start_qt_gspeech_interface.sh\"\". Save it (Ctrl+O), exit (Ctrl+X) and reboot the QTrobot.","title":"1.3 Enable qt_gspeech_interface"},{"location":"examples/qt_gspeech_service/#14-running-the-qtrobot-google-speech-ros-service","text":"To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\"","title":"1.4 Running the QTrobot Google Speech Ros service"},{"location":"examples/qt_gspeech_service/#2-full-installation-of-google-speech-interface","text":"Connect to QTrobot hotspot and ssh to QTrobot ssh qtrobot@192.168.100.1","title":"2. Full installation of google speech interface"},{"location":"examples/qt_gspeech_service/#21-prepare-qtrobot-get-files-and-setup-the-environment","text":"Clone the github repository into \"robot/code\" folder cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git Copy the autostart script to autostart folder cp ~/robot/code/tutorials/examples/qt_gspeech_interface/autostart/start_qt_gspeech_interface.sh ~/robot/autostart Enable the qt_gspeech_interface in autostart_screen.sh sudo nano ~/robot/autostart/autostart_screens.sh run_script \"start_qt_gspeech_interface.sh\" Below the last command \"run_script\" copy and paste the command above. Save it (Ctrl+O) and exit (Ctrl+X).","title":"2.1 Prepare QTrobot (get files and setup the environment)"},{"location":"examples/qt_gspeech_service/#22-install-python3-virtualenv-and-portaudio","text":"sudo apt-get update && sudo apt-get install python3-venv portaudio19-dev","title":"2.2 Install python3 virtualenv and portaudio"},{"location":"examples/qt_gspeech_service/#23-create-python3-virtualenv-and-install-requirements","text":"Install everything inside project folder (qt_gspeech_interface) python3 -m venv .venv && source .venv/bin/activate pip3 install --upgrade \"pip < 21.0\" pip3 install -r requirements.txt","title":"2.3 Create Python3 virtualenv and install requirements"},{"location":"examples/qt_gspeech_service/#24-setup-google-cloud","text":"Follow the instructions to setup your Google Console and SpeechToText From 1.Step Download a private key as JSON and save it inside \"~/robot/code/tutorials/examples/qt_gspeech_interface\" folder you will need it to run the application. Install Google Cloud SDK and Google Cloud Python SDK (4.step)","title":"2.4 Setup Google Cloud"},{"location":"examples/qt_gspeech_service/#25-link-your-folder-and-build-catkin-qt_gspeech_interface-package","text":"cd ~/catkin_ws/src && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface . cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface","title":"2.5 Link your folder and build catkin qt_gspeech_interface package"},{"location":"examples/qt_gspeech_service/#26-running-the-qtrobot-google-speech-ros-service","text":"Make sure that you have uncommented \"run_script \"start_qt_gspeech_interface.sh\"\" in autostart_screen.sh. After that just reboot the QTrobot. To check if it is running you can list all rosservices rosservice list You should see /qt_robot/speech/recognize To run: rosservice call /qt_gspeech_service \"language:'' options: - '' timeout:10\"","title":"2.6 Running the QTrobot Google Speech Ros service"},{"location":"examples/qt_gspeech_service/#27-additional","text":"If you want to call this service outside of QTRP environment then you should copy content of qtpc_gspeech to your catkin workspace copy qtpc_gspeech to your PC or QTPC These commands include \"robot/code\" folder just for the purpose of the example. Include your folder name in the command. cd ~/robot/code && git clone https://github.com/luxai-qtrobot/tutorials.git cd ~/catkin_ws/src/ && ln -s ~/robot/code/tutorials/examples/qt_gspeech_interface/qtpc_gspeech qt_gspeech_interface cd ~/catkin_ws && catkin_make --pkg qt_gspeech_interface","title":"2.7 Additional"},{"location":"examples/qt_motors_command/","text":"QTrobot Motors command Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"QTrobot Motors command"},{"location":"examples/qt_motors_command/#qtrobot-motors-command","text":"Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import sys import rospy from std_msgs.msg import Float64MultiArray if __name__ == '__main__' : rospy . init_node ( 'qt_motor_command' ) # create a publisher right_pub = rospy . Publisher ( '/qt_robot/right_arm_position/command' , Float64MultiArray , queue_size = 1 ) # wait for publisher/subscriber connections wtime_begin = rospy . get_time () while ( right_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for subscriber connections...\" ) if rospy . get_time () - wtime_begin > 10.0 : rospy . logerr ( \"Timeout while waiting for subscribers connection!\" ) sys . exit () rospy . sleep ( 1 ) rospy . loginfo ( \"publishing motor commnad...\" ) try : ref = Float64MultiArray () RightShoulderPitch = 90 RightShoulderRoll = 0 RightElbowRoll = - 70 ref . data = [ RightShoulderPitch , RightShoulderRoll , RightElbowRoll ] right_pub . publish ( ref ) except rospy . ROSInterruptException : rospy . logerr ( \"could not publish motor commnad!\" ) rospy . loginfo ( \"motor commnad published\" )","title":"QTrobot Motors command"},{"location":"examples/qt_motors_custom_controller/","text":"QTrobot Motors ROS controller Get the full code in our github tutorial repository. This example shows how to write a new ROS controller for QTrobot. Before getting into that, make sure you know and understand what is ROS Conrol . What it does? this controller simply play a predefined trajectory directly by commanding the QTrobot motors at hardware level. You can use it as a simple template to implement more sophisticate controllers. Compilation and build copy motors_custom_controller to the ~/catckin_ws on the RPI of QTrobot build the example code: cd ~/catckin_ws catkin_make --pkg motors_custom_controller Configuration and launching modify the qt_motor config file to define your new controller. to do that open /opt/ros/kinetic/share/qt_motor/config/qtrobot-controller.yaml file and add the following lines to the end of the file. custom: type: motors_custom_controller/QTCustomController myparam: bar modify qt_motor launch file to load and run your controller. to do that, open /opt/ros/kinetic/share/qt_motor/launch/qt_motor.launch and add /qt_robot/custom controller to controller_manager node. it should look like this: <!-- load the controllers --> <node pkg=\"controller_manager\" name=\"controller\" type=\"spawner\" launch-prefix=\"bash -c 'sleep 15; $0 $@' \" respawn=\"false\" output=\"screen\" clear_params=\"true\" args=\"/qt_robot/head_position /qt_robot/right_arm_position /qt_robot/left_arm_position /qt_robot/joints_state /qt_robot/motors /qt_robot/gesture /qt_robot/custom\"/> Re-launch qt_motor node you can simply reboot the robot or launch the qt_motor node from terminal to see its output. * stop the running instance: ps -aux | grep qt_motor | grep SCREEN ... kill xxx launch qt_motor node roslaunch qt_motor qt_motor.launch you should see your controller ( QTCustomController ) running by checking the terminal output: ... [ INFO ] [ 1576498210.924577 ] : Controller Spawner : Loaded controllers : / qt_robot / head_position , / qt_robot / right_arm_position , / qt_robot / left_arm_position , / qt_robot / joints_state , / qt_robot / motors , / qt_robot / gesture , / qt_robot / custom [ INFO ] [ 1576498210.984396566 ] : QTMotorsController : starting [ INFO ] [ 1576498210.985034385 ] : QTGestureController : starting [ INFO ] [ 1576498210.985518609 ] : QTCustomController : starting ... Testing your custom controller implement a service to Start/Stop it. to start the trajectory player, open another termianl and rosservice call /qt_robot/custom/startstop \"command: true\" to stop the trajectory player: rosservice call /qt_robot/custom/startstop \"command: false\"","title":"QTrobot Motors ROS controller"},{"location":"examples/qt_motors_custom_controller/#qtrobot-motors-ros-controller","text":"Get the full code in our github tutorial repository. This example shows how to write a new ROS controller for QTrobot. Before getting into that, make sure you know and understand what is ROS Conrol .","title":"QTrobot Motors ROS controller"},{"location":"examples/qt_motors_custom_controller/#what-it-does","text":"this controller simply play a predefined trajectory directly by commanding the QTrobot motors at hardware level. You can use it as a simple template to implement more sophisticate controllers.","title":"What it does?"},{"location":"examples/qt_motors_custom_controller/#compilation-and-build","text":"copy motors_custom_controller to the ~/catckin_ws on the RPI of QTrobot build the example code: cd ~/catckin_ws catkin_make --pkg motors_custom_controller","title":"Compilation and build"},{"location":"examples/qt_motors_custom_controller/#configuration-and-launching","text":"modify the qt_motor config file to define your new controller. to do that open /opt/ros/kinetic/share/qt_motor/config/qtrobot-controller.yaml file and add the following lines to the end of the file. custom: type: motors_custom_controller/QTCustomController myparam: bar modify qt_motor launch file to load and run your controller. to do that, open /opt/ros/kinetic/share/qt_motor/launch/qt_motor.launch and add /qt_robot/custom controller to controller_manager node. it should look like this: <!-- load the controllers --> <node pkg=\"controller_manager\" name=\"controller\" type=\"spawner\" launch-prefix=\"bash -c 'sleep 15; $0 $@' \" respawn=\"false\" output=\"screen\" clear_params=\"true\" args=\"/qt_robot/head_position /qt_robot/right_arm_position /qt_robot/left_arm_position /qt_robot/joints_state /qt_robot/motors /qt_robot/gesture /qt_robot/custom\"/>","title":"Configuration and launching"},{"location":"examples/qt_motors_custom_controller/#re-launch-qt_motor-node","text":"you can simply reboot the robot or launch the qt_motor node from terminal to see its output. * stop the running instance: ps -aux | grep qt_motor | grep SCREEN ... kill xxx launch qt_motor node roslaunch qt_motor qt_motor.launch you should see your controller ( QTCustomController ) running by checking the terminal output: ... [ INFO ] [ 1576498210.924577 ] : Controller Spawner : Loaded controllers : / qt_robot / head_position , / qt_robot / right_arm_position , / qt_robot / left_arm_position , / qt_robot / joints_state , / qt_robot / motors , / qt_robot / gesture , / qt_robot / custom [ INFO ] [ 1576498210.984396566 ] : QTMotorsController : starting [ INFO ] [ 1576498210.985034385 ] : QTGestureController : starting [ INFO ] [ 1576498210.985518609 ] : QTCustomController : starting ...","title":"Re-launch qt_motor node"},{"location":"examples/qt_motors_custom_controller/#testing","text":"your custom controller implement a service to Start/Stop it. to start the trajectory player, open another termianl and rosservice call /qt_robot/custom/startstop \"command: true\" to stop the trajectory player: rosservice call /qt_robot/custom/startstop \"command: false\"","title":"Testing"},{"location":"examples/qt_motors_gesture/","text":"QTrobot Motors gesture Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from qt_gesture_controller.srv import * def print_help (): print ( \"QTrobot gesture player example\" ) print ( \"Usage:\" ) print ( \" qt_motors_gesture <name> play a gesture given by its <name>\" ) print ( \"\" ) # main if __name__ == '__main__' : # check the params if len ( sys . argv ) < 2 : print_help () sys . exit ( 1 ) # call the relevant service rospy . init_node ( 'qt_motors_gesture' ) try : gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) res = gesturePlay ( sys . argv [ 1 ], 1.0 ) if not res . status : print ( \"Could not play gesture ' %s '.\" % sys . argv [ 1 ]) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e","title":"QTrobot Motors gesture"},{"location":"examples/qt_motors_gesture/#qtrobot-motors-gesture","text":"Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from qt_gesture_controller.srv import * def print_help (): print ( \"QTrobot gesture player example\" ) print ( \"Usage:\" ) print ( \" qt_motors_gesture <name> play a gesture given by its <name>\" ) print ( \"\" ) # main if __name__ == '__main__' : # check the params if len ( sys . argv ) < 2 : print_help () sys . exit ( 1 ) # call the relevant service rospy . init_node ( 'qt_motors_gesture' ) try : gesturePlay = rospy . ServiceProxy ( '/qt_robot/gesture/play' , gesture_play ) res = gesturePlay ( sys . argv [ 1 ], 1.0 ) if not res . status : print ( \"Could not play gesture ' %s '.\" % sys . argv [ 1 ]) except rospy . ServiceException , e : print \"Service call failed: %s .\" % e","title":"QTrobot Motors gesture"},{"location":"examples/qt_motors_moveit/","text":"Controlling QTrobot arms using MoveIt Get the full code in our github tutorial repository. This advanced example demonstrates how to use ROS MoveIT to control QTrobot arms. The example draw some shapes (i,e. rectangle and spirals) on the XY plane in robot frame. Preparation and requirements Before running the example, please ensure that following setups of your QTrobot and the machine which you are running the example. QTrobot setup By default QTrobot motors interface runs in 'normal' mode. in normal mode, the motors control loop and joints state publisher run in low frequency (2-5hz). More importantly the joint position values are in degree . To be able to use MoveIt with QTrobot, you need to configure it to run in 'advanced' mode: - joints position value is in radian - motors main controller loop and joints state publisher runs in 30hz. - required interfaces and controller such as robot_state_publisher and JointTrajectoryController are available Update ros-kinetic-qt-motor package if necessary: First check and update (if necessary) the ros-kinetic-qt-motor package on the QTrobot head machine (QTRP). ssh to the QTRP (e.g. via QTPC or your desktop) and qtrobot@QTXXX: apt list ros-kinetic-qt-motor ros-kinetic-qt-motor/now 1.2.0-0xenial armhf [installed,local] if the version of ros-kinetic-qt-motor is older then 1.2.0 , you NEED to update it: make a copy of your current QTrobot configuration ( qtrobot-hardware.yaml ). You need this file to put it back after updating the package: qtrobot@QTXXX: cp /opt/ros/kinetic/share/qt_motor/config/qtrobot-hardware.yaml ~/ install the latest version of ros-kinetic-qt-motor . in my case it is 1.2.0-0 : qtrobot@QTXXX: cd ~/robot/packages/dep qtrobot@QTXXX: git pull qtrobot@QTXXX: sudo apt remove ros-kinetic-qt-motor qtrobot@QTXXX: sudo dpkg -i ros-kinetic-qt-motor_1.2.0-0xenial_armhf.deb put back your QTrobot configuration ( qtrobot-hardware.yaml ): qtrobot@QTXXX: sudo cp ~/qtrobot-hardware.yaml /opt/ros/kinetic/share/qt_motor/config/ Change motor launcher autostart script to run in advanced mode: qtrobot@QTXXX: nano ~/robot/autostart/start_qt_motor.sh and change the corresponding line to look like the following and save and exit: roslaunch qt_motor qt_motor_advanced.launch Reboot the robot to run the advance motor interface. Check the advanced mode setup: After rebooting the reboot, you can check (from QTPC, QTRP or your machine) if the motor interface is running in the advanced mode: joint state publisher frequency: qtrobot@QTXXX: rostopic hz /qt_robot/joints/state average rate: 30.041 min: 0.029s max: 0.047s std dev: 0.00391s window: 29 joints value should be in radian: qtrobot@QTXXX: rostopic echo /qt_robot/joints/state position: [0.015707962851830046, 0.0, -0.6073745663782212, 1.569051024174513, -0.9896016991965904, -0.5689773095185405, -0.3455751785790718, -0.8360127383368947] trajectory controller is running: qtrobot@QTXXX: rostopic type /qt_robot/left_arm_controller/follow_joint_trajectory/goal control_msgs/FollowJointTrajectoryActionGoal Your machine setup (e.g. QTPC) After checking and updating the QTrobot setup, you can install the iKfast solver plugin for MoveIt on the machine which you plan to run the example: clone the QTrobot open software repository: cd ~/ git clone https://github.com/luxai-qtrobot/software.git build the plugins: cd ~/catkin_ws ln -s ~/software/qtrobot_ikfast_right_arm_plugin/ ./ ln -s ~/software/qtrobot_ikfast_right_left_plugin/ ./ cd ../ catkin_make clone and build the motors_moveit Assuming that you have cloned the tutorial repository somewhere on your home folder (e.g. ~/tutorials ): cd ~/catkin_ws ln -s ~/tutorials/examples/motors_moveit ./ cd ../ catkin_make or copy the motors_moveit to your catkin_ws. How to run the examples Launch moveit_qtrobot.launch to start move_group planner and rviz: roslaunch motors_moveit moveit_qtrobot.launch wait until rviz shows up, then run one of the following demos: Drawing rectangle rosrun motors_moveit draw_rectangle.py joint_states:=/qt_robot/joints/state Drawing spiral rosrun motors_moveit draw_spiral.py joint_states:=/qt_robot/joints/state","title":"Controlling QTrobot arms using MoveIt"},{"location":"examples/qt_motors_moveit/#controlling-qtrobot-arms-using-moveit","text":"Get the full code in our github tutorial repository. This advanced example demonstrates how to use ROS MoveIT to control QTrobot arms. The example draw some shapes (i,e. rectangle and spirals) on the XY plane in robot frame.","title":"Controlling QTrobot arms using MoveIt"},{"location":"examples/qt_motors_moveit/#preparation-and-requirements","text":"Before running the example, please ensure that following setups of your QTrobot and the machine which you are running the example.","title":"Preparation and requirements"},{"location":"examples/qt_motors_moveit/#qtrobot-setup","text":"By default QTrobot motors interface runs in 'normal' mode. in normal mode, the motors control loop and joints state publisher run in low frequency (2-5hz). More importantly the joint position values are in degree . To be able to use MoveIt with QTrobot, you need to configure it to run in 'advanced' mode: - joints position value is in radian - motors main controller loop and joints state publisher runs in 30hz. - required interfaces and controller such as robot_state_publisher and JointTrajectoryController are available Update ros-kinetic-qt-motor package if necessary: First check and update (if necessary) the ros-kinetic-qt-motor package on the QTrobot head machine (QTRP). ssh to the QTRP (e.g. via QTPC or your desktop) and qtrobot@QTXXX: apt list ros-kinetic-qt-motor ros-kinetic-qt-motor/now 1.2.0-0xenial armhf [installed,local] if the version of ros-kinetic-qt-motor is older then 1.2.0 , you NEED to update it: make a copy of your current QTrobot configuration ( qtrobot-hardware.yaml ). You need this file to put it back after updating the package: qtrobot@QTXXX: cp /opt/ros/kinetic/share/qt_motor/config/qtrobot-hardware.yaml ~/ install the latest version of ros-kinetic-qt-motor . in my case it is 1.2.0-0 : qtrobot@QTXXX: cd ~/robot/packages/dep qtrobot@QTXXX: git pull qtrobot@QTXXX: sudo apt remove ros-kinetic-qt-motor qtrobot@QTXXX: sudo dpkg -i ros-kinetic-qt-motor_1.2.0-0xenial_armhf.deb put back your QTrobot configuration ( qtrobot-hardware.yaml ): qtrobot@QTXXX: sudo cp ~/qtrobot-hardware.yaml /opt/ros/kinetic/share/qt_motor/config/ Change motor launcher autostart script to run in advanced mode: qtrobot@QTXXX: nano ~/robot/autostart/start_qt_motor.sh and change the corresponding line to look like the following and save and exit: roslaunch qt_motor qt_motor_advanced.launch Reboot the robot to run the advance motor interface. Check the advanced mode setup: After rebooting the reboot, you can check (from QTPC, QTRP or your machine) if the motor interface is running in the advanced mode: joint state publisher frequency: qtrobot@QTXXX: rostopic hz /qt_robot/joints/state average rate: 30.041 min: 0.029s max: 0.047s std dev: 0.00391s window: 29 joints value should be in radian: qtrobot@QTXXX: rostopic echo /qt_robot/joints/state position: [0.015707962851830046, 0.0, -0.6073745663782212, 1.569051024174513, -0.9896016991965904, -0.5689773095185405, -0.3455751785790718, -0.8360127383368947] trajectory controller is running: qtrobot@QTXXX: rostopic type /qt_robot/left_arm_controller/follow_joint_trajectory/goal control_msgs/FollowJointTrajectoryActionGoal","title":"QTrobot setup"},{"location":"examples/qt_motors_moveit/#your-machine-setup-eg-qtpc","text":"After checking and updating the QTrobot setup, you can install the iKfast solver plugin for MoveIt on the machine which you plan to run the example: clone the QTrobot open software repository: cd ~/ git clone https://github.com/luxai-qtrobot/software.git build the plugins: cd ~/catkin_ws ln -s ~/software/qtrobot_ikfast_right_arm_plugin/ ./ ln -s ~/software/qtrobot_ikfast_right_left_plugin/ ./ cd ../ catkin_make","title":"Your machine setup (e.g. QTPC)"},{"location":"examples/qt_motors_moveit/#clone-and-build-the-motors_moveit","text":"Assuming that you have cloned the tutorial repository somewhere on your home folder (e.g. ~/tutorials ): cd ~/catkin_ws ln -s ~/tutorials/examples/motors_moveit ./ cd ../ catkin_make or copy the motors_moveit to your catkin_ws.","title":"clone and build the motors_moveit"},{"location":"examples/qt_motors_moveit/#how-to-run-the-examples","text":"Launch moveit_qtrobot.launch to start move_group planner and rviz: roslaunch motors_moveit moveit_qtrobot.launch wait until rviz shows up, then run one of the following demos:","title":"How to run the examples"},{"location":"examples/qt_motors_moveit/#drawing-rectangle","text":"rosrun motors_moveit draw_rectangle.py joint_states:=/qt_robot/joints/state","title":"Drawing rectangle"},{"location":"examples/qt_motors_moveit/#drawing-spiral","text":"rosrun motors_moveit draw_spiral.py joint_states:=/qt_robot/joints/state","title":"Drawing spiral"},{"location":"examples/qt_motors_state/","text":"QTrobot Motors state Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from sensor_msgs.msg import JointState def joint_states_callback ( msg ): strmsg = \"\" for i , joint_name in enumerate ( msg . name ): strmsg += \" %s : %.2f , \" % ( joint_name , msg . position [ i ]) rospy . loginfo ( strmsg ) # main if __name__ == '__main__' : # call the relevant service rospy . init_node ( 'qt_motors_state' ) rospy . Subscriber ( '/qt_robot/joints/state' , JointState , joint_states_callback ) rospy . spin ()","title":"QTrobot Motors state"},{"location":"examples/qt_motors_state/#qtrobot-motors-state","text":"Get the full code in our github tutorial repository. Code #!/usr/bin/env python ''' Copyright (C) 2018 LuxAI S.A Authors: Ali Paikan CopyPolicy: Released under the terms of the LGPLv2.1 or later, see LGPL.TXT ''' import rospy from sensor_msgs.msg import JointState def joint_states_callback ( msg ): strmsg = \"\" for i , joint_name in enumerate ( msg . name ): strmsg += \" %s : %.2f , \" % ( joint_name , msg . position [ i ]) rospy . loginfo ( strmsg ) # main if __name__ == '__main__' : # call the relevant service rospy . init_node ( 'qt_motors_state' ) rospy . Subscriber ( '/qt_robot/joints/state' , JointState , joint_states_callback ) rospy . spin ()","title":"QTrobot Motors state"},{"location":"examples/qt_speech_interface/","text":"Accessing QTrobot speech interface with Python The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot speech interface with rospy module.This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot, but for this example we will use American English (en-US). Speech interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.1 Speech Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot speech interface is \"/qt_robot/speech/say\" and data type is text \"String\". Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a ros publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( speechSay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To publish text to the QTrobot speech interface we need to call publish function of the ROS publisher(speechSay_pub) that we created. Step 3 # publish a text message to TTS speechSay_pub . publish ( \"Hello!\" ) NOTICE Default language is set in '/opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml'. the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them. Get the full code in our github tutorial repository.","title":"Accessing QTrobot speech interface with Python"},{"location":"examples/qt_speech_interface/#accessing-qtrobot-speech-interface-with-python","text":"The QTrobot is using ROS for easier access to basic robot functionalities with ROS publish/subscriber and Service/Client interfaces. Rospy is a pure Python client library for ROS. The rospy client API enables Python programmers to quickly interface with ROS Topics, Services, and Parameters. We will use Python code to access QTrobot speech interface with rospy module.This interface implements QTrobot text to speech functionality which support many languages. The supported languages can be different for each robot, but for this example we will use American English (en-US). Speech interface is accessible with publish/subscribe and service/client methods. In this example we will use publish/subscribe method. You can also check \"4.1 Speech Interface\" chapter on Wiki ROS. First we should import rospy, QTrobot interface module and data type. Step 1 import sys import rospy from std_msgs.msg import String from qt_robot_interface.srv import * Next up we initialize ROS node and declare a name for it. To create a ROS publisher we need topic name, data type and size of our queue. Topic name for accessing QTrobot speech interface is \"/qt_robot/speech/say\" and data type is text \"String\". Step 2 # ros node rospy . init_node ( 'python_qt_example' ) # creating a ros publisher speechSay_pub = rospy . Publisher ( '/qt_robot/speech/say' , String , queue_size = 10 ) # waiting for connection establishment wtime_begin = rospy . get_time () while ( speechSay_pub . get_num_connections () == 0 ) : rospy . loginfo ( \"waiting for publisher connection\" ) if rospy . get_time () - wtime_begin > 5.0 : rospy . logerr ( \"Timeout while waiting for publisher connection!\" ) sys . exit () rospy . sleep ( 1 ) To publish text to the QTrobot speech interface we need to call publish function of the ROS publisher(speechSay_pub) that we created. Step 3 # publish a text message to TTS speechSay_pub . publish ( \"Hello!\" ) NOTICE Default language is set in '/opt/ros/kinetic/share/qt_robot_interface/config/qtrobot-interface.yaml'. the default pitch is usually '140' and speed is '80'. When calling 'speech_config', leave pitch and speed parameters to '0' if you do not want to change them. Get the full code in our github tutorial repository.","title":"Accessing QTrobot speech interface with Python"},{"location":"examples/qt_voice_activity/","text":"QTrobot Voice activity Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : print ( \"Voice activity %d \" % Mic_tuning . is_voice ()) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice activity"},{"location":"examples/qt_voice_activity/#qtrobot-voice-activity","text":"Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : print ( \"Voice activity %d \" % Mic_tuning . is_voice ()) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice activity"},{"location":"examples/qt_voice_direction/","text":"QTrobot Voice direction Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : if Mic_tuning . is_voice (): print ( \"I detected voice activity at angle %d \" % Mic_tuning . direction ) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice direction"},{"location":"examples/qt_voice_direction/#qtrobot-voice-direction","text":"Get the full code in our github tutorial repository. Code from tuning import Tuning import usb.core import usb.util import time if __name__ == '__main__' : dev = usb . core . find ( idVendor = 0x2886 , idProduct = 0x0018 ) if dev : Mic_tuning = Tuning ( dev ) while True : try : if Mic_tuning . is_voice (): print ( \"I detected voice activity at angle %d \" % Mic_tuning . direction ) time . sleep ( 0.2 ) except KeyboardInterrupt : break","title":"QTrobot Voice direction"}]}