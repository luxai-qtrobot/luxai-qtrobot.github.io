"use strict";(self.webpackChunkqtrobot_documentation=self.webpackChunkqtrobot_documentation||[]).push([[5149],{4694:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>c,toc:()=>d});var i=o(5893),r=o(1151),s=o(9656);const t={id:"microphone",title:"QTrobot Audio processing and Microphone",hide_table_of_contents:!1},a=void 0,c={id:"modules/microphone",title:"QTrobot Audio processing and Microphone",description:"QTrobot has an integrated High-performance digital microphones array in the head. It is a ReSpeaker Mic Array v2.0 board from SeedStudio with plenty of features such as voice activity detection, direction of arrival, beamforming and noise Suppression. This powerful microphone can be used in variety of scenarios such as voice interaction, interactive vision-voice applications, multichannel raw audio recording and processing and etc. It is connected to Raspberry Pi (QTRP) via USB port and it is open for developers to freely tune, configure and use it in standard ways.",source:"@site/docs/modules/microphone.mdx",sourceDirName:"modules",slug:"/modules/microphone",permalink:"/docs/modules/microphone",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"microphone",title:"QTrobot Audio processing and Microphone",hide_table_of_contents:!1},sidebar:"modules",previous:{title:"QTrobot Sound and Speech",permalink:"/docs/modules/speakers"},next:{title:"QTrobot Vision and 3D Camera",permalink:"/docs/modules/camera"}},l={},d=[{value:"Software interfaces",id:"software-interfaces",level:2},{value:"Offline speech recognition",id:"offline-speech-recognition",level:2},{value:"Accessing voice recognition from terminal",id:"accessing-voice-recognition-from-terminal",level:3},{value:"Accessing voice recognition from code",id:"accessing-voice-recognition-from-code",level:3},{value:"Accessing voice recognition using QTrobot visual studio blocks",id:"accessing-voice-recognition-using-qtrobot-visual-studio-blocks",level:3},{value:"Installing more languages",id:"installing-more-languages",level:3},{value:"Online speech recognition",id:"online-speech-recognition",level:2},{value:"Setup instructions",id:"setup-instructions",level:3},{value:"Google account and API credentials",id:"google-account-and-api-credentials",level:4},{value:"Python virtualenvironment",id:"python-virtualenvironment",level:4},{value:"Link to catkin workspace and build",id:"link-to-catkin-workspace-and-build",level:4},{value:"Autostart",id:"autostart",level:4},{value:"Accessing voice recognition from terminal",id:"accessing-voice-recognition-from-terminal-1",level:3},{value:"<strong>Tips</strong> for better speech recognition",id:"tips-for-better-speech-recognition",level:2},{value:"Accessing audio, voice direction and other data",id:"accessing-audio-voice-direction-and-other-data",level:2},{value:"Configuring qt_respeaker_app",id:"configuring-qt_respeaker_app",level:3},{value:"Recording raw audio data",id:"recording-raw-audio-data",level:3},{value:"Microphone tuning tool",id:"microphone-tuning-tool",level:2},{value:"What can I tune?",id:"what-can-i-tune",level:3},{value:"What can I do with the tunned values?",id:"what-can-i-do-with-the-tunned-values",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["QTrobot has an integrated High-performance digital microphones array in the head. It is a ",(0,i.jsx)(n.a,{href:"https://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0/",children:(0,i.jsx)(n.strong,{children:"ReSpeaker Mic Array v2.0"})})," board from SeedStudio with plenty of features such as voice activity detection, direction of arrival, beamforming and noise Suppression. This powerful microphone can be used in variety of scenarios such as voice interaction, interactive vision-voice applications, multichannel raw audio recording and processing and etc. It is connected to Raspberry Pi (QTRP) via USB port and it is open for developers to freely tune, configure and use it in standard ways."]}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:"/img/microphone.png",alt:"microphone"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Specification:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"4 High-performance digital microphones"}),"\n",(0,i.jsx)(n.li,{children:"supports far-field voice capture"}),"\n",(0,i.jsx)(n.li,{children:"Speech algorithm on-chip"}),"\n",(0,i.jsx)(n.li,{children:"12 programmable RGB LED indicators"}),"\n",(0,i.jsx)(n.li,{children:"Microphones: ST MP34DT01TR-M"}),"\n",(0,i.jsx)(n.li,{children:"Sensitivity: -26 dBFS (omnidirectional)"}),"\n",(0,i.jsx)(n.li,{children:"Acoustic overload point: 120 dB SPL"}),"\n",(0,i.jsx)(n.li,{children:"SNR: 61 dB"}),"\n",(0,i.jsx)(n.li,{children:"Max Sample Rate:16Khz"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"software-interfaces",children:"Software interfaces"}),"\n",(0,i.jsxs)(n.p,{children:["Like any other standard microphones, the QTrobot Respeaker microphone is an standard Linux capture device which is managed by ALSA driver. Bellow you can see the output of ",(0,i.jsx)(n.code,{children:"arecord -l"})," command which lists all capture devices in QTRP:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"**** List of CAPTURE Hardware Devices ****\r\ncard 2: ArrayUAC10 [ReSpeaker 4 Mic Array (UAC1.0)], device 0: USB Audio [USB Audio]\r\n  Subdevices: 0/1\r\n  Subdevice #0: subdevice #0\n"})}),"\n",(0,i.jsx)(n.p,{children:"QTrobot comes with following pre-installed software for speech recognition, accessing raw raw microphone data and tuning tool. These software interfaces are installed on QTRP because direct access to the microphone device is required."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_respeaker_app",children:(0,i.jsx)(n.strong,{children:"qt_respeaker_app"})}),": ROS services for streaming multichannel microphone audio data, voice activity, voice direction and etc. (",(0,i.jsx)(n.strong,{children:"running by default"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/tools/respeaker_mic_tunning",children:(0,i.jsx)(n.strong,{children:"respeaker_mic_tuning"})}),": A graphical tools for detailed tuning of Respeaker microphone"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Installed on QTPC:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_vosk_app",children:(0,i.jsx)(n.strong,{children:"qt_vosk_app"})}),": an ",(0,i.jsx)(n.em,{children:"offline"})," and multilingual speech recognition ROS service based on VOSK library. (",(0,i.jsx)(n.strong,{children:"running by default"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_gspeech_app",children:(0,i.jsx)(n.strong,{children:"qt_gspeech_app"})}),": an ",(0,i.jsx)(n.em,{children:"online"})," and multilingual speech recognition ROS service based on Google Speech-To-Text API"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"offline-speech-recognition",children:"Offline speech recognition"}),"\n",(0,i.jsx)(n.admonition,{title:"QTrobot AI@Edge",type:"info",children:(0,i.jsxs)(n.p,{children:["By default there is no speech recognition enabled, but you can easily setup ",(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_riva_asr_app",children:(0,i.jsx)(n.strong,{children:"qt_riva_asr_app"})})," to have offline speech recognition on the QTrobot."]})}),"\n",(0,i.jsxs)(n.p,{children:["QTrobot uses ",(0,i.jsx)(n.code,{children:"qt_vosk_app"})," for offline speech recognition. The ",(0,i.jsx)(n.code,{children:"qt_vosk_app"})," is installed in QTPC ",(0,i.jsx)(n.code,{children:"~/catkin_ws"})," and it is running by default. Some of the required language models for speech recognitions are installed in ",(0,i.jsx)(n.code,{children:"~/robot/vosk/models"})," in QTPC such as ",(0,i.jsx)(n.code,{children:"en_US"})," for English, ",(0,i.jsx)(n.code,{children:"de_DE"})," for German, ",(0,i.jsx)(n.code,{children:"fr_FR"})," for French languages.\r\nYou can try and use the speech recognitions in different ways."]}),"\n",(0,i.jsx)(n.h3,{id:"accessing-voice-recognition-from-terminal",children:"Accessing voice recognition from terminal"}),"\n",(0,i.jsxs)(n.p,{children:["Like many other ROS services, you can call ",(0,i.jsx)(n.code,{children:"/qt_robot/speech/recognize"})," with the desired language and options. The options can be used to tell the service to stop and return the recognized words as soon one of the given options is detected."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"$ rosservice call /qt_robot/speech/recognize \"language: 'en_US'\r\noptions:[]\r\ntimeout: 0\"\n"})}),"\n",(0,i.jsx)(n.h3,{id:"accessing-voice-recognition-from-code",children:"Accessing voice recognition from code"}),"\n",(0,i.jsxs)(n.p,{children:["Take a look at our ",(0,i.jsx)(n.a,{href:"/docs/tutorials/python/python_ros_vosk",children:(0,i.jsx)(n.strong,{children:"Python offline speech recognition"})})," tutorial to learn how to call ",(0,i.jsx)(n.code,{children:"qt_vosk_app"})," services from a Python code. Here is also a Python code snippet:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from qt_vosk_app.srv import *\r\nrecognize = rospy.ServiceProxy('/qt_robot/speech/recognize', speech_recognize)\r\nresp = recognize(\"en_US\", ['blue', 'green', 'red'], 10)\r\nprint(\"I got: %s\", resp.transcript)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"accessing-voice-recognition-using-qtrobot-visual-studio-blocks",children:"Accessing voice recognition using QTrobot visual studio blocks"}),"\n",(0,i.jsxs)(n.p,{children:["QTrobot studio offers very flexible and powerful blocks to handle complex ROS messages and interact with other publishers, subscribers and services. You can follow ",(0,i.jsx)(n.a,{href:"/docs/tutorials/graphical/studio_ros",children:(0,i.jsx)(n.strong,{children:"Using ROS blocks"})})," tutorial to learn how to call ",(0,i.jsx)(n.code,{children:"qt_vosk_app"})," using QTrobot visual studio blocks."]}),"\n",(0,i.jsx)(n.h3,{id:"installing-more-languages",children:"Installing more languages"}),"\n",(0,i.jsxs)(n.p,{children:["If your required language is not already installed on the robot, you can go through the following steps to add it to the ",(0,i.jsx)(n.code,{children:"qt_vosk_app"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["find the proper model of your desired language from ",(0,i.jsx)(n.a,{href:"https://alphacephei.com/vosk/models",children:"https://alphacephei.com/vosk/models"})," list."]}),"\n",(0,i.jsxs)(n.li,{children:["Download and unzip the model into ",(0,i.jsx)(n.code,{children:"~/robot/vosk/models"})," QTPC  folder."]}),"\n",(0,i.jsxs)(n.li,{children:["Rename the model folder to the short language code (ISO) such as ",(0,i.jsx)(n.code,{children:"it_IT"})," for Italian."]}),"\n",(0,i.jsxs)(n.li,{children:["relaunch ",(0,i.jsx)(n.code,{children:"qt_vosk_app"})," or simply reboot the QTrobot."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"online-speech-recognition",children:"Online speech recognition"}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsx)(n.p,{children:"Always check if you have the latest code of the 'qt_gspeech_app' on your QTPC. You can check with 'git status' command in the '~/robot/code/software' folder."}),(0,i.jsx)(n.p,{children:"Latest update (10. May 2023):"}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Enabled Automatic punctuation"}),"\n",(0,i.jsx)(n.li,{children:"Default Timeout of 5 min: call the service with timeout=0 to enable it"}),"\n",(0,i.jsx)(n.li,{children:"BUG FIX: Sometimes Google speech recognition would return empty response when providing timeout in rosservice call, use default timeout instead"}),"\n"]})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_gspeech_app",children:(0,i.jsx)(n.strong,{children:"qt_gspeech_app"})}),"\r\nprovides an ",(0,i.jsx)(n.strong,{children:"online"})," (required INTERNET connection) multilingual speech recognition ROS service based on Google Speech-To-Text API. The ",(0,i.jsx)(n.code,{children:"qt_gspeech_app"})," is not running by default and it ",(0,i.jsx)(n.strong,{children:"cannot"})," be run simultaneously with other voice apps such as ",(0,i.jsx)(n.code,{children:"qt_vosk_app"}),". To disable the ",(0,i.jsx)(n.code,{children:"qt_vosk_app"}),", you can simply comment the corresponding line in ",(0,i.jsx)(n.code,{children:"~/robot/autostart/autostart_screens.sh"})," on QTPC and reboot the robot."]}),"\n",(0,i.jsx)(n.h3,{id:"setup-instructions",children:"Setup instructions"}),"\n",(0,i.jsx)(n.h4,{id:"google-account-and-api-credentials",children:"Google account and API credentials"}),"\n",(0,i.jsxs)(n.p,{children:["Please follow the setup instructions to ",(0,i.jsx)(n.a,{href:"https://cloud.google.com/speech-to-text/docs/quickstart-client-libraries#before-you-begin",children:(0,i.jsx)(n.strong,{children:"setup your google account"})})," before enabling the ",(0,i.jsx)(n.code,{children:"qt_gspeech_app"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Export your google api credentials in .json file and save it on the QTPC."}),"\n",(0,i.jsxs)(n.p,{children:["Edit and add to the last line in ",(0,i.jsx)(n.code,{children:"~/.bash_aliases"})," file:"]}),"\n",(0,i.jsxs)(n.p,{children:["export GOOGLE_APPLICATION_CREDENTIALS=",(0,i.jsx)(n.code,{children:"<path-to-your-file>"}),"\r\n",(0,i.jsx)(n.em,{children:"example"}),':"/home/qtrobot/credentials.json"']}),"\n",(0,i.jsx)(n.h4,{id:"python-virtualenvironment",children:"Python virtualenvironment"}),"\n",(0,i.jsxs)(n.p,{children:["We recommend you use ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/venv.html",children:"python virtual environments"})," to install all python modules and setup the google speech recognition."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Install python virtual environment:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo apt install python3.8-venv\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Navigate to 'qt_gspeech_app' folder:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/robot/code/software/apps/qt_gspeech_app\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Setup virtual environment:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python3 -m venv .venv\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["To use is you just need to activate it:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source .venv/bin/activate\n"})}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"install all python modules and plugins inside this virtual environment."}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"link-to-catkin-workspace-and-build",children:"Link to catkin workspace and build"}),"\n",(0,i.jsx)(n.p,{children:"To be able to use rosservice you need to link the code to the catkin workspace and build it:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"navigate to catkin workspace:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/catkin_ws/src\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"link the qt_gspeech_app:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ln -s /home/qtrobot/robot/code/software/apps/qt_gspeech_app .\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsx)(n.li,{children:"rebuild catkin workspace:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/catkin_ws && caktin_make -j4\n"})}),"\n",(0,i.jsx)(n.h4,{id:"autostart",children:"Autostart"}),"\n",(0,i.jsxs)(n.p,{children:["To enable it in autostart script (",(0,i.jsx)(n.code,{children:"~/robot/autostart/autostart_screens.sh"}),") on QTPC follow next steps:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"edit autostart_screen.sh"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"nano ~/robot/autostart/autostart_screens.sh\n"})}),"\n",(0,i.jsx)(n.p,{children:"and add this line below other scripts:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'run_script "start_qt_gspeech_app.sh"\n'})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:['create "start_qt_gspeech_app.sh"',"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"nano ~/robot/autostart/start_qt_gspeech_app.sh\n"})}),"\n","add and edit the following content:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# !/bin/bash\r\n\r\n source /home/qtrobot/robot/autostart/qt_robot.inc\r\n SCRIPT_NAME="start_qt_gspeech_app"\r\n LOG_FILE=$(prepare_logfile "$SCRIPT_NAME")\r\n\r\n {\r\n prepare_ros_environment\r\n wait_for_ros_node "/rosout" 60\r\n\r\n source /home/qtrobot/catkin_ws/src/qt_gspeech_app/.venv/bin/activate;\r\n python /home/qtrobot/catkin_ws/src/qt_gspeech_app/src/qt_gspeech_app_node.py;\r\n\r\n } &>> ${LOG_FILE}\n'})}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"save the file and reboot the QTrobot"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"accessing-voice-recognition-from-terminal-1",children:"Accessing voice recognition from terminal"}),"\n",(0,i.jsxs)(n.p,{children:["Similar to the offline version of speech recognition, the interface can be accessed using ROS Service ",(0,i.jsx)(n.code,{children:"/qt_robot/speech/recognize"})," command line tools as shown bellow:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"rosservice call /qt_robot/speech/recognize \"language: 'en_US'\r\noptions:\r\n- ''\r\ntimeout: 10\"\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can refer to the instruction given above for the offline version of the voice recognition to use the ",(0,i.jsx)(n.code,{children:"qt_gspeech_app"})," service in a Python code or using QTrobot visual studio blocks."]}),"\n",(0,i.jsxs)(n.admonition,{type:"note",children:[(0,i.jsxs)(n.p,{children:["If you would like to use ",(0,i.jsx)(n.code,{children:"qt_vosk_app"})," and ",(0,i.jsx)(n.code,{children:"qt_gspeech_app"}),' at the same time, then you will need to change rosservice that they are providing. By default both of them are using "qt_robot/speech/recognize".']}),(0,i.jsx)(n.p,{children:"To change the rosservice for qt_gspeech_app follow the next steps on QTPC:"}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"cd ~/robot/code/software/apps/qt_gspeech_app/src"}),"\n",(0,i.jsx)(n.li,{children:"edit qt_gspeech_app_node.py line 67:"}),"\n"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"self.speech_recognize = rospy.Service('/qt_robot/speech/recognize', speech_recognize, self.callback_recognize)  \n"})}),(0,i.jsx)(n.p,{children:"Change 'qt_robot/speech/recognize' to 'qt_robot/gspeech/recognize'\r\n3. save the file\r\n4. next time you run 'qt_gspeech_app' it will be available on 'qt_robot/gspeech/recognize' rosservice"})]}),"\n",(0,i.jsxs)(n.h2,{id:"tips-for-better-speech-recognition",children:[(0,i.jsx)(n.strong,{children:"Tips"})," for better speech recognition"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(s.Z,{children:" lightbulb "}),"  The microphone is installed on top of the QTrobot's head. Therefore, it is always better to talk to QTrobot from above (higher than robot) the robot so that your voice can clearly reaches the microphone. ",(0,i.jsx)("br",{})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)("br",{})," ",(0,i.jsx)(s.Z,{children:" lightbulb "}),"  For faster and more reactive speech recognition, provide proper options (if applicable) to the service call. For example, if you only need to recognize ",(0,i.jsx)(n.code,{children:"yes"})," or ",(0,i.jsx)(n.code,{children:"no"})," in a sentence, give these values as options so that the service immediately return one of these two values as soon being detected by engine. ",(0,i.jsx)("br",{})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)("br",{})," ",(0,i.jsx)(s.Z,{children:" lightbulb "}),"  In case of ",(0,i.jsx)(n.code,{children:"qt_vosk_app"}),", there might be delay from the moment you switch the language at run-time until the engine loads the model and start analyzing the voice. This is related ONLY to the first call for switching the language."]}),"\n",(0,i.jsx)(n.h2,{id:"accessing-audio-voice-direction-and-other-data",children:"Accessing audio, voice direction and other data"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_respeaker_app",children:(0,i.jsx)(n.strong,{children:"qt_respeaker_app"})})," provide ROS services for streaming multichannel microphone audio data, voice activity, voice direction and etc. Here is a list of the topics which are published by ",(0,i.jsx)(n.code,{children:"qt_respeaker_app"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel0"}),": processed audio for ASR (mix of 4 microphones data)"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel1"})," : mic1 raw data"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel2"})," : mic2 raw data"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel3"})," : mic3 raw data"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel4"})," : mic4 raw data"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel5"})," : merged playback"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/is_speaking"}),": VAD (Voice Activity Detection)"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/qt_respeaker_app/sound_direction"}),": DOA (Direction of Arrival)"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["When using VOD values, please not that ",(0,i.jsx)(n.code,{children:"270"})," indicates to the front of QTrobot due to the microphone orientation in the robot's head."]})}),"\n",(0,i.jsx)(n.h3,{id:"configuring-qt_respeaker_app",children:"Configuring qt_respeaker_app"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"qt_respeaker_app"})," is already installed in the ",(0,i.jsx)(n.code,{children:"~/catkin_ws"})," folder on QTRP. There is a config file ",(0,i.jsx)(n.code,{children:"qt_respeaker_app.yaml"})," in the app folder which can be used to configure the Respeaker microphone especially with tuning parameters. Here are  some of the default tuning parameters:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"qt_respeaker_app: \r\n  suppress_pyaudio_error: true\r\n  update_rate: 10.0\r\n  tuning: \r\n    AGCGAIN: 50.0\r\n    AGCONOFF: 0\r\n    CNIONOFF: 0\r\n    GAMMA_NS_SR: 1.8\r\n    MIN_NS_SR: 0.01\r\n    STATNOISEONOFF_SR: 1  \n"})}),"\n",(0,i.jsxs)(n.p,{children:["For most of the cases the default parameters should just work fine for you. However, you may need to adjust some of these values such as ",(0,i.jsx)(n.code,{children:"AGCGAIN"})," to have loader (more gain) in streamed audio or ",(0,i.jsx)(n.code,{children:"MIN_NS_SR"})," and ",(0,i.jsx)(n.code,{children:"GAMMA_NS_SR"})," to better eliminate background noises such as QTrobot internal fan's noise."]}),"\n",(0,i.jsx)(n.h3,{id:"recording-raw-audio-data",children:"Recording raw audio data"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"qt_respeaker_app"})," streams each microphone channel's data in seperate topics: ",(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel0 - 5"}),". To record any or multiple of these channels, you can simply subscribes to the correspondig channels from QTPC (or any other computer in ROS network) and store the audio data in a WAV file or other audio formats. You can take a looke at ",(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/blob/master/apps/qt_respeaker_app/examples/audio_record.py",children:"audio_record.py"})," examples to see how to record ",(0,i.jsx)(n.code,{children:"/qt_respeaker_app/channel0"})," in a ",(0,i.jsx)(n.code,{children:".wav"})," file. Here is also a simple code snippet:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import wave\r\nfrom audio_common_msgs.msg import AudioData\r\n\r\ndef channel_callback(msg, wf):\r\n    wf.writeframes(msg.data)\r\n\r\nwf = wave.open(\"audio.wav\", 'wb')\r\nwf.setnchannels(1)\r\nwf.setsampwidth(2)\r\nwf.setframerate(16000)    \r\nrospy.Subscriber('/qt_respeaker_app/channel0', AudioData, channel_callback, wf)\r\n...\r\nwf.close()\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The above code snippet records processed audio for ASR from channel 0 and save it in ",(0,i.jsx)(n.code,{children:"audio.wav"})," file.  You can later process or listen to it.\r\nBy default some tuning parameters for noise reduction and automatic gain level are set in ",(0,i.jsx)(n.code,{children:"config/qt_respeaker_app.yaml"}),". If, for example, you need to record audio with different gain level, you can simply change the ",(0,i.jsx)(n.code,{children:"AGCGAIN: 100.0"})," and reluach the ",(0,i.jsx)(n.code,{children:"qt_respeaker_app"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"microphone-tuning-tool",children:"Microphone tuning tool"}),"\n",(0,i.jsxs)(n.p,{children:["We have developed ",(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/tools/respeaker_mic_tunning",children:(0,i.jsx)(n.strong,{children:"respeaker_mic_tuning"})})," graphical tools based on ",(0,i.jsx)(n.a,{href:"https://github.com/respeaker/usb_4_mic_array",children:"ReSpeaker USB 4 Mic Array tuning software"})," to easier find and tune the Respeaker microphone  parameters. Using the gui, you can tune the parameters at runtime and find the one that value which best fits your scenario such as different audio gain level or different background noise elimination."]}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:"/img/tunning_gui.png",alt:"tunning_gui"})}),"\n",(0,i.jsxs)(n.p,{children:["The tool is available in QTRP under ",(0,i.jsx)(n.code,{children:"~/robot/code/software/tools/"})," folder. To run it:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Open a terminal on QTPC (with display attached to the robot)"}),"\n",(0,i.jsxs)(n.li,{children:["SSH to QTRP using ",(0,i.jsx)(n.code,{children:"-X"})," paramter: ",(0,i.jsx)(n.code,{children:"ssh -X qtrp"})]}),"\n",(0,i.jsxs)(n.li,{children:["switch to the ",(0,i.jsx)(n.code,{children:"~/robot/code/software/tools/respeaker_mic_tunning"})," folder"]}),"\n",(0,i.jsxs)(n.li,{children:["run the tuning tool: ",(0,i.jsx)(n.code,{children:"python3 ./tunning_gui.py"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"what-can-i-tune",children:"What can I tune?"}),"\n",(0,i.jsx)(n.p,{children:"You can find the best value and tune each Respeaker parameter depending on your need. You may need to learn and got some knowledge of audio signal processing to understand all these parameters. However, in most cases you simply need to adjust the following paramters of the Respeaker microphone:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"AGCONOFF"}),": to turn of or on automatic gain control. When it is 'OFF', the audio will be captured with a constant gain set by ",(0,i.jsx)(n.code,{children:"AGCGAIN"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"AGCGAIN"}),": the sudio signal gain value. Higher value will results loader (higher volume) audio streamed signal."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"GAMMA_NS_SR"})," and ",(0,i.jsx)(n.code,{children:"MIN_NS_SR"}),": these two values together specify how much background noise should be eleminated. these values already set up in QTrobot to supress and eliminate the background noise from internal fan. You can adjust these values to have more clear audio signal."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"what-can-i-do-with-the-tunned-values",children:"What can I do with the tunned values?"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"respeaker_mic_tuning"})," temporary adjusts the paramters of Respeaker microphone. However, values of these paramters will be reseted after rebooting the robot.\r\ntherefore, when you find the currect values which fits your application scenario, you can use those values to confgiure the corresponding QTrobot interfaces.\r\nFor example, by setting them in ",(0,i.jsx)(n.code,{children:"qt_respeaker_app.yaml"})," for ",(0,i.jsx)(n.code,{children:"qt_respeaker_app"})," or use them to configure within your own code. here is simple code snippet to set Respeaker's paramters via a Python code:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import usb.core\r\nfrom tuning import Tuning\r\n\r\nmic = usb.core.find(idVendor=0x2886, idProduct=0x0018)\r\ndev = Tuning(mic)\r\n    \r\ndev.write("AGCONOFF", 0)\r\ndev.write("AGCGAIN", 100.0)\r\n...\n'})}),"\n",(0,i.jsxs)(n.p,{children:["You can take a look at our ",(0,i.jsx)(n.a,{href:"https://github.com/luxai-qtrobot/software/blob/master/apps/qt_vosk_app/src/qt_vosk_app_node.py",children:"qt_vosk_app_node.py"})," as a reference code."]})]})}function p(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);