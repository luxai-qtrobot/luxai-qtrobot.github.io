"use strict";(self.webpackChunkqtrobot_documentation=self.webpackChunkqtrobot_documentation||[]).push([[2335],{1922:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>c,toc:()=>d});var s=n(5893),r=n(1151),o=n(9656);const i={id:"python_ros_gestures",title:"Human gesture detection",hide_table_of_contents:!0},a="Human gesture detection",c={id:"tutorials/python/python_ros_gestures",title:"Human gesture detection",description:"signalcellularalt &nbsp;Level:&nbsp; Intermediate",source:"@site/versioned_docs/version-QTRD_v1/tutorials/python/python_ros_gestures_detection.mdx",sourceDirName:"tutorials/python",slug:"/tutorials/python/python_ros_gestures",permalink:"/docs/v1/tutorials/python/python_ros_gestures",draft:!1,unlisted:!1,tags:[],version:"QTRD_v1",frontMatter:{id:"python_ros_gestures",title:"Human gesture detection",hide_table_of_contents:!0},sidebar:"code_tutorials_sidebar",previous:{title:"Human facial expression detection",permalink:"/docs/v1/tutorials/python/python_ros_expression"},next:{title:"Human hands detection",permalink:"/docs/v1/tutorials/python/python_ros_hands"}},l={},d=[{value:"Create a python project",id:"create-a-python-project",level:2},{value:"Code",id:"code",level:2},{value:"Explanation",id:"explanation",level:2}];function u(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"human-gesture-detection",children:"Human gesture detection"}),"\n","\n","\n",(0,s.jsxs)(t.admonition,{title:"Overview",type:"info",children:[(0,s.jsxs)(t.p,{children:[(0,s.jsx)(o.Z,{children:"signal_cellular_alt"})," \xa0",(0,s.jsx)(t.strong,{children:"Level:"}),"\xa0 ",(0,s.jsx)(t.em,{children:"Intermediate"}),"\n",(0,s.jsx)("br",{})," ",(0,s.jsx)(o.Z,{children:" track_changes "})," \xa0",(0,s.jsx)(t.strong,{children:"Goal:"}),"\xa0 ",(0,s.jsx)(t.em,{children:"learn how to detect Human gesture expressions with QTrobot Nuitrack interface"}),"\n",(0,s.jsx)("br",{})," ",(0,s.jsx)(o.Z,{children:" task_alt "})," \xa0",(0,s.jsx)(t.strong,{children:"Requirements:"})]}),(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\xa0\xa0",(0,s.jsx)(t.a,{href:"/docs/intro_code",children:"Quick start with coding on QTrobot"})]}),"\n",(0,s.jsxs)(t.li,{children:["\xa0\xa0",(0,s.jsx)(t.a,{href:"/docs/tutorials/python/python_ros_project",children:"Create a ROS python project"})]}),"\n",(0,s.jsxs)(t.li,{children:["\xa0\xa0",(0,s.jsx)(t.a,{href:"/docs/tutorials/python/python_ros_publish",children:"QTrobot interfaces using ROS Topics"})]}),"\n",(0,s.jsxs)(t.li,{children:["\xa0\xa0",(0,s.jsx)(t.a,{href:"/docs/tutorials/python/python_ros_subscribe",children:"QTrobot interfaces using ROS Subscribers"})]}),"\n"]})]}),"\n",(0,s.jsxs)(t.p,{children:["In this tutorial we will learn about how to read human gestures with ",(0,s.jsx)(t.a,{href:"/docs/api_ros#human-3d-tracking-interface",children:"QTrobot Nuitrack interface"})," and react to them."]}),"\n",(0,s.jsx)(t.h2,{id:"create-a-python-project",children:"Create a python project"}),"\n",(0,s.jsxs)(t.p,{children:["First we create a python project for our tutorial. let's call it ",(0,s.jsx)(t.code,{children:"tutorial_qt_subscribe"})," and add the required python file:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:'cd ~/catkin_ws/src\ncatkin_create_pkg tutorial_qt_gestures rospy roscpp -D "Reading human gestures movement"\ncd tutorial_qt_gestures/src\ntouch tutorial_qt_gestures_node.py\nchmod +x tutorial_qt_gestures_node.py\n'})}),"\n",(0,s.jsx)(t.h2,{id:"code",children:"Code"}),"\n",(0,s.jsxs)(t.p,{children:["Open the ",(0,s.jsx)(t.code,{children:"tutorial_qt_gestures_node.py"})," file and the add the following code:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'#!/usr/bin/env python\nimport sys\nimport rospy\nfrom qt_gesture_controller.srv import gesture_play\nfrom qt_nuitrack_app.msg import Gestures\n\ndef gesture_callback(msg):\n    if msg.gestures[0].name == "SWIPE RIGHT":\n        gesturePlay("QT/swipe_right",0)\n    elif msg.gestures[0].name == "SWIPE LEFT":\n        gesturePlay("QT/swipe_left",0)\n\nif __name__ == \'__main__\':\n    rospy.init_node(\'my_tutorial_node\')\n    rospy.loginfo("my_tutorial_node started!")\n\n    gesturePlay = rospy.ServiceProxy(\'/qt_robot/gesture/play\', gesture_play)\n    # define ros subscriber\n    rospy.Subscriber(\'/qt_nuitrack_app/gestures\', Gestures, gesture_callback)\n   \n    try:\n        rospy.spin()\n    except KeyboardInterrupt:\n        pass\n\n    rospy.loginfo("finsihed!")\n\n'})}),"\n",(0,s.jsx)(t.h2,{id:"explanation",children:"Explanation"}),"\n",(0,s.jsxs)(t.p,{children:["First we imported ",(0,s.jsx)(t.code,{children:"Gestures"})," message type from ",(0,s.jsx)(t.code,{children:"qt_nuitrack_app.msg"})," message library. This message type is used in communication with ",(0,s.jsx)(t.code,{children:"/qt_nuitrack_app/gestures"}),"."]}),"\n",(0,s.jsxs)(t.admonition,{title:"Tip",type:"tip",children:[(0,s.jsx)(t.p,{children:"How do we know which messages an interface uses? well, There is a useful command in ROS which tells you that:"}),(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"rostopic info /qt_nuitrack_app/gestures\nType: qt_nuitrack_app/Gestures\n...\n"})})]}),"\n",(0,s.jsxs)(t.p,{children:["Then we created a subscriber for",(0,s.jsx)(t.code,{children:"/qt_nuitrack_app/gestures"})," with callback function ",(0,s.jsx)(t.code,{children:"gesture_callback"}),'.\nIn the callback we react accordingly to what is detected. If we detect gesture "SWIPE RIGHT" we also play a gesture "swipe_right". That will make QTrobot repeat your gestures.']}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'def gesture_callback(msg):\n    if msg.gestures[0].name == "SWIPE RIGHT":\n        gesturePlay("QT/swipe_right",0)\n    elif msg.gestures[0].name == "SWIPE LEFT":\n        gesturePlay("QT/swipe_left",0)\n'})}),"\n",(0,s.jsx)(t.p,{children:'Stand in front of QTrobot and do one of the gestures "SWIPE LEFT" or "SWIPE RIGHT".\nWhen QTrobot detects the gesture, it will repeat the same pre-recorded gesture.'})]})}function h(e={}){const{wrapper:t}={...(0,r.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}}}]);