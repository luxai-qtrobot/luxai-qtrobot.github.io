"use strict";(self.webpackChunkqtrobot_documentation=self.webpackChunkqtrobot_documentation||[]).push([[6707],{48993:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return u},metadata:function(){return c},assets:function(){return d},toc:function(){return m},default:function(){return _}});var a=n(87462),r=n(63366),o=(n(67294),n(3905)),s=n(55381),i=n(14702),p=["components"],l={id:"python_ros_gestures",title:"Human gesture detection",hide_table_of_contents:!0},u="Human gesture detection",c={unversionedId:"tutorials/python/python_ros_gestures",id:"version-QTRD_v1/tutorials/python/python_ros_gestures",title:"Human gesture detection",description:"signalcellularalt &nbsp;Level:&nbsp; Intermediate",source:"@site/versioned_docs/version-QTRD_v1/tutorials/python/python_ros_gestures_detection.md",sourceDirName:"tutorials/python",slug:"/tutorials/python/python_ros_gestures",permalink:"/docs/v1/tutorials/python/python_ros_gestures",tags:[],version:"QTRD_v1",frontMatter:{id:"python_ros_gestures",title:"Human gesture detection",hide_table_of_contents:!0},sidebar:"version-QTRD_v1/code_tutorials_sidebar",previous:{title:"Human facial expression detection",permalink:"/docs/v1/tutorials/python/python_ros_expression"},next:{title:"Human hands detection",permalink:"/docs/v1/tutorials/python/python_ros_hands"}},d={},m=[{value:"Create a python project",id:"create-a-python-project",level:2},{value:"Code",id:"code",level:2},{value:"Explanation",id:"explanation",level:2}],h={toc:m};function _(e){var t=e.components,n=(0,r.Z)(e,p);return(0,o.kt)("wrapper",(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"human-gesture-detection"},"Human gesture detection"),(0,o.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"Overview")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)(s.Z,{mdxType:"Icon"},"signal_cellular_alt")," ",(0,o.kt)(i.Z,{mdxType:"Markdown"},"\xa0**Level:**\xa0 *Intermediate*"),(0,o.kt)("br",null)," ",(0,o.kt)(s.Z,{mdxType:"Icon"}," track_changes ")," ",(0,o.kt)(i.Z,{mdxType:"Markdown"},"\xa0**Goal:**\xa0 *learn how to detect Human gesture expressions with QTrobot Nuitrack interface*"),(0,o.kt)("br",null)," ",(0,o.kt)(s.Z,{mdxType:"Icon"}," task_alt ")," ",(0,o.kt)(i.Z,{mdxType:"Markdown"},"\xa0**Requirements:**"),(0,o.kt)("ul",{parentName:"div"},(0,o.kt)("li",{parentName:"ul"},"\xa0","\xa0",(0,o.kt)("a",{parentName:"li",href:"/docs/intro_code"},"Quick start with coding on QTrobot")),(0,o.kt)("li",{parentName:"ul"},"\xa0","\xa0",(0,o.kt)("a",{parentName:"li",href:"/docs/tutorials/python/python_ros_project"},"Create a ROS python project")),(0,o.kt)("li",{parentName:"ul"},"\xa0","\xa0",(0,o.kt)("a",{parentName:"li",href:"/docs/tutorials/python/python_ros_publish"},"QTrobot interfaces using ROS Topics")),(0,o.kt)("li",{parentName:"ul"},"\xa0","\xa0",(0,o.kt)("a",{parentName:"li",href:"/docs/tutorials/python/python_ros_subscribe"},"QTrobot interfaces using ROS Subscribers"))))),(0,o.kt)("p",null,"In this tutorial we will learn about how to read human gestures with ",(0,o.kt)("a",{parentName:"p",href:"/docs/api_ros#human-3d-tracking-interface"},"QTrobot Nuitrack interface")," and react to them. "),(0,o.kt)("h2",{id:"create-a-python-project"},"Create a python project"),(0,o.kt)("p",null,"First we create a python project for our tutorial. let's call it ",(0,o.kt)("inlineCode",{parentName:"p"},"tutorial_qt_subscribe")," and add the required python file: "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'cd ~/catkin_ws/src\ncatkin_create_pkg tutorial_qt_gestures rospy roscpp -D "Reading human gestures movement"\ncd tutorial_qt_gestures/src\ntouch tutorial_qt_gestures_node.py\nchmod +x tutorial_qt_gestures_node.py\n')),(0,o.kt)("h2",{id:"code"},"Code"),(0,o.kt)("p",null,"Open the ",(0,o.kt)("inlineCode",{parentName:"p"},"tutorial_qt_gestures_node.py")," file and the add the following code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'#!/usr/bin/env python\nimport sys\nimport rospy\nfrom qt_gesture_controller.srv import gesture_play\nfrom qt_nuitrack_app.msg import Gestures\n\ndef gesture_callback(msg):\n    if msg.gestures[0].name == "SWIPE RIGHT":\n        gesturePlay("QT/swipe_right",0)\n    elif msg.gestures[0].name == "SWIPE LEFT":\n        gesturePlay("QT/swipe_left",0)\n\nif __name__ == \'__main__\':\n    rospy.init_node(\'my_tutorial_node\')\n    rospy.loginfo("my_tutorial_node started!")\n\n    gesturePlay = rospy.ServiceProxy(\'/qt_robot/gesture/play\', gesture_play)\n    # define ros subscriber\n    rospy.Subscriber(\'/qt_nuitrack_app/gestures\', Gestures, gesture_callback)\n   \n    try:\n        rospy.spin()\n    except KeyboardInterrupt:\n        pass\n\n    rospy.loginfo("finsihed!")\n\n')),(0,o.kt)("h2",{id:"explanation"},"Explanation"),(0,o.kt)("p",null,"First we imported ",(0,o.kt)("inlineCode",{parentName:"p"},"Gestures")," message type from ",(0,o.kt)("inlineCode",{parentName:"p"},"qt_nuitrack_app.msg")," message library. This message type is used in communication with ",(0,o.kt)("inlineCode",{parentName:"p"},"/qt_nuitrack_app/gestures"),". "),(0,o.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"How do we know which messages an interface uses? well, There is a useful command in ROS which tells you that: "),(0,o.kt)("pre",{parentName:"div"},(0,o.kt)("code",{parentName:"pre"},"rostopic info /qt_nuitrack_app/gestures\nType: qt_nuitrack_app/Gestures\n...\n")))),(0,o.kt)("p",null,"Then we created a subscriber for",(0,o.kt)("inlineCode",{parentName:"p"},"/qt_nuitrack_app/gestures")," with callback function ",(0,o.kt)("inlineCode",{parentName:"p"},"gesture_callback"),'.\nIn the callback we react accordingly to what is detected. If we detect gesture "SWIPE RIGHT" we also play a gesture "swipe_right". That will make QTrobot repeat your gestures.'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'def gesture_callback(msg):\n    if msg.gestures[0].name == "SWIPE RIGHT":\n        gesturePlay("QT/swipe_right",0)\n    elif msg.gestures[0].name == "SWIPE LEFT":\n        gesturePlay("QT/swipe_left",0)\n')),(0,o.kt)("p",null,'Stand in front of QTrobot and do one of the gestures "SWIPE LEFT" or "SWIPE RIGHT".\nWhen QTrobot detects the gesture, it will repeat the same pre-recorded gesture.'))}_.isMDXComponent=!0}}]);