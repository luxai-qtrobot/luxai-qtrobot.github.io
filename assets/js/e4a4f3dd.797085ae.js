"use strict";(self.webpackChunkqtrobot_documentation=self.webpackChunkqtrobot_documentation||[]).push([[7202],{1562:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>c,toc:()=>l});var r=n(5893),s=n(1151),i=n(9656);const a={id:"camera",title:"QTrobot Vision and 3D Camera",hide_table_of_contents:!1},o=void 0,c={id:"modules/camera",title:"QTrobot Vision and 3D Camera",description:"QTrobot has an integrated 3D camera in the head. It is a Intel\xae RealSense\u2122 Depth Camera D455. It is connected to NUC mini PC (QTPC) via USB-C port and it is open for developers to configure and use it.",source:"@site/versioned_docs/version-QTRD_v1/modules/camera.mdx",sourceDirName:"modules",slug:"/modules/camera",permalink:"/docs/v1/modules/camera",draft:!1,unlisted:!1,tags:[],version:"QTRD_v1",frontMatter:{id:"camera",title:"QTrobot Vision and 3D Camera",hide_table_of_contents:!1},sidebar:"modules",previous:{title:"QTrobot Audio processing and Microphone",permalink:"/docs/v1/modules/microphone"},next:{title:"QTrobot Motion and Actuators",permalink:"/docs/v1/modules/motors"}},d={},l=[{value:"Software Interfaces",id:"software-interfaces",level:2},{value:"Human 3D body and facial tracking",id:"human-3d-body-and-facial-tracking",level:2},{value:"Using QT Nuitrack interface from terminal",id:"using-qt-nuitrack-interface-from-terminal",level:3},{value:"Using QT Nuitrack interface from code",id:"using-qt-nuitrack-interface-from-code",level:3},{value:"Using QT Nuitrack interface from visual studio blocks",id:"using-qt-nuitrack-interface-from-visual-studio-blocks",level:3},{value:"<strong>Tips</strong> for better skeleton and facial emotion recognition",id:"tips-for-better-skeleton-and-facial-emotion-recognition",level:3},{value:"Image recognition with QTrobot",id:"image-recognition-with-qtrobot",level:2},{value:"Detecting a custom object",id:"detecting-a-custom-object",level:3},{value:"<strong>Tips</strong> for better object recognition",id:"tips-for-better-object-recognition",level:3},{value:"How to use Intel RealSense SDK with QTrobot",id:"how-to-use-intel-realsense-sdk-with-qtrobot",level:2},{value:"How to use standard ROS usb_cam with QTrobot",id:"how-to-use-standard-ros-usb_cam-with-qtrobot",level:2}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(t.p,{children:["QTrobot has an integrated 3D camera in the head. It is a ",(0,r.jsx)(t.a,{href:"https://store.intelrealsense.com/buy-intel-realsense-depth-camera-d455.html",children:(0,r.jsx)(t.strong,{children:"Intel\xae RealSense\u2122 Depth Camera D455"})}),". It is connected to NUC mini PC (QTPC) via USB-C port and it is open for developers to configure and use it."]}),"\n",(0,r.jsx)("center",{children:(0,r.jsx)("img",{width:"80%",src:"/img/camera.svg",alt:"architecture"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h2,{id:"software-interfaces",children:"Software Interfaces"}),"\n",(0,r.jsxs)(t.p,{children:["Like any other standard camera, the QTrobot 3D camera is an standard Linux video capture device. Bellow you can see the example output of ",(0,r.jsx)(t.code,{children:"v4l2-ctl --list-devices"})," command which lists all video capture devices in QTPC:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"Intel(R) RealSense(TM) 430: Int (usb-0000:00:14.0-4):\r\n        /dev/video0\r\n        /dev/video1\r\n        /dev/video2\r\n        /dev/video3\r\n        /dev/video4\r\n        /dev/video5        \n"})}),"\n",(0,r.jsx)(t.p,{children:"QTrobot comes with the pre-installed software for human 3D body and facial tracking, image recognition and ros usb camera driver. This is installed on QTPC, because direct access to the 3D camera is required."}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.a,{href:"https://github.com/luxai-qtrobot/software/tree/master/apps/qt_nuitrack_app",children:(0,r.jsx)(t.strong,{children:"qt_nuitrack_app"})}),": implements different human 3D body and facial tracking including human full body skeleton, hands position and gestures, facial and emotion recognition using ",(0,r.jsx)(t.a,{href:"https://nuitrack.com",children:(0,r.jsx)(t.strong,{children:"Nuitrack SDK"})}),"."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.a,{href:"http://wiki.ros.org/find_object_2d",children:(0,r.jsx)(t.strong,{children:"find_object_2d"})}),": implements object recognition"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.a,{href:"https://wiki.ros.org/usb_cam",children:(0,r.jsx)(t.strong,{children:"usb_cam"})}),": ROS camera driver for V4L USB Cameras"]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"human-3d-body-and-facial-tracking",children:"Human 3D body and facial tracking"}),"\n",(0,r.jsxs)(t.p,{children:["QTrobot uses ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," for human 3D body and facial tracking. The ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," is installed in QTPC ",(0,r.jsx)(t.code,{children:"~/catkin_ws"})," and it is running by default.\r\nThe ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," uses ",(0,r.jsx)(t.strong,{children:"Nuitrack\u2122"})," which is a 3D tracking middleware developed by ",(0,r.jsx)(t.strong,{children:"3DiVi Inc"}),". This is a solution for skeleton tracking and gesture recognition that enables capabilities of Natural User Interface (NUI). ",(0,r.jsx)(t.strong,{children:"Nuitrack\u2122 framework"})," is multi-language and cross-platform. ",(0,r.jsx)(t.strong,{children:"Nuitrack\u2122 API"}),"s include the set of interfaces for developing applications, which utilize Natural Interaction."]}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{children:"Key Features"}),(0,r.jsx)(t.th,{children:"Application Areas"}),(0,r.jsx)(t.th,{})]})}),(0,r.jsx)(t.tbody,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{children:(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"Full Body Skeletal Tracking (19 Joints)"}),(0,r.jsx)("li",{children:"3D Point Cloud"}),(0,r.jsx)("li",{children:"Face tracking and facial features"}),(0,r.jsx)("li",{children:"Gesture Recognition"}),(0,r.jsx)("li",{children:"Unity and Unreal Engine Plugins"}),(0,r.jsx)("li",{children:"OpenNI 1.5 compatible"})]})}),(0,r.jsx)(t.td,{children:(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"Robot Vision"}),(0,r.jsx)("li",{children:"Medical Rehabilitation"}),(0,r.jsx)("li",{children:"Full Body Tracking for AR / VR"}),(0,r.jsx)("li",{children:"Audience Analytics"}),(0,r.jsx)("li",{children:"Smart Home"}),(0,r.jsx)("li",{children:"Games and Training (Fitness, Dance Lessons)"})]})}),(0,r.jsx)(t.td,{children:(0,r.jsx)("center",{children:(0,r.jsx)("img",{style:{maxWidth:300},src:"/img/facial_feature.png"})})})]})})]}),"\n",(0,r.jsxs)(t.p,{children:["The ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," extracts data from ",(0,r.jsx)(t.code,{children:"Nuitrack SDK"})," and publishes human skeletons, hands, gestures and faces information to its ROS topics:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"/qt_nuitrack_app/faces\r\n/qt_nuitrack_app/gestures\r\n/qt_nuitrack_app/hands\r\n/qt_nuitrack_app/skeletons\n"})}),"\n",(0,r.jsx)(t.h3,{id:"using-qt-nuitrack-interface-from-terminal",children:"Using QT Nuitrack interface from terminal"}),"\n",(0,r.jsxs)(t.p,{children:["Like many other ROS topics, you can subscribe to ",(0,r.jsx)(t.code,{children:"/qt_nuitrack_app/gestures"})," to read detected gesture."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"rostopic echo /qt_nuitrack_app/gestures\n"})}),"\n",(0,r.jsxs)(t.p,{children:["Stand in front of QTrobot and do one of following gestures ",(0,r.jsx)(t.code,{children:"SWIPE UP"}),", ",(0,r.jsx)(t.code,{children:"SWIPE DOWN"}),", ",(0,r.jsx)(t.code,{children:"SWIPE LEFT"})," or ",(0,r.jsx)(t.code,{children:"SWIPE RIGHT"}),".\r\nIf the gesture is detected you will see something like this example:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:'[INFO] [1629900965.371938]: gestures:\r\n  -\r\n    id: 2\r\n    name: "SWIPE UP"\n'})}),"\n",(0,r.jsx)(t.h3,{id:"using-qt-nuitrack-interface-from-code",children:"Using QT Nuitrack interface from code"}),"\n",(0,r.jsxs)(t.p,{children:["Take a look at our ",(0,r.jsx)(t.a,{href:"/docs/tutorials/python/python_ros_gestures",children:(0,r.jsx)(t.strong,{children:"Python Human gesture detection"})})," tutorial to learn how to read gesture data from ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," from a Python code. Here is a snippet:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'from qt_nuitrack_app.msg import Gestures\r\n\r\ndef gesture_callback(msg):\r\n    if msg.gestures[0].name == "SWIPE UP":\r\n        print("I got: %s", msg.gestures[0].name)\r\n\r\nrospy.Subscriber(\'/qt_nuitrack_app/gestures\', Gestures, gesture_callback)\n'})}),"\n",(0,r.jsxs)(t.p,{children:["We have also other python tutorials using ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"}),":"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"/docs/tutorials/python/python_ros_expression",children:(0,r.jsx)(t.strong,{children:"Python - Human facial expression detection"})})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"/docs/tutorials/python/python_ros_hands",children:(0,r.jsx)(t.strong,{children:"Python - Human hands detection"})})}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"using-qt-nuitrack-interface-from-visual-studio-blocks",children:"Using QT Nuitrack interface from visual studio blocks"}),"\n",(0,r.jsxs)(t.p,{children:["QTrobot studio offers very flexible and powerful blocks to handle complex ROS messages and interact with other publishers, subscribers and services. You can follow ",(0,r.jsx)(t.a,{href:"/docs/tutorials/graphical/studio_ros",children:(0,r.jsx)(t.strong,{children:"Using ROS blocks"})})," tutorial to learn about ROS blocs."]}),"\n",(0,r.jsxs)(t.p,{children:["Take a look at our ",(0,r.jsx)(t.a,{href:"/docs/tutorials/graphical/studio_gesture_mem_game",children:"Create an interactive memory game using human gestures"})," tutorials to learn how to call ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," using QTrobot visual studio blocks."]}),"\n",(0,r.jsxs)(t.h3,{id:"tips-for-better-skeleton-and-facial-emotion-recognition",children:[(0,r.jsx)(t.strong,{children:"Tips"})," for better skeleton and facial emotion recognition"]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(i.Z,{children:" lightbulb "}),"  Use QTrobot in a fairly well luminated environment. Intel Realsense 3D cameras are design for mostly indoor applications and in general they are sensitive to lights. ",(0,r.jsx)("br",{}),"\r\n",(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "}),"  When using QTrobot with camera applications, do not put the robot against window or any other source of light (i.e. robot should not look towards window). That may create blurry and unclear camera image and decreases performance of 3D software. ",(0,r.jsx)("br",{}),"\r\n",(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "})," Stay in front of the QTrobot camera so that camera can see most parts of your body (around 1.5m-2m away from the robot). For skeletong, gesture, facial expression, etc. to work properly, nuitrack SDK requires first to see almost whole body (standing position) to detect it first. then you can move closer if it's needed. ",(0,r.jsx)("br",{}),"\r\n",(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "}),"  Similarly, to have facial data recognized by ",(0,r.jsx)(t.code,{children:"/qt_nuitrack_app/faces"}),", first almost your whole body should be visible to QTrobot's camera. If you are in sitting position, just stand up and stay few steps away from QTrobot so that it can see your whole body. After detecting your face, you may walk closer and even sit in front of its camera.",(0,r.jsx)("br",{})]}),"\n",(0,r.jsx)(t.h2,{id:"image-recognition-with-qtrobot",children:"Image recognition with QTrobot"}),"\n",(0,r.jsxs)(t.p,{children:["QTrobot uses ROS ",(0,r.jsx)(t.a,{href:"http://wiki.ros.org/find_object_2d",children:"find_object_2d"})," module for easy and flexible image recognition.  Using a camera, objects are detected and published on a ROS topic with ID and position. Take a look at our ",(0,r.jsx)(t.a,{href:"/docs/tutorials/graphical/studio_card_mem_game",children:(0,r.jsx)(t.strong,{children:"Create an interactive memory game using cards"})})," tutorial to learn how to read data from ",(0,r.jsx)(t.code,{children:"find_objects_2d"}),"."]}),"\n",(0,r.jsxs)(t.p,{children:["Using QTrobot's camera, the find_object_2d simply detect whatever images that are in ",(0,r.jsx)(t.code,{children:"~/robot/data/images"})," folder on QTPC. For example, we can use the following images. These images already exist in the corresponding folder to be recognized by find_object_2d."]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"emotion cards",src:n(6421).Z+"",width:"1663",height:"478"})}),"\n",(0,r.jsxs)(t.p,{children:["If we show one of the above card to QTrobot, the find_object_2d recognizes it and publishes the corresponding id (number in the filename) along with other information to ",(0,r.jsx)(t.code,{children:"/find_object/objects"})," topic. The topic uses a message of standard type std_msgs/Float32MultiArray. The first element in the list is the id of the image which is of our interest for this scenario."]}),"\n",(0,r.jsxs)(t.p,{children:["Like many other ROS topics, you can subscribe to ",(0,r.jsx)(t.code,{children:"/find_object/objects"})," to read detected objects."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"rostopic echo /find_object/objects\n"})}),"\n",(0,r.jsx)(t.p,{children:"For example, if we show the T card image (i.e 2.jpg), the following message will be published. The first element in data field (2.0) is the id of the 2.jpg image."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",metastring:"{4}",children:"layout:\r\n  dim: []\r\n  data_offset: 0\r\ndata: [2.0, 553.0, 553.0, 0.582885205745697, -0.03982475772500038, ...\r\n---\n"})}),"\n",(0,r.jsx)(t.h3,{id:"detecting-a-custom-object",children:"Detecting a custom object"}),"\n",(0,r.jsxs)(t.p,{children:["Simply add images to ",(0,r.jsx)(t.code,{children:"~/robot/data/images"})," folder on QTPC and re-run ",(0,r.jsx)(t.code,{children:"find_object_2d"}),". Subscribe to the ",(0,r.jsx)(t.code,{children:"/find_object/objects"})," topic and read the data.\r\nTake a look at this example how to read data from ",(0,r.jsx)(t.code,{children:"find_object_2d"})," from a Python code. Here is a snippet:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"from std_msgs.msg import Float32MultiArray\r\n\r\ndef image_callback(msg):\r\n    print(msg.data)\r\n\r\nrospy.Subscriber('/find_object/objects', Float32MultiArray, image_callback)\n"})}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["ROS find_object_2d is already pre-installed on QTPC. To enable it you need to add ",(0,r.jsx)(t.code,{children:"run_script start_find_object.sh;"})," to the ",(0,r.jsx)(t.code,{children:"~/robot/autostart/autostart_screens.sh"}),". It needs to access ",(0,r.jsx)(t.code,{children:"/camera/color/image_raw"})," topic to work."]})}),"\n",(0,r.jsxs)(t.h3,{id:"tips-for-better-object-recognition",children:[(0,r.jsx)(t.strong,{children:"Tips"})," for better object recognition"]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(i.Z,{children:" lightbulb "}),"  When using QTrobot with camera applications such as image recogntion, do not put the robot against window or any other source of light (i.e. robot should ",(0,r.jsx)(t.strong,{children:"not"})," look towards window light). That may create blurry and unclear camera image and decreases performance of 3D software. ",(0,r.jsx)("br",{}),"\r\n",(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "}),"  Print your image to be recognized on fairly big and visible card/paper. Don no use very thin paper (transparent) or print the image very small and invisible.",(0,r.jsx)("br",{}),"\r\n",(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "}),"  Show the image fully and clearly to the robot. move the card/paper slightly back and forward in front of the QTrobot camera to find the idle distance with respect to your image card size. ",(0,r.jsx)("br",{}),"\r\n",(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "}),"  Do not use very simple image such as simply colored circle or other simple shapes. By default the ",(0,r.jsx)(t.code,{children:"find_object_2d"})," uses image Detectors/descriptors engine which requires images with lots of features and details in that. In simple words, more complex images are better for this type of recognition. You can learn more about ",(0,r.jsx)(t.a,{href:"http://introlab.github.io/find-object/",children:"Find Object image recognition"})," here. ",(0,r.jsx)("br",{})]}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)("br",{}),(0,r.jsx)(i.Z,{children:" lightbulb "})," Use simple names for your images such as ",(0,r.jsx)(t.code,{children:"15.jpg"})," and place them in ",(0,r.jsx)(t.code,{children:"~/robot/data/images"})," folder on QTPC. Do not forget to relaunch the ",(0,r.jsx)(t.code,{children:"find_object_2d"})," or reboot the robot to load your images."]}),"\n",(0,r.jsx)(t.h2,{id:"how-to-use-intel-realsense-sdk-with-qtrobot",children:"How to use Intel RealSense SDK with QTrobot"}),"\n",(0,r.jsxs)(t.p,{children:["The ",(0,r.jsx)(t.a,{href:"https://github.com/IntelRealSense/librealsense/",children:"Intel\xae RealSense\u2122 SDK 2.0"})," is a cross-platform library for Intel\xae RealSense\u2122 depth cameras, which allows depth and color streaming, and provides intrinsic and extrinsic calibration information. The library also offers synthetic streams (pointcloud, depth aligned to color and vise-versa), and a built-in support for record and playback of streaming sessions."]}),"\n",(0,r.jsx)(t.p,{children:"The SDK comes with:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Intel\xae RealSense\u2122 Viewer"}),": With this application, you can quickly access your Intel\xae RealSense\u2122 Depth Camera to view the depth stream, visualize point clouds, record and playback streams, configure your camera settings, modify advanced controls, enable depth visualization and post processing and much more."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Depth Quality Tool"}),": This application allows you to test the camera\u2019s depth quality, including: standard deviation from plane fit, normalized RMS \u2013 the subpixel accuracy, distance accuracy and fill rate. You should be able to easily get and interpret several of the depth quality metrics and record and save the data for offline analysis."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Debug Tools"}),": Device enumeration, FW logger, etc as can be seen at the tools directory"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Code Samples"}),": These simple examples demonstrate how to easily use the SDK to include code snippets that access the camera into your applications. Check some of the C++ examples including capture, pointcloud and more and basic C examples"]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Wrappers"}),": \tPython, C#/.NET, Node.js API, as well as integration with the following 3rd-party technologies: ROS, ROS2, LabVIEW, OpenCV, PCL, Unity, Matlab, OpenNI, UnrealEngine4 and more to come."]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["You can follow this ",(0,r.jsx)(t.a,{href:"https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md",children:"instalation process"})," for QTPC. Most of these interfaces/softwares ",(0,r.jsx)(t.strong,{children:"exclusively"})," need access to the camera. That means, for example, one should first stop ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," or any other running software that is using camera, and run its own software."]}),"\n",(0,r.jsxs)(t.p,{children:["To disable the ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," you need to edit autostart script (",(0,r.jsx)(t.code,{children:"~/robot/autostart/autostart_screens.sh"}),") on ",(0,r.jsx)(t.strong,{children:"QTPC"}),", which runs on boot of the QTrobot."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"nano ~/robot/autostart/autostart_screens.sh\n"})}),"\n",(0,r.jsx)(t.p,{children:"and comment this line below:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:'#run_script "start_qt_nuitrack_app.sh"\n'})}),"\n",(0,r.jsxs)(t.p,{children:["After that on every next reboot of the QTrobot ",(0,r.jsx)(t.code,{children:"qt_nuitrack_app"})," will not run by default."]}),"\n",(0,r.jsx)(t.h2,{id:"how-to-use-standard-ros-usb_cam-with-qtrobot",children:"How to use standard ROS usb_cam with QTrobot"}),"\n",(0,r.jsxs)(t.p,{children:["ROS ",(0,r.jsx)(t.a,{href:"https://wiki.ros.org/usb_cam",children:(0,r.jsx)(t.strong,{children:"usb_cam"})})," is ROS driver for generic USB cameras. It uses libusb_cam and publishes images as type of sensor_msgs::Image and uses ",(0,r.jsx)(t.a,{href:"https://wiki.ros.org/image_transport",children:"image_transport"})," library to allow compressed image transport."]}),"\n",(0,r.jsxs)(t.p,{children:["The usb_cam publishes raw image data from camera to ",(0,r.jsx)(t.code,{children:"/camera/color/image_raw"})," topic. The message type is ",(0,r.jsx)(t.code,{children:"sensor_msgs/Image"}),":"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"Header header        # Header timestamp, etc\r\nuint32 height         # image height, that is, number of rows\r\nuint32 width          # image width, that is, number of columns\r\nstring encoding       # Encoding of pixels -- channel meaning, ordering, size\r\nuint8 is_bigendian    # is this data bigendian?\r\nuint32 step           # Full row length in bytes\r\nuint8[] data          # actual matrix data, size is (step * rows)\n"})}),"\n",(0,r.jsxs)(t.p,{children:["Some cameras (particularly webcams) output their image data already in JPEG format (compressed image). When running the ",(0,r.jsx)(t.code,{children:"usb_cam"})," you can read also ",(0,r.jsx)(t.code,{children:"/camera/color/image_raw/compressed"})," which publises compressed image format ",(0,r.jsx)(t.code,{children:"sensor_msgs/CompressedImage"}),":"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"Header header        # Header timestamp, etc \r\nstring format        # Specifies the format of the data (jpeg, png)\r\nuint8[] data         # Compressed image buffer\n"})}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["ROS usb_cam is already pre-installed on QTPC. To enable it you need to add ",(0,r.jsx)(t.code,{children:"run_script start_ros_usb_cam.sh;"})," to the ",(0,r.jsx)(t.code,{children:"~/robot/autostart/autostart_screens.sh"})," and disable the other interfaces that are using the camera."]})})]})}function u(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},6421:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/mem_game_images-be51e4d25a5e8528fa19aa7e6868e844.png"}}]);