"use strict";(self.webpackChunkqtrobot_documentation=self.webpackChunkqtrobot_documentation||[]).push([[8563],{3754:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>p,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>c,toc:()=>l});var o=s(5893),t=s(1151),i=s(9656);const r={id:"python_ros_expression",title:"Human facial expression detection",hide_table_of_contents:!0},a="Human facial expression detection",c={id:"tutorials/python/python_ros_expression",title:"Human facial expression detection",description:"signalcellularalt &nbsp;Level:&nbsp; Intermediate",source:"@site/versioned_docs/version-QTRD_v1/tutorials/python/python_ros_expression_detection.mdx",sourceDirName:"tutorials/python",slug:"/tutorials/python/python_ros_expression",permalink:"/docs/v1/tutorials/python/python_ros_expression",draft:!1,unlisted:!1,tags:[],version:"QTRD_v1",frontMatter:{id:"python_ros_expression",title:"Human facial expression detection",hide_table_of_contents:!0},sidebar:"code_tutorials_sidebar",previous:{title:"Commanding QTrobot motors",permalink:"/docs/v1/tutorials/python/python_ros_motors"},next:{title:"Human gesture detection",permalink:"/docs/v1/tutorials/python/python_ros_gestures"}},p={},l=[{value:"Create a python project",id:"create-a-python-project",level:2},{value:"Code",id:"code",level:2},{value:"Explanation",id:"explanation",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"human-facial-expression-detection",children:"Human facial expression detection"}),"\n","\n","\n",(0,o.jsxs)(n.admonition,{title:"Overview",type:"info",children:[(0,o.jsxs)(n.p,{children:[(0,o.jsx)(i.Z,{children:"signal_cellular_alt"})," \xa0",(0,o.jsx)(n.strong,{children:"Level:"}),"\xa0 ",(0,o.jsx)(n.em,{children:"Intermediate"}),"\n",(0,o.jsx)("br",{})," ",(0,o.jsx)(i.Z,{children:" track_changes "})," \xa0",(0,o.jsx)(n.strong,{children:"Goal:"}),"\xa0 ",(0,o.jsx)(n.em,{children:"learn how to detect Human facial expressions with QTrobot Nuitrack interface"}),"\n",(0,o.jsx)("br",{})," ",(0,o.jsx)(i.Z,{children:" task_alt "})," \xa0",(0,o.jsx)(n.strong,{children:"Requirements:"})]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\xa0\xa0",(0,o.jsx)(n.a,{href:"/docs/intro_code",children:"Quick start with coding on QTrobot"})]}),"\n",(0,o.jsxs)(n.li,{children:["\xa0\xa0",(0,o.jsx)(n.a,{href:"/docs/tutorials/python/python_ros_project",children:"Create a ROS python project"})]}),"\n",(0,o.jsxs)(n.li,{children:["\xa0\xa0",(0,o.jsx)(n.a,{href:"/docs/tutorials/python/python_ros_publish",children:"QTrobot interfaces using ROS Topics"})]}),"\n",(0,o.jsxs)(n.li,{children:["\xa0\xa0",(0,o.jsx)(n.a,{href:"/docs/tutorials/python/python_ros_subscribe",children:"QTrobot interfaces using ROS Subscribers"})]}),"\n"]})]}),"\n",(0,o.jsxs)(n.p,{children:["In this tutorial we will learn how to detect Human facial expressions with ",(0,o.jsx)(n.a,{href:"/docs/api_ros#human-3d-tracking-interface",children:"QTrobot Nuitrack interface"})," using ROS Subscribes."]}),"\n",(0,o.jsx)(n.h2,{id:"create-a-python-project",children:"Create a python project"}),"\n",(0,o.jsxs)(n.p,{children:["First we create a python project for our tutorial. let's call it ",(0,o.jsx)(n.code,{children:"tutorial_qt_expressions"})," and add the required python file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'cd ~/catkin_ws/src\ncatkin_create_pkg tutorial_qt_expressions std_msgs rospy roscpp -D "Reading human face expressions"\ncd tutorial_qt_expressions/src\ntouch tutorial_qt_expressions_node.py\nchmod +x tutorial_qt_expressions_node.py\n'})}),"\n",(0,o.jsx)(n.h2,{id:"code",children:"Code"}),"\n",(0,o.jsxs)(n.p,{children:["Open the ",(0,o.jsx)(n.code,{children:"tutorial_qt_expressions_node.py"})," file and the add the following code:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python\nimport sys\nimport rospy\nfrom std_msgs.msg import String\nfrom qt_nuitrack_app.msg import Faces\nfrom qt_robot_interface.srv import *\n\n\ndef face_callback(msg):\n    emotions = [msg.faces[0].emotion_angry, msg.faces[0].emotion_happy, msg.faces[0].emotion_surprise]\n    em = max(emotions)\n    em_index = emotions.index(em)\n    if em_index == 0 and em >= 0.9:\n        speech_pub.publish("It looks like you are angry! This is my angry face")\n        emotionShow("QT/angry")\n        rospy.sleep(2)\n    elif em_index == 1 and em >= 0.9:\n        speech_pub.publish("You are happy! This is my happy face")\n        emotionShow("QT/happy")\n        rospy.sleep(2)\n    elif em_index == 2 and em >= 0.9:\n        speech_pub.publish("This is surprising!")\n        emotionShow("QT/surprise")\n        rospy.sleep(2)\n    \n    \nif __name__ == \'__main__\':\n    rospy.init_node(\'my_tutorial_node\')\n    rospy.loginfo("my_tutorial_node started!")\n\n    emotionShow = rospy.ServiceProxy(\'/qt_robot/emotion/show\', emotion_show)\n    speech_pub = rospy.Publisher(\'/qt_robot/speech/say\', String, queue_size=1)\n\n    # define ros subscriber\n    rospy.Subscriber(\'/qt_nuitrack_app/faces\', Faces, face_callback)\n   \n    try:\n        rospy.spin()\n    except KeyboardInterrupt:\n        pass\n\n    rospy.loginfo("finsihed!")\n\n'})}),"\n",(0,o.jsx)(n.h2,{id:"explanation",children:"Explanation"}),"\n",(0,o.jsxs)(n.p,{children:["First we imported ",(0,o.jsx)(n.code,{children:"Faces"})," message type from ",(0,o.jsx)(n.code,{children:"qt_nuitrack_app.msg"})," message library. This message type is used in communication with ",(0,o.jsx)(n.code,{children:"/qt_nuitrack_app/faces"}),"."]}),"\n",(0,o.jsxs)(n.admonition,{title:"Tip",type:"tip",children:[(0,o.jsx)(n.p,{children:"How do we know which messages an interface uses? well, There is a useful command in ROS which tells you that:"}),(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"rostopic info /qt_nuitrack_app/faces\nType: qt_nuitrack_app/Faces\n...\n"})})]}),"\n",(0,o.jsxs)(n.p,{children:["Then we created a subscriber for",(0,o.jsx)(n.code,{children:"/qt_nuitrack_app/faces"})," with callback function ",(0,o.jsx)(n.code,{children:"face_callback"}),".\nIn the callback we react accordingly to what is detected. We check all the emotions that are detected and we use the one which has the highest rating. If we detect happy emotion we also make QTrobot show happy emotion."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def face_callback(msg):\n    emotions = [msg.faces[0].emotion_angry, msg.faces[0].emotion_happy, msg.faces[0].emotion_surprise]\n    em = max(emotions)\n    em_index = emotions.index(em)\n    if em_index == 0 and em >= 0.9:\n        speech_pub.publish("It looks like you are angry! This is my angry face")\n        emotionShow("QT/angry")\n        rospy.sleep(2)\n    elif em_index == 1 and em >= 0.9:\n        speech_pub.publish("You are happy! This is my happy face")\n        emotionShow("QT/happy")\n        rospy.sleep(2)\n    elif em_index == 2 and em >= 0.9:\n        speech_pub.publish("This is surprising!")\n        emotionShow("QT/surprise")\n        rospy.sleep(2)\n'})}),"\n",(0,o.jsx)(n.p,{children:"Stand in front of QTrobot and smile or make an angry face.\nWhen QTrobot detects the emotion, it will repeat the same emotion as you showed."})]})}function d(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}}}]);